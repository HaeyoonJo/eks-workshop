[
{
	"uri": "/calico/install_calico/",
	"title": "Install Calico",
	"tags": [],
	"description": "",
	"content": "Apply the Calico manifest from the aws/amazon-vpc-cni-k8s GitHub project. This creates the daemon sets in the kube-system namespace.\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.2/calico.yaml  Let\u0026rsquo;s go over few key features of the Calico manifest:\n1) We see an annotation throughout; annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.\nkind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 template: metadata: labels: k8s-app: calico-node annotations: # This, along with the CriticalAddonsOnly toleration below, # marks the pod as a critical add-on, ensuring it gets # priority scheduling and that its resources are reserved # if it ever gets evicted. *scheduler**.alpha.kubernetes.io/critical-pod: ''* ...  In contrast, Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes the structure of keys and values is constrained, to optimize queries.\n2) We see that the manifest has a tolerations attribute. Taints and tolerations work together to ensure pods are not scheduled onto inappropriate nodes. Taints are applied to nodes, and the only pods that can tolerate the taint are allowed to run on those nodes.\nA taint consists of a key, a value for it and an effect, which can be:\n PreferNoSchedule: Prefer not to schedule intolerant pods to the tainted node NoSchedule: Do not schedule intolerant pods to the tainted node NoExecute: In addition to not scheduling, also evict intolerant pods that are already running on the node.   Like taints, tolerations also have a key value pair and an effect, with the addition of operator. Here in the Calico manifest, we see tolerations has just one attribute: Operator = exists. This means the key value pair is omitted and the toleration will match any taint, ensuring it runs on all nodes.\n tolerations: - operator: Exists  Watch the kube-system daemon sets and wait for the calico-node daemon set to have the DESIRED number of pods in the READY state.\nkubectl get daemonset calico-node --namespace=kube-system  Expected Output:\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE calico-node 3 3 3 3 3 \u0026lt;none\u0026gt; 38s  "
},
{
	"uri": "/calico/stars_policy_demo/create_resources/",
	"title": "Create Resources",
	"tags": [],
	"description": "",
	"content": " Before creating network polices, let\u0026rsquo;s create the required resources.\nCreate a new folder for the configuration files.\nmkdir ~/environment/calico_resources cd ~/environment/calico_resources  Stars Namespace Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/namespace.yaml  Let\u0026rsquo;s examine our file by running cat namespace.yaml.\nkind: Namespace apiVersion: v1 metadata: name: stars  Create a namespace called stars:\nkubectl apply -f namespace.yaml  We will create frontend and backend replication controllers and services in this namespace in later steps.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/management-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/backend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/frontend.yaml wget https://eksworkshop.com/calico/stars_policy_demo/create_resources.files/client.yaml  cat management-ui.yaml:\napiVersion: v1 kind: Namespace metadata: name: management-ui labels: role: management-ui --- apiVersion: v1 kind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001 selector: role: management-ui --- apiVersion: v1 kind: ReplicationController metadata: name: management-ui namespace: management-ui spec: replicas: 1 template: metadata: labels: role: management-ui spec: containers: - name: management-ui image: calico/star-collect:v0.1.0 imagePullPolicy: Always ports: - containerPort: 9001  Create a management-ui namespace, with a management-ui service and replication controller within that namespace:\nkubectl apply -f management-ui.yaml  cat backend.yaml to see how the backend service is built:\napiVersion: v1 kind: Service metadata: name: backend namespace: stars spec: ports: - port: 6379 targetPort: 6379 selector: role: backend --- apiVersion: v1 kind: ReplicationController metadata: name: backend namespace: stars spec: replicas: 1 template: metadata: labels: role: backend spec: containers: - name: backend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=6379 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 6379  Let\u0026rsquo;s examine the frontend service with cat frontend.yaml:\napiVersion: v1 kind: Service metadata: name: frontend namespace: stars spec: ports: - port: 80 targetPort: 80 selector: role: frontend --- apiVersion: v1 kind: ReplicationController metadata: name: frontend namespace: stars spec: replicas: 1 template: metadata: labels: role: frontend spec: containers: - name: frontend image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --http-port=80 - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status,http://client.client:9000/status ports: - containerPort: 80  Create frontend and backend replication controllers and services within the stars namespace:\nkubectl apply -f backend.yaml kubectl apply -f frontend.yaml  Lastly, let\u0026rsquo;s examine how the client namespace, and a client service for a replication controller. are built. cat client.yaml:\nkind: Namespace apiVersion: v1 metadata: name: client labels: role: client --- apiVersion: v1 kind: ReplicationController metadata: name: client namespace: client spec: replicas: 1 template: metadata: labels: role: client spec: containers: - name: client image: calico/star-probe:v0.1.0 imagePullPolicy: Always command: - probe - --urls=http://frontend.stars:80/status,http://backend.stars:6379/status ports: - containerPort: 9000 --- apiVersion: v1 kind: Service metadata: name: client namespace: client spec: ports: - port: 9000 targetPort: 9000 selector: role: client  Apply the client configuraiton.\nkubectl apply -f client.yaml  Check their status, and wait for all the pods to reach the Running status:\n$ kubectl get pods --all-namespaces  Your output should look like this:\nNAMESPACE NAME READY STATUS RESTARTS AGE client client-nkcfg 1/1 Running 0 24m kube-system aws-node-6kqmw 1/1 Running 0 50m kube-system aws-node-grstb 1/1 Running 1 50m kube-system aws-node-m7jg8 1/1 Running 1 50m kube-system calico-node-b5b7j 1/1 Running 0 28m kube-system calico-node-dw694 1/1 Running 0 28m kube-system calico-node-vtz9k 1/1 Running 0 28m kube-system calico-typha-75667d89cb-4q4zx 1/1 Running 0 28m kube-system calico-typha-horizontal-autoscaler-78f747b679-kzzwq 1/1 Running 0 28m kube-system kube-dns-7cc87d595-bd9hq 3/3 Running 0 1h kube-system kube-proxy-lp4vw 1/1 Running 0 50m kube-system kube-proxy-rfljb 1/1 Running 0 50m kube-system kube-proxy-wzlqg 1/1 Running 0 50m management-ui management-ui-wzvz4 1/1 Running 0 24m stars backend-tkjrx 1/1 Running 0 24m stars frontend-q4r84 1/1 Running 0 24m  It may take several minutes to download all the required Docker images.\n To summarize the different resources we created:\n A namespace called stars frontend and backend replication controllers and services within stars namespace A namespace called management-ui Replication controller and service management-ui for the user interface seen on the browser, in the management-ui namespace A namespace called client client replication controller and service in client namespace  "
},
{
	"uri": "/prerequisites/self_paced/account/",
	"title": "AWS 계정 만들기",
	"tags": [],
	"description": "",
	"content": "실습을 진행할 계정은 새 IAM 역할을 만들고 다른 IAM 권한을 지정할 권한이 있어야합니다.\n  관리자 접근 권한을 가진 AWS 계정이 없다면 : 지금 여기를 클릭해서 계정을 만드세요.\n AWS 계정이 있다면, AWS 계정에 대해 관리자 접근 권한을 가진 IAM 사용자로 다음의 단계를 확인하세요 : 워크샵에서 사용하기 위한 새로운 IAM 사용자 만들기\n 사용자 정보를 입력하세요 :  관리자 접근 IAM 권한을 부여합니다 :  Create sser 버튼을 클릭하여 IAM 유저를 생성합니다 :  로그인 URL을 저장 또는 메모해두세요 :   "
},
{
	"uri": "/",
	"title": "Amazon EKS 워크샵",
	"tags": [],
	"description": "",
	"content": "Amazon EKS 워크샵 이 워크샵에서 우리는 VPC, ALB, EC2의 Kubernetes Worker 그리고 Amazon Elastic Container Service for Kubernetes를 구성하는 여러가지 방법을 알아보게 됩니다.\n"
},
{
	"uri": "/conclusion/conclusion/",
	"title": "What Have We Accomplished",
	"tags": [],
	"description": "",
	"content": "We have:\n Deployed an application consisting of microservices Deployed the Kubernetes Dashboard Deployed packages using Helm Deployed a centralized logging infrastructure Configured Automatic scaling of our pods and worker nodes  "
},
{
	"uri": "/calico/stars_policy_demo/default_policy/",
	"title": "Default Pod-to-Pod Communication",
	"tags": [],
	"description": "",
	"content": "In Kubernetes, the pods by default can communicate with other pods, regardless of which host they land on. Every pod gets its own IP address so you do not need to explicitly create links between pods. This is demonstrated by the management-ui.\nkind: Service metadata: name: management-ui namespace: management-ui spec: type: LoadBalancer ports: - port: 80 targetPort: 9001  To open the Management UI, retrieve the DNS name of the Management UI using:\nkubectl get svc -o wide -n management-ui  Copy the EXTERNAL-IP from the output, and paste into a browser. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com” - the full value is the DNS address.\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR management-ui LoadBalancer 10.100.239.7 a8b8c5f77eda911e8b1a60ea5d5305a4-720629306.us-east-1.elb.amazonaws.com 80:31919/TCP 9s role=management-ui  The UI here shows the default behavior, of all services being able to reach each other.\n"
},
{
	"uri": "/calico/stars_policy_demo/apply_network_policies/",
	"title": "Apply Network Policies",
	"tags": [],
	"description": "",
	"content": " In a production level cluster, it is not secure to have open pod to pod communication. Let\u0026rsquo;s see how we can isolate the services from each other.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/default-deny.yaml  Let\u0026rsquo;s examine our file by running cat default-deny.yaml.\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: name: default-deny spec: podSelector: matchLabels: {}  Let\u0026rsquo;s go over the network policy. Here we see the podSelector does not have any matchLabels, essentially blocking all the pods from accessing it.\nApply the network policy in the stars namespace (frontend and backend services) and the client namespace (client service):\nkubectl apply -n stars -f default-deny.yaml kubectl apply -n client -f default-deny.yaml  Upon refreshing your browser, you see that the management UI cannot reach any of the nodes, so nothing shows up in the UI.\nNetwork policies in Kubernetes use labels to select pods, and define rules on what traffic is allowed to reach those pods. They may specify ingress or egress or both. Each rule allows traffic which matches both the from and ports sections.\nCreate two new network policies.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui.yaml wget https://eksworkshop.com/calico/stars_policy_demo/apply_network_policies.files/allow-ui-client.yaml  Again, we can examine our file contents by running: cat allow-ui.yaml\nkind: NetworkPolicy apiVersion: extensions/v1beta1 metadata: namespace: stars name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  cat allow-ui-client.yaml\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: client name: allow-ui spec: podSelector: matchLabels: {} ingress: - from: - namespaceSelector: matchLabels: role: management-ui  Challenge: How do we apply our network policies to allow the traffic we want?\n  Expand here to see the solution   kubectl apply -f allow-ui.yaml kubectl apply -f allow-ui-client.yaml    Upon refreshing your browser, you can see that the management UI can reach all the services, but they cannot communicate with each other.\n"
},
{
	"uri": "/calico/stars_policy_demo/directional_traffic/",
	"title": "Allow Directional Traffic",
	"tags": [],
	"description": "",
	"content": " Let\u0026rsquo;s see how we can allow directional traffic from client to frontend and backend.\nCopy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/calico_resources wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/backend-policy.yaml wget https://eksworkshop.com/calico/stars_policy_demo/directional_traffic.files/frontend-policy.yaml  Let\u0026rsquo;s examine this backend policy with cat backend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST FRONTEND USING PODSELECTOR\u0026gt; ports: - protocol: TCP port: 6379  Challenge: After reviewing the manifest, you\u0026rsquo;ll see we have intentionally left few of the configuration fields for you to EDIT. Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: backend-policy spec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379    Let\u0026rsquo;s examine the frontend policy with cat frontend-policy.yaml:\nkind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - \u0026lt;EDIT: UPDATE WITH THE CONFIGURATION NEEDED TO WHITELIST CLIENT USING NAMESPACESELECTOR\u0026gt; ports: - protocol: TCP port: 80  Challenge: Please edit the configuration as suggested. You can find helpful info in this Kubernetes documentation\n  Expand here to see the solution   kind: NetworkPolicy apiVersion: networking.k8s.io/v1 metadata: namespace: stars name: frontend-policy spec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80    To allow traffic from frontend service to the backend service apply the following manifest:\nkubectl apply -f backend-policy.yaml  And allow traffic from the client namespace to the frontend service:\nkubectl apply -f frontend-policy.yaml  Upon refreshing your browser, you should be able to see the network policies in action:\nLet\u0026rsquo;s have a look at the backend-policy. Its spec has a podSelector that selects all pods with the label role:backend, and allows ingress from all pods that have the label role:frontend and on TCP port 6379, but not the other way round. Traffic is allowed in one direction on a specific port number.\nspec: podSelector: matchLabels: role: backend ingress: - from: - podSelector: matchLabels: role: frontend ports: - protocol: TCP port: 6379  The frontend-policy is similar, except it allows ingress from namespaces that have the label role: client on TCP port 80.\nspec: podSelector: matchLabels: role: frontend ingress: - from: - namespaceSelector: matchLabels: role: client ports: - protocol: TCP port: 80  "
},
{
	"uri": "/monitoring/prereqs/",
	"title": "Prereqs",
	"tags": [],
	"description": "",
	"content": " Is helm installed? We will use helm to install Prometheus \u0026amp; Grafana monitoring tools for this chapter. Please review installing helm chapter for instructions if you don\u0026rsquo;t have it installed.\nhelm ls  Configure Storage Class We will use gp2 EBS volumes for simplicity and demonstration purpose. While deploying in Production, you would use io1 volumes with desired IOPS and increase the default storage size in the manifests to get better performance.\nSave the below manifest as prometheus-storageclass.yaml using your favorite editor.\nChallenge: You need to update provisioner value that is applicable to AWS EBS provisioner. Please see Kubernetes documentation for help\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: \u0026lt;EDIT: UPDATE WITH VALUE OF EBS PROVISIONER\u0026gt; parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Expand here to see the solution   kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: prometheus namespace: prometheus provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Retain mountOptions: - debug    Challenge: Create storageclass \u0026ldquo;prometheus\u0026rdquo; by applying proper kubectl command\n  Expand here to see the solution   kubectl create -f prometheus-storageclass.yaml    "
},
{
	"uri": "/deploy/applications/",
	"title": "예제 애플리케이션 배포",
	"tags": [],
	"description": "",
	"content": "apiVersion: apps/v1 kind: Deployment metadata: name: ecsdemo-nodejs labels: app: ecsdemo-nodejs namespace: default spec: replicas: 1 selector: matchLabels: app: ecsdemo-nodejs strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 25% type: RollingUpdate template: metadata: labels: app: ecsdemo-nodejs spec: containers: - image: brentley/ecsdemo-nodejs:latest imagePullPolicy: Always name: ecsdemo-nodejs ports: - containerPort: 3000 protocol: TCP  위에 예제 파일에는 서비스(service)와 이것이 어떻게 배포되는지를 기술되어 있습니다. 이 내용을 kubectl을 이용하여 쿠버네티스 API에 쓰면, 쿠버네티스는 응용 프로그램이 배포될 때에 우리의 설정에 충족시킬 것입니다.\n컨테이너는 3000번 포트로 수신 대기하고, 네이티브 서비스 디스커버리는 운영중인 컨테이너를 찾고 해당 컨테이너와 통신하는데 사용합니다.\n"
},
{
	"uri": "/calico/stars_policy_demo/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up the demo by deleting the namespaces:\nkubectl delete ns client stars management-ui  "
},
{
	"uri": "/statefulset/storageclass/",
	"title": "Define Storageclass",
	"tags": [],
	"description": "",
	"content": " Introduction Dynamic Volume Provisioning allows storage volumes to be created on-demand. StorageClass should be pre-created to define which provisoner should be used and what parameters should be passed when dynamic provisioning is invoked. (See parameters for AWS EBS)\nDefine Storage Class Copy/Paste the following commands into your Cloud9 Terminal.\nmkdir ~/environment/templates cd ~/environment/templates wget https://eksworkshop.com/statefulset/storageclass.files/mysql-storageclass.yml  Check the configuration of mysql-storageclass.yml file by following command.\ncat ~/environment/templates/mysql-storageclass.yml  You can see provisioner is kubernetes.io/aws-ebs and type is gp2 specified as a parameter.\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: mysql-gp2 provisioner: kubernetes.io/aws-ebs parameters: type: gp2 reclaimPolicy: Delete mountOptions: - debug  Create storageclass \u0026ldquo;mysql-gp2\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-storageclass.yml  We will specify \u0026ldquo;mysql-gp2\u0026rdquo; as the storageClassName in volumeClaimTemplates at \u0026ldquo;Create StatefulSet\u0026rdquo; section later.\nvolumeClaimTemplates: - metadata: name: data spec: accessModes: [\u0026quot;ReadWriteOnce\u0026quot;] storageClassName: mysql-gp2 resources: requests: storage: 10Gi    Related files   mysql-storageclass.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_intro/install/",
	"title": "Install Helm CLI",
	"tags": [],
	"description": "",
	"content": " Before we can get started configuring helm we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get \u0026gt; get_helm.sh chmod +x get_helm.sh ./get_helm.sh  Once you install helm, the command will prompt you to run \u0026lsquo;helm init\u0026rsquo;. Do not run \u0026lsquo;helm init\u0026rsquo;. Follow the instructions to configure helm using Kubernetes RBAC and then install tiller as specified below If you accidentally run \u0026lsquo;helm init\u0026rsquo;, you can safely uninstall tiller by running \u0026lsquo;helm reset \u0026ndash;force\u0026rsquo;\n Configure Helm access with RBAC Helm relies on a service called tiller that requires special permission on the kubernetes cluster, so we need to build a Service Account for tiller to use. We\u0026rsquo;ll then apply this to the cluster.\nTo create a new service account manifest:\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system EoF  Next apply the config:\nkubectl apply -f ~/environment/rbac.yaml  Then we can install helm using the helm tooling\nhelm init --service-account tiller  This will install tiller into the cluster which gives it access to manage resources in your cluster.\n"
},
{
	"uri": "/healthchecks/livenessprobe/",
	"title": "Configure Liveness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Use the command below to create a directory\nmkdir -p ~/environment/healthchecks  Save the manifest as ~/environment/healthchecks/liveness-app.yaml using your favorite editor. You can review the manifest that is described below. In the configuration file, livenessProbe determines how kubelet should check the Container in order to consider whether it is healthy or not. kubelet uses periodSeconds field to do frequent check on the Container. In this case, kubelet checks liveness probe every 5 seconds. initialDelaySeconds field is to tell the kubelet that it should wait for 5 seconds before doing the first probe. To perform a probe, kubelet sends a HTTP GET request to the server hosting this Pod and if the handler for the servers /health returns a success code, then the Container is considered healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.\napiVersion: v1 kind: Pod metadata: name: liveness-app spec: containers: - name: liveness image: brentley/ecsdemo-nodejs livenessProbe: httpGet: path: /health port: 3000 initialDelaySeconds: 5 periodSeconds: 5  Let\u0026rsquo;s create the pod using the manifest\nkubectl apply -f ~/environment/healthchecks/liveness-app.yaml  The above command creates a pod with liveness probe\nkubectl get pod liveness-app  The output looks like below. Notice the RESTARTS\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 0 11s  The kubectl describe command will show an event history which will show any probe failures or restarts.\nkubectl describe pod liveness-app  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 38s default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 38s kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Normal Pulling 37s kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 37s kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 37s kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 37s kubelet, ip-192-168-18-63.ec2.internal Started container  Introduce a Failure We will run the next command to send a SIGUSR1 signal to the nodejs application. By issuing this command we will send a kill signal to the application process in docker runtime.\nkubectl exec -it liveness-app -- /bin/kill -s SIGUSR1 1  Describe the pod after waiting for 15-20 seconds and you will notice kubelet actions of killing the Container and restarting it.\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned liveness-app to ip-192-168-18-63.ec2.internal Normal SuccessfulMountVolume 1m kubelet, ip-192-168-18-63.ec2.internal MountVolume.SetUp succeeded for volume \u0026quot;default-token-8bmt2\u0026quot; Warning Unhealthy 30s (x3 over 40s) kubelet, ip-192-168-18-63.ec2.internal Liveness probe failed: Get http://192.168.13.176:3000/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Normal Pulling 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal pulling image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Pulled 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Successfully pulled image \u0026quot;brentley/ecsdemo-nodejs\u0026quot; Normal Created 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Created container Normal Started 0s (x2 over 1m) kubelet, ip-192-168-18-63.ec2.internal Started container Normal Killing 0s kubelet, ip-192-168-18-63.ec2.internal Killing container with id docker://liveness:Container failed liveness probe.. Container will be killed and recreated.  When the nodejs application entered a debug mode with SIGUSR1 signal, it did not respond to the health check pings and kubelet killed the container. The container was subject to the default restart policy.\nkubectl get pod liveness-app  The output looks like below\nNAME READY STATUS RESTARTS AGE liveness-app 1/1 Running 1 12m  Challenge: How can we check the status of the container health checks?\n  Expand here to see the solution   kubectl logs liveness-app  You can also use kubectl logs to retrieve logs from a previous instantiation of a container with --previous flag, in case the container has crashed\nkubectl logs liveness-app --previous  \u0026lt;Output omitted\u0026gt; Example app listening on port 3000! ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:01 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 16 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:06 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; ::ffff:192.168.43.7 - - [20/Nov/2018:22:53:11 +0000] \u0026quot;GET /health HTTP/1.1\u0026quot; 200 17 \u0026quot;-\u0026quot; \u0026quot;kube-probe/1.10\u0026quot; Starting debugger agent. Debugger listening on [::]:5858    "
},
{
	"uri": "/introduction/",
	"title": "소개",
	"tags": [],
	"description": "",
	"content": " 쿠버네티스 소개 쿠버네티스 기초 개념 안내\n반갑습니다! Amazon EKS 워크샵입니다.\n본 워크샵의 취지는 Amazon EKS의 기능들에 대하여 사용자들에게 알려주는 것입니다.\nEKS와 관련하여 쿠버네티스, 도커, 컨테이너 워크플로에 대한 배경지식이 권장되나 필수는 아닙니다.\n이 장에서는 워크샵의 hands-on을 위한 기초로써 쿠버네티스의 기본 원리에 대해 소개하겠습니다.\n설명은 다음 주제들에 따라 진행될 것입니다:\n 쿠버네티스 (k8s) 기초   쿠버네티스 아키텍처   Amazon EKS   "
},
{
	"uri": "/batch/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction Batch processing refers to performing units of work, referred to as a job in a repetitive and unattended fashion. Jobs are typically grouped together and processed in batches (hence the name).\nKubernetes includes native support for running Jobs. Jobs can run multiple pods in parallel until receiving a set number of completions. Each pod can contain multiple containers as a single unit of work.\nArgo enhances the batch processing experience by introducing a number of features:\n Steps based declaration of workflows Artifact support Step level inputs \u0026amp; outputs Loops Conditionals Visualization (using Argo Dashboard) \u0026hellip;and more  In this module, we will build a simple Kubernetes Job, recreate that job in Argo, and add common features and workflows for more advanced batch processing.\n"
},
{
	"uri": "/servicemesh_with_appmesh/components/",
	"title": "Components",
	"tags": [],
	"description": "",
	"content": "App Mesh is made up of the following components:\n Service mesh: A service mesh is a logical boundary for network traffic between the services that reside within it. For more information, see Service Meshes.\n Virtual nodes: A virtual node acts as a logical pointer to a particular task group, such as an ECS service or a Kubernetes deployment. When you create a virtual node, you must specify the DNS service discovery name for your task group. For more information, see Virtual Nodes.\n Envoy proxy and router manager: The Envoy proxy and its router manager container images configure your microservice task group to use the App Mesh service mesh traffic rules that you set up for your virtual routers and virtual nodes. You add these containers to your task group after you have created your virtual nodes, virtual routers, and routes. For more information, see Envoy and Proxy Route Manager Images.\n Virtual routers: The virtual router handles traffic for one or more service names within your mesh. For more information, see Virtual Routers.\n Routes: A route is associated with a virtual router, and it directs traffic that matches a service name prefix to one or more virtual nodes. For more information, see Routes.\n  In this chapter, we\u0026rsquo;ll set up these components, and deploy a simple microservice to it, and then modify the App Mesh routes to demonstrate a canary deployment.\n"
},
{
	"uri": "/servicemesh_with_istio/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Istio Istio is a completely open source service mesh that layers transparently onto existing distributed applications. It\u0026rsquo;s also a platform, including APIs, that let it integrate into any logging platform, or telemetry or policy system.\nLet\u0026rsquo;s review in more detail what each of the components that make up this service mesh are.\n Envoy\n Processes the inbound/outbound traffic from inter-service and service-to-external-service transparently.  Pilot\n Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (e.g., A/B tests, canary deployments, etc.), and resiliency (timeouts, retries, circuit breakers, etc.)  Mixer\n Mixer enforces access control and usage policies across the service mesh, and collects telemetry data from the Envoy proxy and other services.  Citadel\n Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management.   "
},
{
	"uri": "/monitoring/deploy-prometheus/",
	"title": "Deploy Prometheus",
	"tags": [],
	"description": "",
	"content": " Download Prometheus curl -o prometheus-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/prometheus/values.yaml  Open the prometheus-values.yaml you downloaded by double clicking on the file name on the left panel. You need to make three edits to this file.\nSearch for storageClass in the prometheus-values.yaml, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. You will do this twice, under both server \u0026amp; alertmanager manifests\nThe third edit you will do is to expose Prometheus server as a NodePort. Because Prometheus is exposed as ClusterIP by default, the web UI cannot be reached outside of Kubernetes. By exposing the service as NodePort, we will be able to reach Prometheus web UI from the worker node IP address. Search for type: ClusterIP and add nodePort: 30900 and change the type to NodePort as indicated below.\nThis configuration is not recommended in Production and there are better ways to secure it. You can read more about exposing Prometheus web UI in this link\nWhen you search, you will find there are more than one type: ClusterIP in prometheus-values.yaml. You need to update relevant Prometheus manifest. See below snippet for identifying Prometheus manifest\n ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort    Expand here to see the complete yaml   rbac: create: true ## Define serviceAccount names for components. Defaults to component's fully qualified name. ## serviceAccounts: alertmanager: create: true name: kubeStateMetrics: create: true name: nodeExporter: create: true name: pushgateway: create: true name: server: create: true name: alertmanager: ## If false, alertmanager will not be installed ## enabled: true ## alertmanager container name ## name: alertmanager ## alertmanager container image ## image: repository: prom/alertmanager tag: v0.15.2 pullPolicy: IfNotPresent ## alertmanager priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional alertmanager container arguments ## extraArgs: {} ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;/\u0026quot; ## Additional alertmanager container environment variable ## For instance to add a http_proxy ## extraEnv: {} ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, alertmanager Ingress will be created ## enabled: false ## alertmanager Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## alertmanager Ingress additional labels ## extraLabels: {} ## alertmanager Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - alertmanager.domain.com # - domain.com/alertmanager ## alertmanager Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - alertmanager.domain.com ## Alertmanager Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for alertmanager scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for alertmanager pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, alertmanager will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## alertmanager data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## alertmanager data Persistent Volume Claim annotations ## annotations: {} ## alertmanager data Persistent Volume existing claim name ## Requires alertmanager.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## alertmanager data Persistent Volume mount root path ## mountPath: /data ## alertmanager data Persistent Volume size ## size: 2Gi ## alertmanager data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of alertmanager data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to alertmanager pods ## podAnnotations: {} replicaCount: 1 ## alertmanager resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to alertmanager pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## Enabling peer mesh service end points for enabling the HA alert manager ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md # enableMeshPeer : true ## List of IP addresses at which the alertmanager service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 # nodePort: 30000 type: ClusterIP ## Monitors ConfigMap changes and POSTs to a URL ## Ref: https://github.com/jimmidyson/configmap-reload ## configmapReload: ## configmap-reload container name ## name: configmap-reload ## configmap-reload container image ## image: repository: jimmidyson/configmap-reload tag: v0.2.2 pullPolicy: IfNotPresent ## Additional configmap-reload container arguments ## extraArgs: {} ## Additional configmap-reload mounts ## extraConfigmapMounts: [] # - name: prometheus-alerts # mountPath: /etc/alerts.d # subPath: \u0026quot;\u0026quot; # configMap: prometheus-alerts # readOnly: true ## configmap-reload resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} initChownData: ## If false, data ownership will not be reset at startup ## This allows the prometheus-server to be run with an arbitrary user ## enabled: true ## initChownData container name ## name: init-chown-data ## initChownData container image ## image: repository: busybox tag: latest pullPolicy: IfNotPresent ## initChownData resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} kubeStateMetrics: ## If false, kube-state-metrics will not be installed ## enabled: true ## kube-state-metrics container name ## name: kube-state-metrics ## kube-state-metrics container image ## image: repository: quay.io/coreos/kube-state-metrics tag: v1.4.0 pullPolicy: IfNotPresent ## kube-state-metrics priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## kube-state-metrics container arguments ## args: {} ## Node tolerations for kube-state-metrics scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for kube-state-metrics pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to kube-state-metrics pods ## podAnnotations: {} pod: labels: {} replicaCount: 1 ## kube-state-metrics resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 16Mi # requests: # cpu: 10m # memory: 16Mi ## Security context to be added to kube-state-metrics pods ## securityContext: {} service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the kube-state-metrics service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 type: ClusterIP nodeExporter: ## If false, node-exporter will not be installed ## enabled: true ## If true, node-exporter pods share the host network namespace ## hostNetwork: true ## If true, node-exporter pods share the host PID namespace ## hostPID: true ## node-exporter container name ## name: node-exporter ## node-exporter container image ## image: repository: prom/node-exporter tag: v0.16.0 pullPolicy: IfNotPresent ## Specify if a Pod Security Policy for node-exporter must be created ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/ ## podSecurityPolicy: enabled: False ## node-exporter priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Custom Update Strategy ## updateStrategy: type: OnDelete ## Additional node-exporter container arguments ## extraArgs: {} ## Additional node-exporter hostPath mounts ## extraHostPathMounts: [] # - name: textfile-dir # mountPath: /srv/txt_collector # hostPath: /var/lib/node-exporter # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # configMap: certs-configmap # readOnly: true ## Node tolerations for node-exporter scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for node-exporter pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to node-exporter pods ## podAnnotations: {} ## Labels to be added to node-exporter pods ## pod: labels: {} ## node-exporter resource limits \u0026amp; requests ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 200m # memory: 50Mi # requests: # cpu: 100m # memory: 30Mi ## Security context to be added to node-exporter pods ## securityContext: {} # runAsUser: 0 service: annotations: prometheus.io/scrape: \u0026quot;true\u0026quot; labels: {} # Exposed as a headless service: # https://kubernetes.io/docs/concepts/services-networking/service/#headless-services clusterIP: None ## List of IP addresses at which the node-exporter service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] hostPort: 9100 loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9100 type: ClusterIP server: ## Prometheus server container name ## name: server ## Prometheus server container image ## image: repository: prom/prometheus tag: v2.4.3 pullPolicy: IfNotPresent ## prometheus server priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug ## so that the various internal URLs are still able to access as they are in the default case. ## (Optional) prefixURL: \u0026quot;\u0026quot; ## External URL which can access alertmanager ## Maybe same with Ingress host name baseURL: \u0026quot;\u0026quot; ## This flag controls access to the administrative HTTP API which includes functionality such as deleting time ## series. This is disabled by default. enableAdminApi: false global: ## How frequently to scrape targets by default ## scrape_interval: 1m ## How long until a scrape request times out ## scrape_timeout: 10s ## How frequently to evaluate rules ## evaluation_interval: 1m ## Additional Prometheus server container arguments ## extraArgs: {} ## Additional Prometheus server hostPath mounts ## extraHostPathMounts: [] # - name: certs-dir # mountPath: /etc/kubernetes/certs # subPath: \u0026quot;\u0026quot; # hostPath: /etc/kubernetes/certs # readOnly: true extraConfigmapMounts: [] # - name: certs-configmap # mountPath: /prometheus # subPath: \u0026quot;\u0026quot; # configMap: certs-configmap # readOnly: true ## Additional Prometheus server Secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # subPath: \u0026quot;\u0026quot; # secretName: prom-secret-files # readOnly: true ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}} ## Defining configMapOverrideName will cause templates/server-configmap.yaml ## to NOT generate a ConfigMap resource ## configMapOverrideName: \u0026quot;\u0026quot; ingress: ## If true, Prometheus server Ingress will be created ## enabled: false ## Prometheus server Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## Prometheus server Ingress additional labels ## extraLabels: {} ## Prometheus server Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - prometheus.domain.com # - domain.com/prometheus ## Prometheus server Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-server-tls # hosts: # - prometheus.domain.com ## Server Deployment Strategy type # strategy: # type: Recreate ## Node tolerations for server scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for Prometheus server pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Pod affinity ## affinity: {} ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: persistentVolume: ## If true, Prometheus server will create/use a Persistent Volume Claim ## If false, use emptyDir ## enabled: true ## Prometheus server data Persistent Volume access modes ## Must match those of existing PV or dynamic provisioner ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## accessModes: - ReadWriteOnce ## Prometheus server data Persistent Volume annotations ## annotations: {} ## Prometheus server data Persistent Volume existing claim name ## Requires server.persistentVolume.enabled: true ## If defined, PVC must be created manually before volume will be bound existingClaim: \u0026quot;\u0026quot; ## Prometheus server data Persistent Volume mount root path ## mountPath: /data ## Prometheus server data Persistent Volume size ## size: 8Gi ## Prometheus server data Persistent Volume Storage Class ## If defined, storageClassName: \u0026lt;storageClass\u0026gt; ## If set to \u0026quot;-\u0026quot;, storageClassName: \u0026quot;\u0026quot;, which disables dynamic provisioning ## If undefined (the default) or set to null, no storageClassName spec is ## set, choosing the default provisioner. (gp2 on AWS, standard on ## GKE, AWS \u0026amp; OpenStack) ## storageClass: \u0026quot;prometheus\u0026quot; ## Subdirectory of Prometheus server data Persistent Volume to mount ## Useful if the volume's root directory is not empty ## subPath: \u0026quot;\u0026quot; ## Annotations to be added to Prometheus server pods ## podAnnotations: {} # iam.amazonaws.com/role: prometheus replicaCount: 1 ## Prometheus server resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 500m # memory: 512Mi # requests: # cpu: 500m # memory: 512Mi ## Security context to be added to server pods ## securityContext: {} service: annotations: {} labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the Prometheus server service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [54.210.142.247] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 80 nodePort: 30900 type: NodePort ## Prometheus server pod termination grace period ## terminationGracePeriodSeconds: 300 ## Prometheus data retention period (i.e 360h) ## retention: \u0026quot;\u0026quot; pushgateway: ## If false, pushgateway will not be installed ## enabled: true ## pushgateway container name ## name: pushgateway ## pushgateway container image ## image: repository: prom/pushgateway tag: v0.5.2 pullPolicy: IfNotPresent ## pushgateway priorityClassName ## priorityClassName: \u0026quot;\u0026quot; ## Additional pushgateway container arguments ## extraArgs: {} ingress: ## If true, pushgateway Ingress will be created ## enabled: false ## pushgateway Ingress annotations ## annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: 'true' ## pushgateway Ingress hostnames with optional path ## Must be provided if Ingress is enabled ## hosts: [] # - pushgateway.domain.com # - domain.com/pushgateway ## pushgateway Ingress TLS configuration ## Secrets must be manually created in the namespace ## tls: [] # - secretName: prometheus-alerts-tls # hosts: # - pushgateway.domain.com ## Node tolerations for pushgateway scheduling to nodes with taints ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ## tolerations: [] # - key: \u0026quot;key\u0026quot; # operator: \u0026quot;Equal|Exists\u0026quot; # value: \u0026quot;value\u0026quot; # effect: \u0026quot;NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\u0026quot; ## Node labels for pushgateway pod assignment ## Ref: https://kubernetes.io/docs/user-guide/node-selection/ ## nodeSelector: {} ## Annotations to be added to pushgateway pods ## podAnnotations: {} replicaCount: 1 ## pushgateway resource requests and limits ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/ ## resources: {} # limits: # cpu: 10m # memory: 32Mi # requests: # cpu: 10m # memory: 32Mi ## Security context to be added to push-gateway pods ## securityContext: {} service: annotations: prometheus.io/probe: pushgateway labels: {} clusterIP: \u0026quot;\u0026quot; ## List of IP addresses at which the pushgateway service is available ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips ## externalIPs: [] loadBalancerIP: \u0026quot;\u0026quot; loadBalancerSourceRanges: [] servicePort: 9091 type: ClusterIP ## alertmanager ConfigMap entries ## alertmanagerFiles: alertmanager.yml: global: {} # slack_api_url: '' receivers: - name: default-receiver # slack_configs: # - channel: '@you' # send_resolved: true route: group_wait: 10s group_interval: 5m receiver: default-receiver repeat_interval: 3h ## Prometheus server ConfigMap entries ## serverFiles: alerts: {} rules: {} prometheus.yml: rule_files: - /etc/config/rules - /etc/config/alerts scrape_configs: - job_name: prometheus static_configs: - targets: - localhost:9090 # A scrape configuration for running Prometheus on a Kubernetes cluster. # This uses separate scrape configs for cluster components (i.e. API server, node) # and services to allow each to use different authentication configs. # # Kubernetes labels will be added as Prometheus labels on metrics via the # `labelmap` relabeling action. # Scrape config for API servers. # # Kubernetes exposes API servers as endpoints to the default/kubernetes # service so this uses `endpoints` role and uses relabelling to only keep # the endpoints associated with the default/kubernetes service using the # default named port `https`. This works for single API server deployments as # well as HA API server deployments. - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token # Keep only the default/kubernetes service endpoints for the https port. This # will add targets for each API server which Kubernetes adds an endpoint to # the default/kubernetes service. relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-nodes-cadvisor' # Default to scraping over https. If required, just disable this or change to # `http`. scheme: https # This TLS \u0026amp; bearer token file config is used to connect to the actual scrape # endpoints for cluster components. This is separate to discovery auth # configuration because discovery \u0026amp; scraping are two separate concerns in # Prometheus. The discovery auth config is automatic if Prometheus runs inside # the cluster. Otherwise, more config options have to be provided within the # \u0026lt;kubernetes_sd_config\u0026gt;. tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt # If your node certificates are self-signed or use a different CA to the # master CA, then disable certificate verification below. Note that # certificate verification is an integral part of a secure infrastructure # so this should only be disabled in a controlled environment. You can # disable certificate verification by uncommenting the line below. # insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node # This configuration will work only on kubelet 1.7.3+ # As the scrape endpoints for cAdvisor have changed # if you are using older version you need to change the replacement to # replacement: /api/v1/nodes/${1}:4194/proxy/metrics # more info here https://github.com/coreos/prometheus-operator/issues/633 relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor # Scrape config for service endpoints. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/scrape`: Only scrape services that have a value of `true` # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need # to set this to `https` \u0026amp; most likely set the `tls_config` of the scrape config. # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: If the metrics are exposed on a different port to the # service then set this appropriately. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: 'prometheus-pushgateway' honor_labels: true kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: pushgateway # Example scrape config for probing services via the Blackbox Exporter. # # The relabeling allows the actual service scrape endpoint to be configured # via the following annotations: # # * `prometheus.io/probe`: Only probe services that have a value of `true` - job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name # Example scrape config for pods # # The relabeling allows the actual pod scrape endpoint to be configured via the # following annotations: # # * `prometheus.io/scrape`: Only scrape pods that have a value of `true` # * `prometheus.io/path`: If the metrics path is not `/metrics` override this. # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`. - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # adds additional scrape configs to prometheus.yml # must be a string so you have to add a | after extraScrapeConfigs: # example adds prometheus-blackbox-exporter scrape config extraScrapeConfigs: # - job_name: 'prometheus-blackbox-exporter' # metrics_path: /probe # params: # module: [http_2xx] # static_configs: # - targets: # - https://example.com # relabel_configs: # - source_labels: [__address__] # target_label: __param_target # - source_labels: [__param_target] # target_label: instance # - target_label: __address__ # replacement: prometheus-blackbox-exporter:9115 networkPolicy: ## Enable creation of NetworkPolicy resources. ## enabled: false    Deploy Prometheus helm install -f prometheus-values.yaml stable/prometheus --name prometheus --namespace prometheus  Make a note of prometheus endpoint in helm response (you will need this later). It should look similar to below\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local  Check if Prometheus components deployed as expected\nkubectl get all -n prometheus  You should see response similar to below. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/prometheus-alertmanager-77cfdf85db-s9p48 2/2 Running 0 1m pod/prometheus-kube-state-metrics-74d5c694c7-vqtjd 1/1 Running 0 1m pod/prometheus-node-exporter-6dhpw 1/1 Running 0 1m pod/prometheus-node-exporter-nrfkn 1/1 Running 0 1m pod/prometheus-node-exporter-rtrm8 1/1 Running 0 1m pod/prometheus-pushgateway-d5fdc4f5b-dbmrg 1/1 Running 0 1m pod/prometheus-server-6d665b876-dsmh9 2/2 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/prometheus-alertmanager ClusterIP 10.100.89.154 \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-kube-state-metrics ClusterIP None \u0026lt;none\u0026gt; 80/TCP 1m service/prometheus-node-exporter ClusterIP None \u0026lt;none\u0026gt; 9100/TCP 1m service/prometheus-pushgateway ClusterIP 10.100.136.143 \u0026lt;none\u0026gt; 9091/TCP 1m service/prometheus-server NodePort 10.100.151.245 \u0026lt;none\u0026gt; 80/30900 1m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/prometheus-node-exporter 3 3 3 3 3 \u0026lt;none\u0026gt; 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/prometheus-alertmanager 1 1 1 1 1m deployment.apps/prometheus-kube-state-metrics 1 1 1 1 1m deployment.apps/prometheus-pushgateway 1 1 1 1 1m deployment.apps/prometheus-server 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/prometheus-alertmanager-77cfdf85db 1 1 1 1m replicaset.apps/prometheus-kube-state-metrics-74d5c694c7 1 1 1 1m replicaset.apps/prometheus-pushgateway-d5fdc4f5b 1 1 1 1m replicaset.apps/prometheus-server-6d665b876 1 1 1 1m  You can access Prometheus server URL by going to any one of your Worker node IP address and specify port :30900/targets (for ex, 52.12.161.128:30900/targets. Remember to open port 30900 in your Worker nodes Security Group. In the web UI, you can see all the targets and metrics being monitored by Prometheus\n"
},
{
	"uri": "/deploy/deploynodejs/",
	"title": "NodeJS 백엔드 API 배포",
	"tags": [],
	"description": "",
	"content": "NodeJS 백엔드 API를 올려 봅시다!\n다음의 명령어를 귀하의 클라우드9 워크스페이스에 복사/붙여넣기 합니다.\ncd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  디폴로이먼트 상황을 확인하여 배포 진행 과정을 지켜 볼 수 있습니다.\nkubectl get deployment ecsdemo-nodejs  "
},
{
	"uri": "/eksctl/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "For this module, we need to download the eksctl binary:\ncurl --silent --location \u0026quot;https://github.com/weaveworks/eksctl/releases/download/latest_release/eksctl_$(uname -s)_amd64.tar.gz\u0026quot; | tar xz -C /tmp sudo mv -v /tmp/eksctl /usr/local/bin  Confirm the eksctl command works:\neksctl version  "
},
{
	"uri": "/spotworkers/workers/",
	"title": "EC2 워커 추가하기 - 온디맨드 그리고 스팟 EC2",
	"tags": [],
	"description": "",
	"content": " 이미 EKS 클러스터와 작업자 노드가 있지만 작업자로 구성된 일부 스팟 인스턴스가 필요합니다. 또한 지능적인 스케줄링 결정을 내릴 수 있도록 스팟(Spot)과 주문형(On-Demand)을 식별하는 노드 레이블링 전략이 필요합니다. AWS CloudFormation을 사용하여 EKS 클러스터에 연결할 새 작업 노드를 시작합니다.\n이 템플릿은 여러 인스턴스 유형으로 구성된 단일 k8s 노드 그룹을 구매하는 최신 기능을 최대한 활용하여 ASG(Auto Scaling Group)를 생성합니다. 다음 블로그를 확인하세요: New – EC2 Auto Scaling Groups With Multiple Instance Types \u0026amp; Purchase Options\n워커의 역할 이름 조회 먼저 EKS 워커 노드에서 사용중인 역할 이름을 가져옵니다.\necho $ROLE_NAME  다음 단계에서 매개 변수로 사용할 역할 이름을 복사하세요. 오류 또는 빈 응답이 표시되면 아래의 단계를 펼쳐서 명령을 수행하고 다시 역할 이름을 가져옵니다.\n  역할 이름을 환경변수로 설정하려면 여기를 클릭하여 펼치세요   INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile    # Example Output eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXXXXX  보안 그룹 이름 조회 또한 기존 워커 노드와 함께 사용되는 보안 그룹의 ID를 수집해야합니다.\nSTACK_NAME=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) SG_ID=$(aws cloudformation describe-stack-resources --stack-name $STACK_NAME --logical-resource-id SG | jq -r '.StackResources[].PhysicalResourceId') echo $SG_ID  # Example Output sg-0d9fb7e709dff5675  CloudFormation 스택 실행 CloudFormation 템플릿을 새로운 작업자 노드 집합으로 실행하지만 eksctl 도구로 만든 노드 그룹의 CloudFormation 스택을 업데이트 할 수도 있습니다.\nLaunch 버튼을 클릭하여 AWS Management Console에서 CloudFormation 스택을 생성합니다.\n   Launch template       EKS Workers - Spot and On Demand  Launch    Download      클러스터가 있는 위치에 맞는 리전인지 확인하세요.\n 콘솔이 열리면 매개 변수를 입력해야합니다. 아래 표를 참조하십시오.\n   Parameter Value     Stack Name: eksworkshop-spot-workers   Cluster Name: eksworkshop-eksctl (or whatever you named your cluster)   ClusterControlPlaneSecurityGroup: Select from the dropdown. It will contain your cluster name and the words \u0026lsquo;ControlPlaneSecurityGroup\u0026rsquo;   NodeInstanceRole: Use the role name that copied in the step above. (e.g. eksctl-eksworkshop-eksctl-nodegro-NodeInstanceRole-XXXXX)   UseExistingNodeSecurityGroups: Leave as \u0026lsquo;Yes\u0026rsquo;   ExistingNodeSecurityGroups: Use the SG name that copied in the step above. (e.g. sg-0123456789abcdef)   NodeImageId: Visit this link and select the non-GPU image for your region - Check for empty spaces in copy/paste   KeyName: SSH Key Pair created earlier or any valid key will work   NodeGroupName: Leave as spotworkers   VpcId: Select your workshop VPC from the dropdown   Subnets: Select the 3 private subnets for your workshop VPC from the dropdown   BootstrapArgumentsForOnDemand: --kubelet-extra-args --node-labels=lifecycle=OnDemand   BootstrapArgumentsForSpotFleet: --kubelet-extra-args '--node-labels=lifecycle=Ec2Spot --register-with-taints=spotInstance=true:PreferNoSchedule'    부트 스트랩 인자에 대한 이해 EKS Bootstrap.sh 스크립트는 우리가 사용하고있는 EKS Optimized AMI에 패키지되어 있으며 EKS 클러스터 이름과 같은 단일 입력 만 필요합니다. 부트스트랩 스크립트는 런타임에 kubelet-extra-args를 설정하는 것을 지원합니다. 쿠버네티스가 프로비저닝한 노드의 유형을 알 수 있도록 노드 레이블을 구성했습니다. 노드의 수명주기(레이블의 키값)를 온디맨드와 EC2스팟으로 설정했습니다. 스팟 인스턴스에 포드가 스케줄되지 않는 것을 선호하기 때문에 테인트를 PreferNoSchedule으로 설정합니다. 이것은 NoSchedule의 \u0026ldquo;소프트\u0026rdquo; 버전입니다. 시스템은 해당 노드에 파드를 위치시키는 것을 최대한 피하지만, 다른 곳에 위치시킬 수 없는 경우에는 위치시킬 수 있습니다.\n나머지 기본 매개 변수는 그대로 두고 CloudFormation 화면을 계속 진행합니다. AWS CloudFormation이 IAM 리소스를 생성 할 수 있음을 확인합니다 옆의 체크박스를 선택하고 Create를 클릭하십시오.\n워커 생성에는 약 3분이 소요됩니다.\n 노드 확인 새 노드가 클러스터에 올바르게 결합되었는지 확인하세요. 클러스터에 추가 된 2-3 개의 노드가 추가로 표시되어야합니다.\nkubectl get nodes  노드 레이블을 사용하여 노드의 수명주기를 식별 할 수 있습니다.\nkubectl get nodes --show-labels --selector=lifecycle=Ec2Spot  이 명령으로 2개의 노드 정보가 출력되어야 합니다. 노드 출력의 끝에서 노드 레이블 lifecycle=Ec2Spot을 볼 수 있습니다.\n이제 lifecycle=OnDemand로 모든 노드를 보여줍니다. 이 명령의 출력은 CloudFormation 템플릿에 구성된 대로 노드 1개를 반환해야합니다.\nkubectl get nodes --show-labels --selector=lifecycle=OnDemand  특정 spot 노드 중 하나를 kubectl describe nodes 명령어를 이용하여 EC2 스팟 인스턴스에 적용된 taints를 확인할 수 있습니다. "
},
{
	"uri": "/scaling/deploy_hpa/",
	"title": "Configure Horizontal Pod AutoScaler (HPA)",
	"tags": [],
	"description": "",
	"content": " Deploy the Metrics Server Metrics Server is a cluster-wide aggregator of resource usage data. These metrics will drive the scaling behavior of the deployments. We will deploy the metrics server using Helm configured in a previous module\nhelm install stable/metrics-server \\ --name metrics-server \\ --version 2.0.4 \\ --namespace metrics  Confirm the Metrics API is available. Return to the terminal in the Cloud9 Environment\nkubectl get apiservice v1beta1.metrics.k8s.io -o yaml  If all is well, you should see a status message similar to the one below in the response\nstatus: conditions: - lastTransitionTime: 2018-10-15T15:13:13Z message: all checks passed reason: Passed status: \u0026quot;True\u0026quot; type: Available  We are now ready to scale a deployed application "
},
{
	"uri": "/logging/prereqs/",
	"title": "Configure IAM Policy for Worker Nodes",
	"tags": [],
	"description": "",
	"content": "We will be deploying Fluentd as a DaemonSet, or one pod per worker node. The fluentd log daemon will collect logs and forward to CloudWatch Logs. This will require the nodes to have permissions to send logs and create log groups and log streams. This can be accomplished with an IAM user, IAM role, or by using a tool like Kube2IAM.\nIn our example, we will create an IAM policy and attach it the the Worker node role.\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  Create a new IAM Policy and attach it to the Worker Node Role.\nmkdir ~/environment/iam_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/k8s-logs-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Action\u0026quot;: [ \u0026quot;logs:DescribeLogGroups\u0026quot;, \u0026quot;logs:DescribeLogStreams\u0026quot;, \u0026quot;logs:CreateLogGroup\u0026quot;, \u0026quot;logs:CreateLogStream\u0026quot;, \u0026quot;logs:PutLogEvents\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker --policy-document file://~/environment/iam_policy/k8s-logs-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker  "
},
{
	"uri": "/statefulset/configmap/",
	"title": "Create ConfigMap",
	"tags": [],
	"description": "",
	"content": " Introduction ConfigMap allow you to decouple configuration artifacts and secrets from image content to keep containerized applications portable. Using ConfigMap, you can independently control MySQL configuration.\nCreate ConfigMap Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/configmap.files/mysql-configmap.yml  Check the configuration of mysql-configmap.yml file by following command.\ncat ~/environment/templates/mysql-configmap.yml  ConfigMap stores master.cnf, slave.cnf and pass them when initializing master and slave pods defined in statefulset. master.cnf is for the MySQL master pod which has binary log option (log-bin) to provides a record of the data changes to be sent to slave servers and slave.cnf is for slave pods which has super-read-only option.\napiVersion: v1 kind: ConfigMap metadata: name: mysql-config labels: app: mysql data: master.cnf: | # Apply this config only on the master. [mysqld] log-bin slave.cnf: | # Apply this config only on slaves. [mysqld] super-read-only  Create configmap \u0026ldquo;mysql-config\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-configmap.yml    Related files   mysql-configmap.yml  (0 ko)    "
},
{
	"uri": "/helm_root/helm_micro/create_chart/",
	"title": "Create a Chart",
	"tags": [],
	"description": "",
	"content": "Helm charts have a structure similar to:\n/eksdemo /Chart.yaml # a description of the chart /values.yaml # defaults, may be overridden during install or upgrade /charts/ # May contain subcharts /templates/ # the template files themselves ...  We\u0026rsquo;ll follow this template, and create a new chart called eksdemo with the following commands:\ncd ~/environment helm create eksdemo  "
},
{
	"uri": "/dashboard/dashboard/",
	"title": "쿠버네티스 공식 대시보드 배포",
	"tags": [],
	"description": "",
	"content": "쿠버네티스 공식 대시보드는 기본으로 배포되지 않고, 설치 방법이 공식 문서에 있습니다.\n다음 명령어로 대시보드를 배포할 수 있습니다.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  이것이 개인 클러스터에 배포되기 때문에 프락시를 통해서 접근해야 합니다. kube-proxy는 대시보드 서비스로 요청을 프락시할 수 있습니다. 귀하의 워크스페이스에서 다음 명령어를 실행하세요.\nkubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  이러면 프락시를 시작하고 8080번 포트로 모든 인터페이스에서 접속 대기하며 비로컬 호스트 요청을 필터링을 사용하지 않도록 설정합니다.\n이 명령은 현재 터미널 세션에서 백그라운드에서 계속 실행됩니다.\nXSRF 공격을 방어하는 보안 기능인 요청 필터링을 비활성화 하였습니다. 이것은 프로덕션 환경에서 권장하지 않지만 개발 환경에서는 유용합니다.\n "
},
{
	"uri": "/healthchecks/readinessprobe/",
	"title": "Configure Readiness Probe",
	"tags": [],
	"description": "",
	"content": " Configure the Probe Save the text from following block as ~/environment/healthchecks/readiness-deployment.yaml. The readinessProbe definition explains how a linux command can be configured as healthcheck. We create an empty file /tmp/healthy to configure readiness probe and use the same to understand how kubelet helps to update a deployment with only healthy pods.\napiVersion: apps/v1 kind: Deployment metadata: name: readiness-deployment spec: replicas: 3 selector: matchLabels: app: readiness-deployment template: metadata: labels: app: readiness-deployment spec: containers: - name: readiness-deployment image: alpine command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;touch /tmp/healthy \u0026amp;\u0026amp; sleep 86400\u0026quot;] readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 3  We will now create a deployment to test readiness probe\nkubectl apply -f ~/environment/healthchecks/readiness-deployment.yaml  The above command creates a deployment with 3 replicas and readiness probe as described in the beginning\nkubectl get pods -l app=readiness-deployment  The output looks similar to below\n NAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 1/1 Running 0 31s readiness-deployment-7869b5d679-vd55d 1/1 Running 0 31s readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 31s  Let us also confirm that all the replicas are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below\nReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable  Introduce a Failure Pick one of the pods from above 3 and issue a command as below to delete the /tmp/healthy file which makes the readiness probe fail.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- rm /tmp/healthy  readiness-deployment-7869b5d679-922mx was picked in our example cluster. The /tmp/healthy file was deleted. This file must be present for the readiness check to pass. Below is the status after issuing the command.\nkubectl get pods -l app=readiness-deployment  The output looks similar to below:\nNAME READY STATUS RESTARTS AGE readiness-deployment-7869b5d679-922mx 0/1 Running 0 4m readiness-deployment-7869b5d679-vd55d 1/1 Running 0 4m readiness-deployment-7869b5d679-vxb6g 1/1 Running 0 4m  Traffic will not be routed to the first pod in the above deployment. The ready column confirms that the readiness probe for this pod did not pass and hence was marked as not ready.\nWe will now check for the replicas that are available to serve traffic when a service is pointed to this deployment.\nkubectl describe deployment readiness-deployment | grep Replicas:  The output looks like below\nReplicas: 3 desired | 3 updated | 3 total | 2 available | 1 unavailable  When the readiness probe for a pod fails, the endpoints controller removes the pod from list of endpoints of all services that match the pod.\nChallenge: How would you restore the pod to Ready status?   Expand here to see the solution   Run the below command with the name of the pod to recreate the /tmp/healthy file. Once the pod passes the probe, it gets marked as ready and will begin to receive traffic again.\nkubectl exec -it \u0026lt;YOUR-READINESS-POD-NAME\u0026gt; -- touch /tmp/healthy  kubectl get pods -l app=readiness-deployment   \n"
},
{
	"uri": "/codepipeline/role/",
	"title": "Create IAM Role",
	"tags": [],
	"description": "",
	"content": "In an AWS CodePipeline, we are going to use AWS CodeBuild to deploy a sample Kubernetes service. This requires an AWS Identity and Access Management (IAM) role capable of interacting with the EKS cluster.\nIn this step, we are going to create an IAM role and add an inline policy that we will use in the CodeBuild stage to interact with the EKS cluster via kubectl.\nCreate the role:\ncd ~/environment ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) TRUST=\u0026quot;{ \\\u0026quot;Version\\\u0026quot;: \\\u0026quot;2012-10-17\\\u0026quot;, \\\u0026quot;Statement\\\u0026quot;: [ { \\\u0026quot;Effect\\\u0026quot;: \\\u0026quot;Allow\\\u0026quot;, \\\u0026quot;Principal\\\u0026quot;: { \\\u0026quot;AWS\\\u0026quot;: \\\u0026quot;arn:aws:iam::${ACCOUNT_ID}:root\\\u0026quot; }, \\\u0026quot;Action\\\u0026quot;: \\\u0026quot;sts:AssumeRole\\\u0026quot; } ] }\u0026quot; echo '{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;eks:Describe*\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] }' \u0026gt; /tmp/iam-role-policy aws iam create-role --role-name EksWorkshopCodeBuildKubectlRole --assume-role-policy-document \u0026quot;$TRUST\u0026quot; --output text --query 'Role.Arn' aws iam put-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe --policy-document file:///tmp/iam-role-policy  "
},
{
	"uri": "/x-ray/role/",
	"title": "Modify IAM Role",
	"tags": [],
	"description": "",
	"content": "In order for the X-Ray daemon to communicate with the service, we need to add a policy to the worker nodes\u0026rsquo; AWS Identity and Access Management (IAM) role.\nModify the role in the Cloud9 terminal:\n  Expand here if you need to export the Role Name   INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r .Stacks[].StackName | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile    aws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/AWSXRayDaemonWriteAccess  "
},
{
	"uri": "/advanced-networking/secondary_cidr/",
	"title": "Using Secondary CIDRs with EKS",
	"tags": [],
	"description": "",
	"content": " Using Secondary CIDRs with EKS You can expand your VPC network by adding additional CIDR ranges. This capability can be used if you are running out of IP ranges within your existing VPC or if you have consumed all available RFC 1918 CIDR ranges within your corporate network. EKS supports additional IPv4 CIDR blocks in the 100.64.0.0/10 and 198.19.0.0/16 ranges. You can review this announcement from our what\u0026rsquo;s new blog\nIn this tutorial, we will walk you through the configuration that is needed so that you can launch your Pod networking on top of secondary CIDRs\n"
},
{
	"uri": "/prerequisites/",
	"title": "워크샵 준비하기",
	"tags": [],
	"description": "",
	"content": " Getting Started  AWS 계정 만들기   워크스페이스 생성하기   SSH 키 생성하기   쿠버네티스 도구 설치   실습 서비스 레포지토리 클론   워크스페이스를 위한 IAM 역할 생성   워크스페이스에 IAM 역할 부여하기   워크스페이스의 IAM 설정 업데이트   "
},
{
	"uri": "/prerequisites/self_paced/workspace/",
	"title": "워크스페이스 생성하기",
	"tags": [],
	"description": "",
	"content": " Cloud9 워크스페이스는 루트 계정 사용자가 아닌 관리자 권한을 가진 IAM 사용자가 작성해야합니다. 루트 계정 사용자가 아닌 IAM 사용자로 로그인했는지 확인하십시오.\n This workshop was designed to run in the Oregon (us-west-2) region. Please don\u0026rsquo;t run in any other region. Future versions of this workshop will expand region availability, and this message will be removed.\n -- 광고 차단기, 자바 스크립트 비활성화 도구 및 추적 차단기를 비활성화 해야하며, 그렇지 않으면 워크스페이스 연결에 영향을 줄 수 있습니다. Cloud9은 third-party-cookies가 필요합니다. 특정 도메인을 허용 리스트로 관리할 수 있습니다.\n 가까운 리전에서 Cloud9 실행하기:  Singapore Oregon Ohio Ireland  Cloud9 환경 생성하기: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n Cloud9 환경 생성하기: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n Cloud9 환경 생성하기: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n Cloud9 환경 생성하기: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n  $(function(){$(\"#region\").tabs();});  Create environment 버튼을 선택합니다. 이름을 eksworkshop으로 입력하고, 나머지 설정은 기본값으로 진행합니다. 실행이 완료되면, welcome tab과 하단 작업 영역을 닫고 새로운 터미널 화면을 메인 작업 영역에 열어둡니다.   아래의 화면과 같아야 합니다:  이 테마가 마음에 들면 Cloud9 작업 공간 메뉴에서 View / Themes / Solarized / Solarized Dark를 선택하여 직접 선택할 수 있습니다.\n  "
},
{
	"uri": "/x-ray/x-ray-daemon/",
	"title": "Deploy X-Ray DaemonSet",
	"tags": [],
	"description": "",
	"content": "Now that we have modified the IAM role for the worker nodes to permit write operations to the X-Ray service, we are going to deploy the X-Ray DaemonSet to the EKS cluster. The X-Ray daemon will be deployed to each worker node in the EKS cluster. For reference, see the example implementation used in this module.\nThe AWS X-Ray SDKs are used to instrument your microservices. When using the DaemonSet in the example implementation, you need to configure it to point to xray-service.default:2000.\nThe following showcases how to configure the X-Ray SDK for Go. This is merely an example and not a required step in the workshop.\nfunc init() { xray.Configure(xray.Config{ DaemonAddr: \u0026quot;xray-service.default:2000\u0026quot;, LogLevel: \u0026quot;info\u0026quot;, }) }  To deploy the X-Ray DaemonSet:\nkubectl create -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  To see the status of the X-Ray DaemonSet:\nkubectl describe daemonset xray-daemon  The folllowing is an example of the command output:\nTo view the logs for all of the X-Ray daemon pods run the following\n kubectl logs -l app=xray-daemon  "
},
{
	"uri": "/codepipeline/configmap/",
	"title": "Modify aws-auth ConfigMap",
	"tags": [],
	"description": "",
	"content": "Now that we have the IAM role created, we are going to add the role to the aws-auth ConfigMap for the EKS cluster.\nOnce the ConfigMap includes this new role, kubectl in the CodeBuild stage of the pipeline will be able to interact with the EKS cluster via the IAM role.\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) ROLE=\u0026quot; - rolearn: arn:aws:iam::$ACCOUNT_ID:role/EksWorkshopCodeBuildKubectlRole\\n username: build\\n groups:\\n - system:masters\u0026quot; kubectl get -n kube-system configmap/aws-auth -o yaml | awk \u0026quot;/mapRoles: \\|/{print;print \\\u0026quot;$ROLE\\\u0026quot;;next}1\u0026quot; \u0026gt; /tmp/aws-auth-patch.yml kubectl patch configmap/aws-auth -n kube-system --patch \u0026quot;$(cat /tmp/aws-auth-patch.yml)\u0026quot;  If you would like to edit the aws-auth ConfigMap manually, you can run: $ kubectl edit -n kube-system configmap/aws-auth\n "
},
{
	"uri": "/x-ray/microservices/",
	"title": "Deploy Example Microservices",
	"tags": [],
	"description": "",
	"content": "We now have the foundation in place to deploy microservices, which are instrumented with X-Ray SDKs, to the EKS cluster.\nIn this step, we are going to deploy example front-end and back-end microservices to the cluster. The example services are already instrumented using the X-Ray SDK for Go. Currently, X-Ray has SDKs for Go, Python, Node.js, Ruby, .NET and Java.\nkubectl apply -f https://eksworkshop.com/x-ray/sample-front.files/x-ray-sample-front-k8s.yml kubectl apply -f https://eksworkshop.com/x-ray/sample-back.files/x-ray-sample-back-k8s.yml  To review the status of the deployments, you can run:\nkubectl describe deployments x-ray-sample-front-k8s x-ray-sample-back-k8s  For the status of the services, run the following command:\nkubectl describe services x-ray-sample-front-k8s x-ray-sample-back-k8s  Once the front-end service is deployed, run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser.\nkubectl get service x-ray-sample-front-k8s -o wide  After your ELB is deployed and available, open up the endpoint returned by the previous command in your browser and allow it to remain open. The front-end application makes a new request to the /api endpoint once per second, which in turn calls the back-end service. The JSON document displayed in the browser is the result of the request made to the back-end service.\nThis service was configured with a LoadBalancer so, an AWS Elastic Load Balancer (ELB) is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n When the front-end service is first deployed, it can take up to several minutes for the ELB to be created and DNS updated.\n "
},
{
	"uri": "/codepipeline/forksample/",
	"title": "Fork Sample Repository",
	"tags": [],
	"description": "",
	"content": "We are now going to fork the sample Kubernetes service so that we will be able modify the repository and trigger builds.\nLogin to GitHub and fork the sample service to your own account:\nhttps://github.com/rnzsgh/eks-workshop-sample-api-service-go\nOnce the repo is forked, you can view it in your your GitHub repositories.\nThe forked repo will look like:\n"
},
{
	"uri": "/codepipeline/githubcredentials/",
	"title": "GitHub Access Token",
	"tags": [],
	"description": "",
	"content": "In order for CodePipeline to receive callbacks from GitHub, we need to generate a personal access token.\nOnce created, an access token can be stored in a secure enclave and reused, so this step is only required during the first run or when you need to generate new keys.\nOpen up the New personal access page in GitHub.\nYou may be prompted to enter your GitHub password\n Enter a value for Token description, check the repo permission scope and scroll down and click the Generate token button\nCopy the personal access token and save it in a secure place for the next step\n"
},
{
	"uri": "/x-ray/x-ray/",
	"title": "X-Ray Console",
	"tags": [],
	"description": "",
	"content": "We now have the example microservices deployed, so we are going to investigate our Service Graph and Traces in X-Ray section of the AWS Management Console.\nThe Service map in the console provides a visual representation of the steps identified by X-Ray for a particular trace. Each resource that sends data to X-Ray within the same context appears as a service in the graph. In the example below, we can see that the x-ray-sample-front-k8s service is processing 39 transactions per minute with an average latency of 0.99ms per operation. Additionally, the x-ray-sample-back-k8s is showing an average latency of 0.08ms per transaction.\nNext, go to the traces section in the AWS Management Console to view the execution times for the segments in the requests. At the top of the page, we can see the URL for the ELB endpoint and the corresponding traces below.\nIf you click on the link on the left in the Trace list section you will see the overall execution time for the request (0.5ms for the x-ray-sample-front-k8s which wraps other segments and subsegments), as well as a breakdown of the individual segments in the request. In this visualization, you can see the front-end and back-end segments and a subsegment named x-ray-sample-back-k8s-gen In the back-end service source code, we instrumented a subsegment that surrounds a random number generator.\nIn the Go example, the main segment is initialized in the xray.Handler helper, which in turn sets all necessary information in the http.Request context struct, so that it can be used when initializing the subsegment.\nClick on the image to zoom\n "
},
{
	"uri": "/x-ray/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the Tracing with X-Ray module.\nThe content for this module was based on the Application Tracing on Kubernetes with AWS X-Ray blog post.\nThis module is not used in subsequent steps, so you can remove the resources now or at the end of the workshop.\nDelete the Kubernetes example microservices deployed:\nkubectl delete deployments x-ray-sample-front-k8s x-ray-sample-back-k8s kubectl delete services x-ray-sample-front-k8s x-ray-sample-back-k8s  Delete the X-Ray DaemonSet:\nkubectl delete -f https://eksworkshop.com/x-ray/daemonset.files/xray-k8s-daemonset.yaml  "
},
{
	"uri": "/codepipeline/codepipeline/",
	"title": "CodePipeline Setup",
	"tags": [],
	"description": "",
	"content": " Now we are going to create the AWS CodePipeline using AWS CloudFormation.\nCloudFormation is an infrastructure as code (IaC) tool which provides a common language for you to describe and provision all the infrastructure resources in your cloud environment. CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts.\nEach EKS deployment/service should have its own CodePipeline and be located in an isolated source repository.\nYou can modify the CloudFormation templates provided with this workshop to meet your system requirements to easily onboard new services to your EKS cluster. For each new service the following steps can be repeated.\nClick the Launch button to create the CloudFormation stack in the AWS Management Console.\n   Launch template       CodePipeline \u0026amp; EKS  Launch    Download      After the console is open, enter your GitHub username, personal access token (created in previous step), check the acknowledge box and then click the \u0026ldquo;Create stack\u0026rdquo; button located at the bottom of the page.\nWait for the status to change from \u0026ldquo;CREATE_IN_PROGRESS\u0026rdquo; to CREATE_COMPLETE before moving on to the next step.\nOpen CodePipeline in the Management Console. You will see a CodePipeline that starts with eks-workshop-codepipeline. Click this link to view the details.\nIf you receive a permissions error similar to User x is not authorized to perform: codepipeline:ListPipelines\u0026hellip; upon clicking the above link, the CodePipeline console may have opened up in the wrong region. To correct this, from the Region dropdown in the console, choose the region you provisioned the workshop in. Select Oregon (us-west-2) if you provisioned the workshow per the \u0026ldquo;Start the workshop at an AWS event\u0026rdquo; instructions.\n Once you are on the detail page for the specific CodePipeline, you can see the status along with links to the change and build details.\nIf you click on the \u0026ldquo;details\u0026rdquo; link in the build/deploy stage, you can see the output from the CodeBuild process.\n To review the status of the deployment, you can run:\nkubectl describe deployment hello-k8s  For the status of the service, run the following command:\nkubectl describe service hello-k8s  Challenge: How can we view our exposed service?\nHINT: Which kubectl command will get you the Elastic Load Balancer (ELB) endpoint for this app?\n  Expand here to see the solution   Once the service is built and delivered, we can run the following command to get the Elastic Load Balancer (ELB) endpoint and open it in a browser. If the message is not updated immediately, give Kubernetes some time to deploy the change.\n kubectl get services hello-k8s -o wide  This service was configured with a LoadBalancer so, an AWS Elastic Load Balancer is launched by Kubernetes for the service. The EXTERNAL-IP column contains a value that ends with \u0026ldquo;elb.amazonaws.com\u0026rdquo; - the full value is the DNS address.\n   "
},
{
	"uri": "/monitoring/deploy-grafana/",
	"title": "Deploy Grafana",
	"tags": [],
	"description": "",
	"content": " Download Grafana and update configuration curl -o grafana-values.yaml https://raw.githubusercontent.com/helm/charts/master/stable/grafana/values.yaml  You will make three edits to grafana-values.yaml. Search for storageClassName, uncomment and change the value to \u0026ldquo;prometheus\u0026rdquo;. Search for adminPassword, uncomment and change the password to \u0026ldquo;EKS!sAWSome\u0026rdquo; or something similar. Make a note of this password as you will need it for logging into grafana dashboard later\nThe third edit you will do is for adding Prometheus as a datasource. Search for datasources.yaml and uncomment entire block, update prometheus to the endpoint referred earlier by helm response. The configuration will look similar to below\ndatasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true  Now let\u0026rsquo;s expose Grafana dashboard using AWS ELB service. Search for service:, and update the value of type: ClusterIP to type: LoadBalancer\n  Expand here to see the complete yaml   rbac: create: true pspEnabled: true serviceAccount: create: true name: replicas: 1 deploymentStrategy: RollingUpdate readinessProbe: httpGet: path: /api/health port: 3000 livenessProbe: httpGet: path: /api/health port: 3000 initialDelaySeconds: 60 timeoutSeconds: 30 failureThreshold: 10 image: repository: grafana/grafana tag: 5.3.1 pullPolicy: IfNotPresent ## Optionally specify an array of imagePullSecrets. ## Secrets must be manually created in the namespace. ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ ## # pullSecrets: # - myRegistrKeySecretName securityContext: runAsUser: 472 fsGroup: 472 downloadDashboardsImage: repository: appropriate/curl tag: latest pullPolicy: IfNotPresent ## Pod Annotations # podAnnotations: {} ## Deployment annotations # annotations: {} ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: type: LoadBalancer port: 80 annotations: {} labels: {} ingress: enabled: false annotations: {} # kubernetes.io/ingress.class: nginx # kubernetes.io/tls-acme: \u0026quot;true\u0026quot; labels: {} path: / hosts: - chart-example.local tls: [] # - secretName: chart-example-tls # hosts: # - chart-example.local resources: {} # limits: # cpu: 100m # memory: 128Mi # requests: # cpu: 100m # memory: 128Mi ## Node labels for pod assignment ## ref: https://kubernetes.io/docs/user-guide/node-selection/ # nodeSelector: {} ## Tolerations for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ ## tolerations: [] ## Affinity for pod assignment ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity ## affinity: {} ## Enable persistence using Persistent Volume Claims ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/ ## persistence: enabled: false storageClassName: prometheus # accessModes: # - ReadWriteOnce # size: 10Gi # annotations: {} # subPath: \u0026quot;\u0026quot; # existingClaim: adminUser: admin adminPassword: EKS!sAWSome ## Use an alternate scheduler, e.g. \u0026quot;stork\u0026quot;. ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/ ## # schedulerName: ## Extra environment variables that will be pass onto deployment pods env: {} ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment ## This can be useful for auth tokens, etc envFromSecret: \u0026quot;\u0026quot; ## Additional grafana server secret mounts # Defines additional mounts with secrets. Secrets must be manually created in the namespace. extraSecretMounts: [] # - name: secret-files # mountPath: /etc/secrets # secretName: grafana-secret-files # readOnly: true ## Pass the plugins you want installed as a list. ## plugins: [] # - digrich-bubblechart-panel # - grafana-clock-panel ## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-server.prometheus.svc.cluster.local access: proxy isDefault: true ## Configure grafana dashboard providers ## ref: http://docs.grafana.org/administration/provisioning/#dashboards ## ## `path` must be /var/lib/grafana/dashboards/\u0026lt;provider_name\u0026gt; ## dashboardProviders: {} # dashboardproviders.yaml: # apiVersion: 1 # providers: # - name: 'default' # orgId: 1 # folder: '' # type: file # disableDeletion: false # editable: true # options: # path: /var/lib/grafana/dashboards/default ## Configure grafana dashboard to import ## NOTE: To use dashboards you must also enable/configure dashboardProviders ## ref: https://grafana.com/dashboards ## ## dashboards per provider, use provider name as key. ## dashboards: {} # default: # some-dashboard: # json: | # $RAW_JSON # prometheus-stats: # gnetId: 2 # revision: 2 # datasource: Prometheus # local-dashboard: # url: https://example.com/repository/test.json ## Reference to external ConfigMap per provider. Use provider name as key and ConfiMap name as value. ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both. ## ConfigMap data example: ## ## data: ## example-dashboard.json: | ## RAW_JSON ## dashboardsConfigMaps: {} # default: \u0026quot;\u0026quot; ## Grafana's primary configuration ## NOTE: values in map will be converted to ini format ## ref: http://docs.grafana.org/installation/configuration/ ## grafana.ini: paths: data: /var/lib/grafana/data logs: /var/log/grafana plugins: /var/lib/grafana/plugins provisioning: /etc/grafana/provisioning analytics: check_for_updates: true log: mode: console grafana_net: url: https://grafana.net ## LDAP Authentication can be enabled with the following values on grafana.ini ## NOTE: Grafana will fail to start if the value for ldap.toml is invalid # auth.ldap: # enabled: true # allow_sign_up: true # config_file: /etc/grafana/ldap.toml ## Grafana's LDAP configuration ## Templated by the template in _helpers.tpl ## NOTE: To enable the grafana.ini must be configured with auth.ldap.enabled ## ref: http://docs.grafana.org/installation/configuration/#auth-ldap ## ref: http://docs.grafana.org/installation/ldap/#configuration ldap: # `existingSecret` is a reference to an existing secret containing the ldap configuration # for Grafana in a key `ldap-toml`. existingSecret: \u0026quot;\u0026quot; # `config` is the content of `ldap.toml` that will be stored in the created secret config: \u0026quot;\u0026quot; # config: |- # verbose_logging = true # [[servers]] # host = \u0026quot;my-ldap-server\u0026quot; # port = 636 # use_ssl = true # start_tls = false # ssl_skip_verify = false # bind_dn = \u0026quot;uid=%s,ou=users,dc=myorg,dc=com\u0026quot; ## Grafana's SMTP configuration ## NOTE: To enable, grafana.ini must be configured with smtp.enabled ## ref: http://docs.grafana.org/installation/configuration/#smtp smtp: # `existingSecret` is a reference to an existing secret containing the smtp configuration # for Grafana in keys `user` and `password`. existingSecret: \u0026quot;\u0026quot; ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards sidecar: image: kiwigrid/k8s-sidecar:0.0.3 imagePullPolicy: IfNotPresent resources: # limits: # cpu: 100m # memory: 100Mi # requests: # cpu: 50m # memory: 50Mi dashboards: enabled: false # label that the configmaps with dashboards are marked with label: grafana_dashboard # folder in the pod that should hold the collected dashboards folder: /tmp/dashboards datasources: enabled: false # label that the configmaps with datasources are marked with label: grafana_datasource    Deploy grafana helm install -f grafana-values.yaml stable/grafana --name grafana --namespace grafana  Run the command to check if Grafana is running properly\nkubectl get all -n grafana  You should see similar results. They should all be Ready and Available\nNAME READY STATUS RESTARTS AGE pod/grafana-b9697f8b5-t9w4j 1/1 Running 0 2m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana LoadBalancer 10.100.49.172 abe57f85de73111e899cf0289f6dc4a4-1343235144.us-west-2.elb.amazonaws.com 80:31570/TCP 3m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1 1 1 1 2m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-b9697f8b5 1 1 1 2m  You can get Grafana ELB URL using this command. Copy \u0026amp; Paste the value into browser to access Grafana web UI\nexport ELB=$(kubectl get svc -n grafana grafana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}') echo \u0026quot;http://$ELB\u0026quot;  "
},
{
	"uri": "/statefulset/services/",
	"title": "Create Services",
	"tags": [],
	"description": "",
	"content": " Introduction Kubernetes Service defines a logical set of Pods and a policy by which to access them. Service can be exposed in different ways by specifying a type in the serviceSpec. StatefulSet currently requires a Headless Service to control the domain of its Pods, directly reach each Pod with stable DNS entries. By specifying \u0026ldquo;None\u0026rdquo; for the clusterIP, you can create Headless Service.\nCreate Services Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/services.files/mysql-services.yml  Check the configuration of mysql-services.yml by following command.\ncat ~/environment/templates/mysql-services.yml  You can see the mysql service is for DNS resolution so that when pods are placed by StatefulSet controller, pods can be resolved using pod-name.mysql. mysql-read is a client service that does load balancing for all slaves.\n# Headless service for stable DNS entries of StatefulSet members. apiVersion: v1 kind: Service metadata: name: mysql labels: app: mysql spec: ports: - name: mysql port: 3306 clusterIP: None selector: app: mysql --- # Client service for connecting to any MySQL instance for reads. # For writes, you must instead connect to the master: mysql-0.mysql. apiVersion: v1 kind: Service metadata: name: mysql-read labels: app: mysql spec: ports: - name: mysql port: 3306 selector: app: mysql  Create service mysql and mysql-read by following command\nkubectl create -f ~/environment/templates/mysql-services.yml    Related files   mysql-services.yml  (0 ko)    "
},
{
	"uri": "/codepipeline/change/",
	"title": "Trigger New Release",
	"tags": [],
	"description": "",
	"content": " Update Our Application So far we have walked through setting up CI/CD for EKS using AWS CodePipeline and now we are going to make a change to the GitHub repository so that we can see a new release built and delivered.\nOpen GitHub and select the forked repository with the name eks-workshop-sample-api-service-go.\nClick on main.go file and then click on the edit button, which looks like a pencil.\nChange the text where it says \u0026ldquo;Hello World\u0026rdquo;, add a commit message and then click the \u0026ldquo;Commit changes\u0026rdquo; button.\nYou should leave the master branch selected.\nThe main.go application needs to be compiled, so please ensure that you don\u0026rsquo;t accidentally break the build :)\n After you modify and commit your change in GitHub, in approximately one minute you will see a new build triggered in the AWS Management Console Confirm the Change If you still have the ELB URL open in your browser, refresh to confirm the update. If you need to retrieve the URL again, use kubectl get services hello-k8s -o wide\n"
},
{
	"uri": "/codepipeline/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing the CI/CD with CodePipeline module.\nThis module is not used in subsequent steps, so you can remove the resources now, or at the end of the workshop.\nFirst we need to delete the Kubernetes deployment and service:\nkubectl delete deployments hello-k8s kubectl delete services hello-k8s  Next, we are going to delete the CloudFormation stack created. Open CloudFormation the AWS Management Console.\nCheck the box next to the eksws-codepipeline stack, select the Actions dropdown menu and then click Delete stack:\nNow we are going to delete the ECR repository:\nEmpty and then delete the S3 bucket used by CodeBuild for build artifacts (bucket name starts with eksws-codepipeline). First, select the bucket, then empty the bucket and finally delete the bucket:\nFinally, we are going to delete the IAM role created for CodeBuild to permit changes to the EKS cluster:\naws iam delete-role-policy --role-name EksWorkshopCodeBuildKubectlRole --policy-name eks-describe aws iam delete-role --role-name EksWorkshopCodeBuildKubectlRole  "
},
{
	"uri": "/advanced-networking/secondary_cidr/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Before we configure EKS, we need to enable secondary CIDR blocks in your VPC and make sure they have proper tags and route table configurations\nAdd secondary CIDRs to your VPC There are restrictions on the range of secondary CIDRs you can use to extend your VPC. For more info, see IPv4 CIDR Block Association Restrictions\n You can use below commands to add 100.64.0.0/16 to your EKS cluster VPC. Please note to change the Values parameter to EKS cluster name if you used different name than eksctl-eksworkshop\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') aws ec2 associate-vpc-cidr-block --vpc-id $VPC_ID --cidr-block 100.64.0.0/16  Next step is to create subnets. Before we do this step, let\u0026rsquo;s check how many subnets we are consuming. You can run this command to see EC2 instance and AZ details\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  I have 3 instances and using 3 subnets in my environment. For simplicity, we will use the same AZ\u0026rsquo;s and create 3 secondary CIDR subnets but you can certainly customize according to your networking requirements. Remember to change the AZ names according to your environment\nexport AZ1=us-east-2a export AZ2=us-east-2b export AZ3=us-east-2c CGNAT_SNET1=$(aws ec2 create-subnet --cidr-block 100.64.0.0/19 --vpc-id $VPC_ID --availability-zone $AZ1 | jq -r .Subnet.SubnetId) CGNAT_SNET2=$(aws ec2 create-subnet --cidr-block 100.64.32.0/19 --vpc-id $VPC_ID --availability-zone $AZ2 | jq -r .Subnet.SubnetId) CGNAT_SNET3=$(aws ec2 create-subnet --cidr-block 100.64.64.0/19 --vpc-id $VPC_ID --availability-zone $AZ3 | jq -r .Subnet.SubnetId)  Next step is to add Kubernetes tags on newer Subnets. You can check these tags by querying your current subnets\naws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 --output text  Output shows similar to this\nTAGS aws:cloudformation:logical-id SubnetPublicUSEAST2C TAGS kubernetes.io/role/elb 1 TAGS eksctl.cluster.k8s.io/v1alpha1/cluster-name eksworkshop-eksctl TAGS Name eksctl-eksworkshop-eksctl-cluster/SubnetPublicUSEAST2C TAGS aws:cloudformation:stack-name eksctl-eksworkshop-eksctl-cluster TAGS kubernetes.io/cluster/eksworkshop-eksctl shared TAGS aws:cloudformation:stack-id arn:aws:cloudformation:us-east-2:012345678901:stack/eksctl-eksworkshop-eksctl-cluster/8da51fc0-2b5e-11e9-b535-022c6f51bf82  Here are the commands to add tags to both the subnets\naws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET1 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET2 --tags Key=kubernetes.io/role/elb,Value=1 aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=eksctl.cluster.k8s.io/v1alpha1/cluster-name,Value=eksworkshop-eksctl aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/cluster/eksworkshop-eksctl,Value=shared aws ec2 create-tags --resources $CGNAT_SNET3 --tags Key=kubernetes.io/role/elb,Value=1  As next step, we need to associate three new subnets into a route table. Again for simplicity, we chose to add new subnets to the Public route table that has connectivity to Internet Gateway\nSNET1=$(aws ec2 describe-subnets --filters Name=cidr-block,Values=192.168.0.0/19 | jq -r .Subnets[].SubnetId) RTASSOC_ID=$(aws ec2 describe-route-tables --filters Name=association.subnet-id,Values=$SNET1 | jq -r .RouteTables[].RouteTableId) aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET1 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET2 aws ec2 associate-route-table --route-table-id $RTASSOC_ID --subnet-id $CGNAT_SNET3  "
},
{
	"uri": "/batch/jobs/",
	"title": "Kubernetes Jobs",
	"tags": [],
	"description": "",
	"content": " Kubernetes Jobs A job creates one or more pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.\nSave the below manifest as \u0026lsquo;job-whalesay.yaml\u0026rsquo; using your favorite editor.\napiVersion: batch/v1 kind: Job metadata: name: whalesay spec: template: spec: containers: - name: whalesay image: docker/whalesay command: [\u0026quot;cowsay\u0026quot;, \u0026quot;This is a Kubernetes Job!\u0026quot;] restartPolicy: Never backoffLimit: 4  Run a sample Kubernetes Job using the whalesay image.\nkubectl apply -f job-whalesay.yaml  Wait until the job has completed successfully.\nkubectl get job/whalesay  NAME DESIRED SUCCESSFUL AGE whalesay 1 1 2m  Confirm the output.\nkubectl logs -l job-name=whalesay  ___________________________ \u0026lt; This is a Kubernetes Job! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_appmesh/create_mesh/",
	"title": "Create the App Mesh",
	"tags": [],
	"description": "",
	"content": "We\u0026rsquo;ll first create the actual App Mesh. Copy and paste the following into your terminal to create an App Mesh called APP_MESH_DEMO.\naws appmesh create-mesh --mesh-name APP_MESH_DEMO  "
},
{
	"uri": "/servicemesh_with_istio/download/",
	"title": "Download and Install Istio CLI",
	"tags": [],
	"description": "",
	"content": "Before we can get started configuring Istio we’ll need to first install the command line tools that you will interact with. To do this run the following.\ncd ~/environment curl -L https://git.io/getLatestIstio | sh - // version can be different as istio gets upgraded cd istio-* sudo mv -v bin/istioctl /usr/local/bin/  "
},
{
	"uri": "/monitoring/dashboards/",
	"title": "Dashboards",
	"tags": [],
	"description": "",
	"content": " Create Dashboards Login into Grafana dashboard using credentials supplied during configuration\nYou will notice that \u0026lsquo;Install Grafana\u0026rsquo; \u0026amp; \u0026lsquo;create your first data source\u0026rsquo; are already completed. We will import community created dashboard for this tutorial\nClick \u0026lsquo;+\u0026rsquo; button on left panel and select \u0026lsquo;Import\u0026rsquo;\nEnter 3131 dashboard id under Grafana.com Dashboard \u0026amp; click \u0026lsquo;Load\u0026rsquo;.\nLeave the defaults, select \u0026lsquo;Prometheus\u0026rsquo; as the endpoint under prometheus data sources drop down, click \u0026lsquo;Import\u0026rsquo;.\nThis will show monitoring dashboard for all cluster nodes\nFor creating dashboard to monitor all pods, repeat same process as above and enter 3146 for dashboard id\n"
},
{
	"uri": "/introduction/basics/",
	"title": "쿠버네티스 (k8s) 기초",
	"tags": [],
	"description": "",
	"content": "이번 장에서는 다음의 주제들을 다룹니다.:\n 쿠버네티스란 무엇인가?   쿠버네티스 노드   K8s 오브젝트 개요   K8s 오브젝트 세부내용 (1/2)   K8s 오브젝트 세부내용 (2/2)   "
},
{
	"uri": "/deploy/deploycrystal/",
	"title": "크리스탈 백엔드 API 배포하기",
	"tags": [],
	"description": "",
	"content": "크리스탈 백엔드 API를 올려 봅시다!\n다음의 명령어를 귀하의 클라우드9 워크스페이스에 복사/붙여넣기 합니다.\ncd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  디폴로이먼트 상황을 확인하여 배포 진행 과정을 지켜 볼 수 있습니다.\nkubectl get deployment ecsdemo-crystal  "
},
{
	"uri": "/cleanup/undeploy/",
	"title": "Undeploy the applications",
	"tags": [],
	"description": "",
	"content": "To delete the resources created by the applications, we should delete the application deployments and kubernetes dashboard.\nNote that if you followed the cleanup section of the modules, some of the commands below might fail because there is nothing to delete and its ok.\nUndeploy the applications:\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml  "
},
{
	"uri": "/eksctl/launcheks/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": " DO NOT PROCEED with this step unless you have validated the IAM role in use by the Cloud9 IDE. You will not be able to run the necessary kubectl commands in the later modules unless the EKS cluster is built using the IAM role.\n Challenge: How do I check the IAM role on the workspace?\n  Expand here to see the solution   Run aws sts get-caller-identity and validate that your Arn contains eksworkshop-admin or modernizer-workshop-cl9  (or the role created when starting the workshop) and an Instance Id.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin/i-01234567890abcdef\u0026quot; } or { \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/modernizer-workshop-cl9/i-01234567890abcdef\u0026quot; }  If you do not see the correct role, please go back and validate the IAM role for troubleshooting.\nIf you do see the correct role, proceed to next step to create an EKS cluster.\n  Create an EKS cluster To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/tabs-example/tabs/eksctl/",
	"title": "Launch EKS",
	"tags": [],
	"description": "",
	"content": "To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n "
},
{
	"uri": "/spotworkers/deployhandler/",
	"title": "스팟 인터럽트 핸들러 배포",
	"tags": [],
	"description": "",
	"content": " 이 섹션에서는 클러스터가 스팟 중단에 대처하도록 준비해볼 예정입니다. 특정 인스턴스 유형의 사용 가능한 온디맨드 용량이 고갈되면 스팟 인스턴스에 2분 전에 중단 알림이 보내져 정상적으로 마무리됩니다. 각 스팟 인스턴스에 애플리케이션을 탐지하고 클러스터의 다른 곳에 재배포하는 역할을 가진 파드를 배포할 것입니다.\n우리가 해야 할 첫 번째 일은 각 스팟 인스턴스에 스팟 인터럽트 핸들러를 배치하는 것입니다. 이 핸들러는 중단 알림을 위해 인스턴스에서 EC2 메타 데이터 서비스를 모니터링합니다.\n워크플로우를 정리해보면:\n 스팟 인스턴스가 반환이 요청되었는지 확인합니다. 노드의 종료를 준비하기 위해 2분 알림 화면을 이용합니다. 새로운 파드가 노드에 위치하지 않도록 테인트 설정을 하고 저지선을 구축합니다. 실행중인 파드에 대한 연결이 하나씩 빠집니다. 원하는 개수를 유지하기 위해서 남아있는 노드에 파드가 재배포됩니다.  여기에 쿠버네티스 데몬셋 예제가 있습니다. 데몬셋은 노드당 하나의 파드를 실행합니다.\nmkdir ~/environment/spot cd ~/environment/spot wget https://eksworkshop.com/spot/managespot/deployhandler.files/spot-interrupt-handler-example.yml  적혀있는대로, 매니페스트는 할 필요가 없는 온디맨드를 포함한 모든 노드에 파드를 배치합니다. 스팟 인스턴스에만 배포되도록 데몬셋을 편집합니다. 레이블을 사용하여 올바른 노드를 식별해보세요.\n노드셀렉터를 사용하여 배치를 스팟 인스턴스로 제한하십시오. 자세한 내용은 링크를 참조하세요.\n도전 노드셀렉터를 사용하여 스팟 핸들러 구성   솔루션을 보려면 여기를 펼치세요   아래 코드를 Spec.Template.Spec.nodeSelector 아래의 데몬셋 매니페스트 끝에 작성합니다.\nnodeSelector: lifecycle: Ec2Spot   \n데몬셋을 배포합니다.\nkubectl apply -f ~/environment/spot/spot-interrupt-handler-example.yml  DaemonSet을 배포하는 중 오류가 발생하면 YAML 파일에 작은 오류가 있을 수 있습니다. 이 페이지 하단에 있는 솔루션 파일과 비교해 보세요.\n 파드를 확인해보세요. 각 스팟 노드 당 하나씩 있어야 합니다.\nkubectl get daemonsets    Related files   spot-interrupt-handler-example.yml  (1 ko)   spot-interrupt-handler-solution.yml  (1 ko)    "
},
{
	"uri": "/statefulset/statefulset/",
	"title": "Create StatefulSet",
	"tags": [],
	"description": "",
	"content": " Introduction StatefulSet consists of serviceName, replicas, template and volumeClaimTemplates. serviceName is \u0026ldquo;mysql\u0026rdquo;, headless service we created in previous section, replicas is 3, the desired number of pod, template is the configuration of pod, volumeClaimTemplates is to claim volume for pod based on storageClassName, gp2 that we created in \u0026ldquo;Define Storageclass\u0026rdquo; section. Percona Xtrabackup is in template to clone source MySQL server to its slaves.\nCreate StatefulSet Copy/Paste the following commands into your Cloud9 Terminal.\ncd ~/environment/templates wget https://eksworkshop.com/statefulset/statefulset.files/mysql-statefulset.yml  Create statefulset \u0026ldquo;mysql\u0026rdquo; by following command.\nkubectl create -f ~/environment/templates/mysql-statefulset.yml  Watch StatefulSet Watch the status of statefulset.\nkubectl get -w statefulset  It will take few minutes for pods to initialize and have statefulset created. DESIRED is the replicas number you define at StatefulSet.\nNAME DESIRED CURRENT AGE mysql 3 1 8s mysql 3 2 59s mysql 3 3 2m mysql 3 3 3m  Open another Cloud9 Terminal and watch the progress of pods creation using the following command.\nkubectl get pods -l app=mysql --watch  You can see ordered, graceful deployment with a stable, unique name for each pod.\nNAME READY STATUS RESTARTS AGE mysql-0 0/2 Init:0/2 0 30s mysql-0 0/2 Init:1/2 0 35s mysql-0 0/2 PodInitializing 0 47s mysql-0 1/2 Running 0 48s mysql-0 2/2 Running 0 59s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Pending 0 0s mysql-1 0/2 Init:0/2 0 0s mysql-1 0/2 Init:1/2 0 35s mysql-1 0/2 Init:1/2 0 45s mysql-1 0/2 PodInitializing 0 54s mysql-1 1/2 Running 0 55s mysql-1 2/2 Running 0 1m mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 \u0026lt;invalid\u0026gt; mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 32s mysql-2 0/2 Init:1/2 0 43s mysql-2 0/2 PodInitializing 0 50s mysql-2 1/2 Running 0 52s mysql-2 2/2 Running 0 56s  Press Ctrl+C to stop watching.\nCheck the dynamically created PVC by following command.\nkubectl get pvc -l app=mysql  You can see data-mysql-0,1,2 are created by STORAGECLASS mysql-gp2.\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d  (Optional) Check 10Gi 3 EBS volumes are created across availability zones at your AWS console.   Related files   mysql-statefulset.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/customize/",
	"title": "Customize Defaults",
	"tags": [],
	"description": "",
	"content": " If you look in the newly created eksdemo directory, you\u0026rsquo;ll see several files and directories. Specifically, inside the /templates directory, you\u0026rsquo;ll see:\n NOTES.txt: The “help text” for your chart. This will be displayed to your users when they run helm install. deployment.yaml: A basic manifest for creating a Kubernetes deployment service.yaml: A basic manifest for creating a service endpoint for your deployment _helpers.tpl: A place to put template helpers that you can re-use throughout the chart  We\u0026rsquo;re actually going to create our own files, so we\u0026rsquo;ll delete these boilerplate files\nrm -rf ~/environment/eksdemo/templates/ rm ~/environment/eksdemo/Chart.yaml rm ~/environment/eksdemo/values.yaml  Create a new Chart.yaml file which will describe the chart\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/Chart.yaml apiVersion: v1 appVersion: \u0026quot;1.0\u0026quot; description: A Helm chart for EKS Workshop Microservices application name: eksdemo version: 0.1.0 EoF  Next we\u0026rsquo;ll copy the manifest files for each of our microservices into the templates directory as servicename.yaml\n#create subfolders for each template type mkdir -p ~/environment/eksdemo/templates/deployment mkdir -p ~/environment/eksdemo/templates/service # Copy and rename frontend manifests cp ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/frontend.yaml cp ~/environment/ecsdemo-frontend/kubernetes/service.yaml ~/environment/eksdemo/templates/service/frontend.yaml # Copy and rename crystal manifests cp ~/environment/ecsdemo-crystal/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/crystal.yaml cp ~/environment/ecsdemo-crystal/kubernetes/service.yaml ~/environment/eksdemo/templates/service/crystal.yaml # Copy and rename nodejs manifests cp ~/environment/ecsdemo-nodejs/kubernetes/deployment.yaml ~/environment/eksdemo/templates/deployment/nodejs.yaml cp ~/environment/ecsdemo-nodejs/kubernetes/service.yaml ~/environment/eksdemo/templates/service/nodejs.yaml  All files in the templates directory are sent through the template engine. These are currently plain YAML files that would be sent to Kubernetes as-is.\nReplace hard-coded values with template directives Let\u0026rsquo;s replace some of the values with template directives to enable more customization by removing hard-coded values.\nOpen ~/environment/eksdemo/templates/deployment/frontend.yaml in your Cloud9 editor.\nThe following steps should be completed seperately for frontend.yaml, crystal.yaml, and nodejs.yaml.\n Under spec, find replicas: 1 and replace with the following:\nreplicas: {{ .Values.replicas }}  Under spec.template.spec.containers.image, replace the image with the correct template value from the table below:\n   Filename Value     frontend.yaml - image: {{ .Values.frontend.image }}:{{ .Values.version }}   crystal.yaml - image: {{ .Values.crystal.image }}:{{ .Values.version }}   nodejs.yaml - image: {{ .Values.nodejs.image }}:{{ .Values.version }}    Create a values.yaml file with our template defaults This file will populate our template directives with default values.\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/eksdemo/values.yaml # Default values for eksdemo. # This is a YAML-formatted file. # Declare variables to be passed into your templates. # Release-wide Values replicas: 3 version: 'latest' # Service Specific Values nodejs: image: brentley/ecsdemo-nodejs crystal: image: brentley/ecsdemo-crystal frontend: image: brentley/ecsdemo-frontend EoF  "
},
{
	"uri": "/logging/setup_es/",
	"title": "Provision an Elasticsearch Cluster",
	"tags": [],
	"description": "",
	"content": "This example creates a two instance Amazon Elasticsearch cluster named kubernetes-logs. This cluster is created in the same region as the Kubernetes cluster and CloudWatch log group.\nNote that this cluster has an open access policy which will need to be locked down in production environments.\n aws es create-elasticsearch-domain \\ --domain-name kubernetes-logs \\ --elasticsearch-version 6.3 \\ --elasticsearch-cluster-config \\ InstanceType=m4.large.elasticsearch,InstanceCount=2 \\ --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 \\ --access-policies '{\u0026quot;Version\u0026quot;:\u0026quot;2012-10-17\u0026quot;,\u0026quot;Statement\u0026quot;:[{\u0026quot;Effect\u0026quot;:\u0026quot;Allow\u0026quot;,\u0026quot;Principal\u0026quot;:{\u0026quot;AWS\u0026quot;:[\u0026quot;*\u0026quot;]},\u0026quot;Action\u0026quot;:[\u0026quot;es:*\u0026quot;],\u0026quot;Resource\u0026quot;:\u0026quot;*\u0026quot;}]}'  It takes a little while for the cluster to be created and arrive at an active state. The AWS Console should show the following status when the cluster is ready.\nYou could also check this via AWS CLI:\naws es describe-elasticsearch-domain --domain-name kubernetes-logs --query 'DomainStatus.Processing'  If the output value is false that means the domain has been processed and is now available to use.\nFeel free to move on to the next section for now.\n"
},
{
	"uri": "/scaling/test_hpa/",
	"title": "Scale an Application with HPA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an application and expose as a service on TCP port 80. The application is a custom-built image based on the php-apache image. The index.php page performs calculations to generate CPU load. More information can be found here\nkubectl run php-apache --image=k8s.gcr.io/hpa-example --requests=cpu=200m --expose --port=80  Create an HPA resource This HPA scales up when CPU exceeds 50% of the allocated container resource.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10  View the HPA using kubectl. You probably will see \u0026lt;unknown\u0026gt;/50% for 1-2 minutes and then you should be able to see 0%/50%\nkubectl get hpa  Generate load to trigger scaling Open a new terminal in the Cloud9 Environment and run the following command to drop into a shell on a new container\nkubectl run -i --tty load-generator --image=busybox /bin/sh  Execute a while loop to continue getting http:///php-apache\nwhile true; do wget -q -O - http://php-apache; done  In the previous tab, watch the HPA with the following command\nkubectl get hpa -w  You will see HPA scale the pods from 1 up to our configured maximum (10) until the CPU average is below our target (50%)\nYou can now stop (Ctrl + C) load test that was running in the other terminal. You will notice that HPA will slowly bring the replica count to min number based on its configuration. You should also get out of load testing application by pressing Ctrl + D\n"
},
{
	"uri": "/eksctl/",
	"title": "Launch using eksctl",
	"tags": [],
	"description": "",
	"content": " Launch using eksctl by Weaveworks We have some very powerful partner tools that allow us to automate much of the experience of creating an EKS cluster, simplifying the process.\nIn this module, we will highlight a tool contributed by Weaveworks called eksctl, based on the official AWS CloudFormation templates, and will use it to launch and configure our EKS cluster and nodes.\n  "
},
{
	"uri": "/prerequisites/sshkey/",
	"title": "SSH 키 생성하기",
	"tags": [],
	"description": "",
	"content": "여기에서부터, 아래처럼 박스에 보이는 명령어들을 Cloud9 IDE에 입력하게 됩니다. ** 클립 보드에 복사 ** 기능 (오른쪽 위 모서리)을 사용하여 간단하게 복사하여 Cloud9에 붙여 넣을 수 있습니다. 붙이기 위해서는 Windows의 경우 Ctrl + V를, Mac의 경우 Command + V를 사용할 수 있습니다.\n Cloud9에서 SSH 키를 생성하려면 아래 명령을 실행하세요. 필요한 경우 이 키는 작업자 노드 인스턴스에서 ssh 액세스를 허용하는 데 사용됩니다.\nssh-keygen  enter키를 세 차례 누르게 되면 기본 옵션으로 키가 생성됩니다.\n 공개 키를 사용중인 EC2 리전에 업로드하세요:\naws ec2 import-key-pair --key-name \u0026quot;eksworkshop\u0026quot; --public-key-material file://~/.ssh/id_rsa.pub  "
},
{
	"uri": "/prerequisites/k8stools/",
	"title": "쿠버네티스 도구 설치",
	"tags": [],
	"description": "",
	"content": " Amazon EKS 클러스터에는 쿠버네티스 클러스터에 대한 IAM 인증을 허용하기 위해 kubectl 및 kubelet 바이너리와 aws-iam-authenticator 바이너리가 필요합니다.\n이번 워크샵에서는 Linux 바이너리를 다운로드하는 명령어를 제공합니다. Mac OSX / Windows를 사용하는 경우 다운로드 링크는 공식 EKS 문서를 참조하십시오.\n kubectl 설정을 저장하기 위한 기본 ~/.kube 디렉토리 생성 mkdir -p ~/.kube  kubectl 설치 sudo curl --silent --location -o /usr/local/bin/kubectl \u0026quot;https://amazon-eks.s3-us-west-2.amazonaws.com/1.11.5/2018-12-06/bin/linux/amd64/kubectl\u0026quot; sudo chmod +x /usr/local/bin/kubectl  AWS IAM Authenticator 설치 go get -u -v github.com/kubernetes-sigs/aws-iam-authenticator/cmd/aws-iam-authenticator sudo mv ~/go/bin/aws-iam-authenticator /usr/local/bin/aws-iam-authenticator  설치 확인 kubectl version --short --client aws-iam-authenticator help  JQ 설치 sudo yum -y install jq  "
},
{
	"uri": "/prerequisites/clone/",
	"title": "실습 서비스 레포지토리 클론",
	"tags": [],
	"description": "",
	"content": "cd ~/environment git clone https://github.com/brentley/ecsdemo-frontend.git git clone https://github.com/brentley/ecsdemo-nodejs.git git clone https://github.com/brentley/ecsdemo-crystal.git  "
},
{
	"uri": "/monitoring/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Delete Prometheus and grafana helm delete prometheus helm del --purge prometheus helm delete grafana helm del --purge grafana  "
},
{
	"uri": "/deploy/servicetype/",
	"title": "서비스(service) 종류 확인",
	"tags": [],
	"description": "",
	"content": "프론트엔드 서비스를 올리기 전에 우리가 사용할 서비스(service) 종류를 살펴봅니다. 이것은 kubernetes/service.yaml으로 프론트엔드 서비스를 위한 설정입니다.\napiVersion: v1 kind: Service metadata: name: ecsdemo-frontend spec: selector: app: ecsdemo-frontend type: LoadBalancer ports: - protocol: TCP port: 80 targetPort: 3000  type: LoadBalancer을 주목하십시요. 이는 ELB를 설정하여 서비스로 들어오는 트래픽을 처리하게 합니다.\n백엔드 서비스를 위한 kubernetes/service.yaml과 비교합니다.\napiVersion: v1 kind: Service metadata: name: ecsdemo-nodejs spec: selector: app: ecsdemo-nodejs ports: - protocol: TCP port: 80 targetPort: 3000  프론트엔드와는 달리 특정 서비스 종류를 설정하지 않았습니다. 쿠버네티스 공식 문서에 따르면 기본 서비스 종류는 ClusterIP 입니다. 이는 클러스터 내부 IP를 서비스로 노출한다. 이 값을 선택하면 클러스터 내부에서만 해당 서비스에 접근할 수 있습니다.\n"
},
{
	"uri": "/statefulset/testmysql/",
	"title": "Test MySQL",
	"tags": [],
	"description": "",
	"content": "You can use mysql-client to send some data to the master, mysql-0.mysql by following command.\nkubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\ mysql -h mysql-0.mysql \u0026lt;\u0026lt;EOF CREATE DATABASE test; CREATE TABLE test.messages (message VARCHAR(250)); INSERT INTO test.messages VALUES ('hello, from mysql-client'); EOF  Run the following to test slaves (mysql-read) received the data.\nkubectl run mysql-client --image=mysql:5.7 -it --rm --restart=Never --\\ mysql -h mysql-read -e \u0026quot;SELECT * FROM test.messages\u0026quot;  The output should look like this.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  To test load balancing across slaves, run the following command.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  Each MySQL instance is assigned a unique identifier, and it can be retrieved using @@server_id. It will print the server id serving the request and the timestamp.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 12:44:57 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:44:58 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:44:59 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 12:45:00 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 12:45:01 | +-------------+---------------------+  Leave this open in a separate window while you test failure in the next section.\n"
},
{
	"uri": "/healthchecks/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Our Liveness Probe example used HTTP request and Readiness Probe executed a command to check health of a pod. Same can be accomplished using a TCP request as described in the documentation.\n kubectl delete -f ~/environment/healthchecks/liveness-app.yaml kubectl delete -f ~/environment/healthchecks/readiness-deployment.yaml  "
},
{
	"uri": "/prerequisites/self_paced/iamrole/",
	"title": "워크스페이스를 위한 IAM 역할 생성",
	"tags": [],
	"description": "",
	"content": " 이 딥 링크를 따라 관리자 권한으로 IAM 역할을 만듭니다. AWS Service 및 EC2가 선택되었는지 확인한 다음 Next를 클릭하여 권한을 봅니다. AdministratorAccess가 선택되어 있는지 확인한 다음 Next를 클릭하여 검토합니다. 이름을 eksworkshop-admin라고 입력하고, Create Role을 클릭하여 생성합니다.   "
},
{
	"uri": "/prerequisites/self_paced/ec2instance/",
	"title": "워크스페이스에 IAM 역할 부여하기",
	"tags": [],
	"description": "",
	"content": " 이 딥 링크를 따라 Cloud9 EC2 인스턴스를 찾습니다. 인스턴스를 선택하고 Actions / Instance Settings / Attach/Replace IAM Role 순서대로 메뉴를 선택합니다.  IAM Role 드랍 다운 리스트에서 eksworkshop-admin을 선택하고 적용합니다.   "
},
{
	"uri": "/deploy/servicerole/",
	"title": "ELB 서비스 롤 존재 확인",
	"tags": [],
	"description": "",
	"content": "이전에 로드 밸런서를 생성하지 않은 AWS 계정인 경우, ELB의 서비스 롤(role)이 아직 존재하지 않을 수 있습니다.\n롤을 확인하고 누락된 경우에는 만듭니다.\n다음의 명령어를 귀하의 클라우드9 워크스페이스에 복사/붙여넣기 합니다.\naws iam get-role --role-name \u0026quot;AWSServiceRoleForElasticLoadBalancing\u0026quot; || aws iam create-service-linked-role --aws-service-name \u0026quot;elasticloadbalancing.amazonaws.com\u0026quot;  "
},
{
	"uri": "/advanced-networking/secondary_cidr/configure-cni/",
	"title": "Configure CNI",
	"tags": [],
	"description": "",
	"content": " Before we start making changes to VPC CNI, let\u0026rsquo;s make sure we are using latest CNI version\nRun this command to find CNI version\nkubectl describe daemonset aws-node --namespace kube-system | grep Image | cut -d \u0026quot;/\u0026quot; -f 2  Here is a sample response\namazon-k8s-cni:1.2.1  Upgrade version to 1.3 if you have an older version\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Wait till all the pods are recycled. You can check the status of pods by using this command\nkubectl get pods -n kube-system -w  Configure Custom networking Edit aws-node configmap and add AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG environment variable to the node container spec and set it to true\nNote: You only need to add two lines into configmap\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: - name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Save the file and exit your text editor\nTerminate worker nodes so that Autoscaling launches newer nodes that come bootstrapped with custom network config\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  "
},
{
	"uri": "/batch/install/",
	"title": "Install Argo CLI",
	"tags": [],
	"description": "",
	"content": " Install Argo CLI Before we can get started configuring argo we\u0026rsquo;ll need to first install the command line tools that you will interact with. To do this run the following.\nsudo curl -sSL -o /usr/local/bin/argo https://github.com/argoproj/argo/releases/download/v2.2.1/argo-linux-amd64 sudo chmod +x /usr/local/bin/argo  "
},
{
	"uri": "/servicemesh_with_appmesh/create_virtual_nodes/",
	"title": "Create Virtual Nodes",
	"tags": [],
	"description": "",
	"content": " Next, we\u0026rsquo;ll create five Virtual Nodes, one for each of the microservices in our application.\nMore about Virtual Nodes A virtual node acts as a logical pointer to a k8s service.\nService Discovery / DNS name of the virtual node is defined in the serviceDiscovery.dns attribute.\nInbound traffic parameters for the virtual node are specified in the listener attribute.\nOutbound traffic the virtual node forwards to should be specified in the backend attribute.\ncolorgateway-vn We\u0026rsquo;ll first define the virtual node colorgateway-vn. colorgateway-vn will be the entrypoint to our app.\nThe following JSON defines:\n a virtual node named colorgateway-vn accessible via the hostname colorgateway.default.svc.cluster.local listening on port 9080 forwards to a k8s service (backend) named colorteller.default.svc.cluster.local  Copy and paste the following into your terminal to create the colorgateway virtual node:\naws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorgateway.default.svc.cluster.local\u0026quot; } }, \u0026quot;backends\u0026quot;: [ \u0026quot;colorteller.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorgateway-vn\u0026quot; }'  colorteller-vn collorteller-vn always white as its color response.\nThe following JSON defines:\n a virtual node named colorteller-vn accessible via the hostname colorteller.default.svc.cluster.local listening on port 9080  Copy and paste the following into your terminal to create the colorteller-vn virtual node:\naws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-vn\u0026quot; }'  Similarly, colorteller-black-vn, colorteller-blue-vn, and colorteller-red-vn return the colors black, blue, and red respectively.\nCopy and paste these three virtual node definitions into your terminal to create the black, blue, and red collorteller virtual nodes:\ncolorteller-black-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-black.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot; }'  colorteller-blue-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-blue.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-blue-vn\u0026quot; }'  colorteller-red-vn aws appmesh create-virtual-node --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;listeners\u0026quot;: [ { \u0026quot;portMapping\u0026quot;: { \u0026quot;port\u0026quot;: 9080, \u0026quot;protocol\u0026quot;: \u0026quot;http\u0026quot; } } ], \u0026quot;serviceDiscovery\u0026quot;: { \u0026quot;dns\u0026quot;: { \u0026quot;serviceName\u0026quot;: \u0026quot;colorteller-red.default.svc.cluster.local\u0026quot; } } }, \u0026quot;virtualNodeName\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot; }'  "
},
{
	"uri": "/servicemesh_with_istio/install/",
	"title": "Install Istio",
	"tags": [],
	"description": "",
	"content": " Install Istio\u0026rsquo;s CRD The Custom Resource Definition, also known as a CRD, is an API resource which allows you to define custom resources.\nkubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml  Install Istio Helm is required for the following examples. If you have not installed Helm yet, please first reference the Helm chapter before proceeding.\nkubectl create -f install/kubernetes/helm/helm-service-account.yaml helm template install/kubernetes/helm/istio --name istio --namespace istio-system --set global.configValidation=false --set sidecarInjectorWebhook.enabled=false --set grafana.enabled=true --set servicegraph.enabled=true \u0026gt; istio.yaml kubectl create namespace istio-system kubectl apply -f istio.yaml  Watch the progress of installation using:\nkubectl get pod -n istio-system -w  And hit CTRL-C when you\u0026rsquo;re ready to proceed.\nNAME READY STATUS RESTARTS AGE grafana-9cfc9d4c9-csvw7 1/1 Running 0 3m istio-citadel-6d7f9c545b-w7hjs 1/1 Running 0 3m istio-cleanup-secrets-vrkm5 0/1 Completed 0 3m istio-egressgateway-866885bb49-cz6jr 1/1 Running 0 3m istio-galley-6d74549bb9-t8sqb 1/1 Running 0 3m istio-grafana-post-install-4bgxv 0/1 Completed 0 3m istio-ingressgateway-6c6ffb7dc8-dnmqx 1/1 Running 0 3m istio-pilot-685fc95d96-jhfhv 2/2 Running 0 3m istio-policy-688f99c9c4-pb558 2/2 Running 0 3m istio-security-post-install-5dw8n 0/1 Completed 0 3m istio-telemetry-69b794ff59-spkp2 2/2 Running 0 3m prometheus-f556886b8-cxb9n 1/1 Running 0 3m servicegraph-778f94d6f8-tfmp6 1/1 Running 0 3m  "
},
{
	"uri": "/introduction/basics/what_is_k8s/",
	"title": "쿠버네티스란 무엇인가?",
	"tags": [],
	"description": "",
	"content": " 수십 년간의 경험과 모범 사례를 토대로 합니다. 선언형(declarative) 구성과 자동화를 지원합니다. 대규모 도구, 서비스, 지원 환경을 기반으로 합니다.  쿠버네티스에 대한 자세한 내용은 쿠버네티스 공식 웹사이트에서 찾아보실 수 있습니다.\n"
},
{
	"uri": "/spotworkers/preferspot/",
	"title": "스팟에 애플리케이션 배포하기",
	"tags": [],
	"description": "",
	"content": " 마이크로서비스 예제를 다시 설계합니다. 스팟 인스턴스가 사용 가능하다면 프론트앤드 서비스를 스팟에 배포하도록 합니다. 이 설정을 위해서 매니패스트 파일에 노드 어피니티(Node Affinity) 설정을 이용할 것입니다.\n노드 어피니티 및 톨러레이션 설정 Cloud9 에디터에서 디플로이먼트 메니페스트 파일을 엽니다. ~/environment/ecsdemo-frontend/kubernetes/deployment.yaml\n스펙을 편집하여 노트 어피니티를 스팟 인스턴스를 필수(require)가 아닌 선호(prefer)하도록 구성하십시오. 이렇게 하면 사용 가능한 스팟 인스턴스가 없거나 올바르게 레이블 된 경우 온디맨드 노드에 파드를 스케줄 할 수 있습니다.\n또한 포드가 EC2 스팟 인스턴스에서 구성한 테인트를 \u0026ldquo;허용\u0026rdquo;할 수 있도록 톨러레이션(허용 범위)을 구성하려고 합니다.\n노드 어피니티 예제는 다음 링크를 확인하세요.\n테인트와 톨러레이션 예제는 다음 링크를 확인하세요.\n도전 어피니티와 톨러레이션 설정\n  솔루션을 보려면 여기를 펼치세요   아래의 코드를 디플로이먼트 파일의 spec.template.spec 아래에 추가하세요.\naffinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: lifecycle operator: In values: - Ec2Spot tolerations: - key: \u0026quot;spotInstance\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;true\u0026quot; effect: \u0026quot;PreferNoSchedule\u0026quot;  아래에 솔루션 파일이 있습니다. 직접 작성한 코드와 비교해보세요.\n     Related files   deployment-solution.yml  (1 ko)    프론트앤드 서비스 스팟에 재배포 먼저 스팟 인스턴스에 배포된 모든 파드를 확인해보겠습니다.\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  이제 편집된 프론트앤드 메니페스트로 마이크로서비스를 재배포합니다.\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl apply -f kubernetes/service.yaml kubectl apply -f kubernetes/deployment.yaml  스팟 인스턴스에 배포된 모드 파드를 확인할 수 있으며, 프론트앤드 파드가 스팟 인스턴스 위에서 실행되는 것을 볼 수 있습니다.\nfor n in $(kubectl get nodes -l lifecycle=Ec2Spot --no-headers | cut -d \u0026quot; \u0026quot; -f1); do kubectl get pods --all-namespaces --no-headers --field-selector spec.nodeName=${n} ; done  "
},
{
	"uri": "/deploy/deployfrontend/",
	"title": "프론트엔드 서비스 배포하기",
	"tags": [],
	"description": "",
	"content": "루비 프론트엔드를 올려 봅시다!\n다음의 명령어를 귀하의 클라우드9 워크스페이스에 복사/붙여넣기 합니다.\ncd ~/environment/ecsdemo-frontend kubectl apply -f kubernetes/deployment.yaml kubectl apply -f kubernetes/service.yaml  디폴로이먼트 상황을 확인하여 배포 진행 과정을 지켜 볼 수 있습니다.\nkubectl get deployment ecsdemo-frontend  "
},
{
	"uri": "/cleanup/eksctl/",
	"title": "Delete the EKSCTL Cluster",
	"tags": [],
	"description": "",
	"content": "In order to delete the resources created for this EKS cluster, run the following commands:\nDelete the cluster:\neksctl delete cluster --name=eksworkshop-eksctl  The nodegroup will have to complete the deletion process before the EKS cluster can be deleted. The total process will take approximately 15 minutes, and can be monitored via the CloudFormation Console\n "
},
{
	"uri": "/eksctl/test/",
	"title": "Test the Cluster",
	"tags": [],
	"description": "",
	"content": " Confirm your Nodes:\nkubectl get nodes  Export the Worker Role Name for use throughout the workshop\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') echo \u0026quot;export ROLE_NAME=${ROLE_NAME}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile  Congratulations! You now have a fully working Amazon EKS Cluster that is ready to use!\n"
},
{
	"uri": "/scaling/deploy_ca/",
	"title": "Configure Cluster Autoscaler (CA)",
	"tags": [],
	"description": "",
	"content": " Cluster Autoscaler for AWS provides integration with Auto Scaling groups. It enables users to choose from four different options of deployment:\n One Auto Scaling group - This is what we will use Multiple Auto Scaling groups Auto-Discovery Master Node setup  Configure the Cluster Autoscaler (CA) We have provided a manifest file to deploy the CA. Copy the commands below into your Cloud9 Terminal.\nmkdir ~/environment/cluster-autoscaler cd ~/environment/cluster-autoscaler wget https://eksworkshop.com/scaling/deploy_ca.files/cluster_autoscaler.yml  Configure the ASG We will need to provide the name of the Autoscaling Group that we want CA to manipulate. Collect the name of the Auto Scaling Group (ASG) containing your worker nodes. Record the name somewhere. We will use this later in the manifest file.\nYou can find it in the console by following this link.\nCheck the box beside the ASG and click Actions and Edit\nChange the following settings:\n Min: 2 Max: 8  Click Save\nConfigure the Cluster Autoscaler Using the file browser on the left, open cluster_autoscaler.yml\nSearch for command: and within this block, replace the placeholder text \u0026lt;AUTOSCALING GROUP NAME\u0026gt; with the ASG name that you copied in the previous step. Also, update AWS_REGION value to reflect the region you are using and Save the file.\ncommand: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --nodes=2:8:eksctl-eksworkshop-eksctl-nodegroup-0-NodeGroup-SQG8QDVSR73G env: - name: AWS_REGION value: us-east-1  This command contains all of the configuration for the Cluster Autoscaler. The primary config is the --nodes flag. This specifies the minimum nodes (2), max nodes (8) and ASG Name.\nAlthough Cluster Autoscaler is the de facto standard for automatic scaling in K8s, it is not part of the main release. We deploy it like any other pod in the kube-system namespace, similar to other management pods.\nCreate an IAM Policy We need to configure an inline policy and add it to the EC2 instance profile of the worker nodes\nCollect the Instance Profile and Role NAME from the CloudFormation Stack\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName')  mkdir ~/environment/asg_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/asg_policy/k8s-asg-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;autoscaling:DescribeAutoScalingGroups\u0026quot;, \u0026quot;autoscaling:DescribeAutoScalingInstances\u0026quot;, \u0026quot;autoscaling:SetDesiredCapacity\u0026quot;, \u0026quot;autoscaling:TerminateInstanceInAutoScalingGroup\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker --policy-document file://~/environment/asg_policy/k8s-asg-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker  Deploy the Cluster Autoscaler kubectl apply -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml  Watch the logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  We are now ready to scale our cluster   Related files   cluster_autoscaler.yml  (3 ko)    "
},
{
	"uri": "/logging/deploy/",
	"title": "Deploy Fluentd",
	"tags": [],
	"description": "",
	"content": "mkdir ~/environment/fluentd cd ~/environment/fluentd wget https://eksworkshop.com/logging/deploy.files/fluentd.yml  Explore the fluentd.yml to see what is being deployed. There is a link at the bottom of this page. The Fluentd log agent configuration is located in the Kubernetes ConfigMap. Fluentd will be deployed as a DaemonSet, i.e. one pod per worker node. In our case, a 3 node cluster is used and so 3 pods will be shown in the output when we deploy.\nUpdate REGION and CLUSTER_NAME environment variables in fluentd.yml as required. They are set to us-west-2 and eksworkshop-eksctl by default.\n kubectl apply -f ~/environment/fluentd/fluentd.yml  Watch for all of the pods to change to running status\nkubectl get pods -w --namespace=kube-system  We are now ready to check that logs are arriving in CloudWatch Logs\nSelect the region that is mentioned in fluentd.yml to browse the Cloudwatch Log Group if required.\n  Related files   fluentd.yml  (5 ko)    "
},
{
	"uri": "/helm_root/helm_micro/deploy/",
	"title": "Deploy the eksdemo Chart",
	"tags": [],
	"description": "",
	"content": " Use the dry-run flag to test our templates To test the syntax and validity of the Chart without actually deploying it, we\u0026rsquo;ll use the dry-run flag.\nThe following command will build and output the rendered templates without installing the Chart:\nhelm install --debug --dry-run --name workshop ~/environment/eksdemo  Confirm that the values created by the template look correct.\nDeploy the chart Now that we have tested our template, lets install it.\nhelm install --name workshop ~/environment/eksdemo  Similar to what we saw previously in the NGINX Helm Chart example, an output of the Deployment, Pod, and Service objects are output, similar to:\nNAME: workshop LAST DEPLOYED: Fri Nov 16 21:42:00 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1/Service NAME AGE ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Deployment ecsdemo-crystal 0s ecsdemo-frontend 0s ecsdemo-nodejs 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE ecsdemo-crystal-764b9cb9bc-4dwqt 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-hcb62 0/1 ContainerCreating 0 0s ecsdemo-crystal-764b9cb9bc-vl7nr 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-2xrtb 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-bfnc5 0/1 ContainerCreating 0 0s ecsdemo-frontend-67876457f6-rb6rg 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-994cq 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-9qtbm 0/1 ContainerCreating 0 0s ecsdemo-nodejs-c458bf55d-s9zkh 0/1 ContainerCreating 0 0s  "
},
{
	"uri": "/statefulset/testfailure/",
	"title": "Test Failure",
	"tags": [],
	"description": "",
	"content": " Unhealthy container MySQL container uses readiness probe by running mysql -h 127.0.0.1 -e \u0026lsquo;SELECT 1\u0026rsquo; on the server to make sure MySQL server is still active. Open a new terminal and simulate MySQL as being unresponsive by following command.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off  This command renames the /usr/bin/mysql command so that readiness probe can\u0026rsquo;t find it. During the next health check, the pod should report one of it\u0026rsquo;s containers is not healthy. This can be verified by following command.\nkubectl get pod mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 1/2 Running 0 12m  mysql-read load balancer detects failures and takes action by not sending traffic to the failed container, @@server_id 102. You can check this by the loop running in separate window from previous section. The loop shows the following output.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:00:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:00:49 | +-------------+---------------------+  Revert back to its initial state at the previous terminal.\nkubectl exec mysql-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql  Check the status again to see that both containers are running and healthy\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Running 0 5h  The loop in another terminal is now showing @@server_id 102 is back and all three servers are running. Press Ctrl+C to stop watching.\nFailed pod To simulate a failed pod, delete mysql-2 pod by following command.\nkubectl delete pod mysql-2  pod \u0026quot;mysql-2\u0026quot; deleted  StatefulSet controller recognizes failed pod and creates a new one to maintain the number of replicas with them same name and link to the same PersistentVolumeClaim.\n$ kubectl get pod -w mysql-2  NAME READY STATUS RESTARTS AGE mysql-2 2/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Terminating 0 1d mysql-2 0/2 Pending 0 0s mysql-2 0/2 Pending 0 0s mysql-2 0/2 Init:0/2 0 0s mysql-2 0/2 Init:1/2 0 10s mysql-2 0/2 PodInitializing 0 11s mysql-2 1/2 Running 0 12s mysql-2 2/2 Running 0 16s  Press Ctrl+C to stop watching.\n"
},
{
	"uri": "/dashboard/connect/",
	"title": "대시보드 사용",
	"tags": [],
	"description": "",
	"content": "이제 쿠버네티스 대시보드를 사용할 수 있습니다.\n 귀하의 클라우드9 환경에서 Tools / Preview / Preview Running Application 을 클릭합니다. URL의 끝으로 가서 다음을 추가합니다.  /api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/  터미널 탭을 열고 다음을 입력합니다.\naws-iam-authenticator token -i eksworkshop-eksctl --token-only  이 명령의 출력을 복사하고 Token 옆에 라디오버튼을 클릭 하고, 아래 텍스트 필드에 복사한 마지막 명령의 출력을 붙여넣습니다.\n그리고 Sign In 버튼을 누릅니다.\n전체 탭에서 대시보드를 보고 싶으면, 아래처럼 Pop Out 버튼을 클릭하세요. "
},
{
	"uri": "/prerequisites/workspaceiam/",
	"title": "워크스페이스의 IAM 설정 업데이트",
	"tags": [],
	"description": "",
	"content": " Cloud9는 일반적으로 IAM 자격 증명을 동적으로 관리합니다. 현재 aws-iam-authenticator 플러그인과 호환되지 않으므로 이를 비활성화하고 대신 IAM 역할을 사용합니다.\n  작업 공간으로 돌아가서 우측상단의 톱니바퀴 버튼을 클릭하거나 새 탭을 실행하여 환경설정 탭을 엽니다. AWS SETTINGS을 선택합니다. AWS managed temporary credentials 설정을 끕니다. 환경설정 탭을 닫습니다.   임시 자격 증명이 없는지 확실히 하기 위해 기존의 자격 증명 파일도 제거합니다 :\nrm -vf ${HOME}/.aws/credentials  현재 리전을 기준으로 aws-cli를 구성해야 합니다 :\nexport AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region') echo \u0026quot;export AWS_REGION=${AWS_REGION}\u0026quot; \u0026gt;\u0026gt; ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region   IAM 역할 확인 GetCallerIdentity CLI 명령을 사용하여 Cloud9 IDE가 올바른 IAM 역할을 사용하는지 확인합니다.\n먼저 AWS CLI에서 IAM 역할 이름을 가져옵니다.\nINSTANCE_PROFILE_NAME=`basename $(aws ec2 describe-instances --filters Name=tag:Name,Values=aws-cloud9-${C9_PROJECT}-${C9_PID} | jq -r '.Reservations[0].Instances[0].IamInstanceProfile.Arn' | awk -F \u0026quot;/\u0026quot; \u0026quot;{print $2}\u0026quot;)` aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME --query \u0026quot;InstanceProfile.Roles[0].RoleName\u0026quot; --output text  역할 이름이 출력됩니다.\neksworkshop-admin or modernizer-workshop-cl9  그 결과를 다음과 비교해보세요.\naws sts get-caller-identity  유효한 경우 Arn 이 위의 역할 이름과 인스턴스 ID를 포함하는 경우 계속 진행할 수 있습니다.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/eksworkshop-admin/i-01234567890abcdef\u0026quot; } or { \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/modernizer-workshop-cl9/i-01234567890abcdef\u0026quot; }  문제가 있는 경우 Arn 이 TeamRole, MasterRole을 포함하거나 역할 이름과 일치하지 않으면 더 이상 진행하지 마세요. 돌아가서 이 페이지의 지시사항을 다시 확인하십시오.\n{ \u0026quot;Account\u0026quot;: \u0026quot;123456789012\u0026quot;, \u0026quot;UserId\u0026quot;: \u0026quot;AROA1SAMPLEAWSIAMROLE:i-01234567890abcdef\u0026quot;, \u0026quot;Arn\u0026quot;: \u0026quot;arn:aws:sts::123456789012:assumed-role/TeamRole/MasterRole\u0026quot; }  "
},
{
	"uri": "/dashboard/",
	"title": "쿠버네티스 대시보드 배포",
	"tags": [],
	"description": "",
	"content": " 쿠버네티스 대시보드를 배포합니다. 이번 장에서는 쿠버네티스 공식 대시보드를 배포하고, 귀하의 Cloud9를 통해 접속합니다.\n"
},
{
	"uri": "/deploy/",
	"title": "마이크로서비스 예제 배포",
	"tags": [],
	"description": "",
	"content": " 예제 마이크로 서비스를 배포해봅시다.  예제 애플리케이션 배포   NodeJS 백엔드 API 배포   크리스탈 백엔드 API 배포하기   서비스(service) 종류 확인   ELB 서비스 롤 존재 확인   프론트엔드 서비스 배포하기   서비스(service) 주소 찾기   백엔드 서비스 스케일링   프론트엔드 스케일링   애플리케이션 정리   "
},
{
	"uri": "/spotworkers/",
	"title": "EKS에서 스팟 인스턴스 사용",
	"tags": [],
	"description": "",
	"content": " EKS에서 스팟 인스턴스 사용 이 모듈에서는 Spot 인스턴스를 이용하여, 다양한 규모의 Amazon EKS로 Kubernetes 클러스터를 프로비저닝, 관리 및 운영하는 비용 및 규모를 최적화하는 방법을 학습합니다. "
},
{
	"uri": "/helm_root/",
	"title": "Helm",
	"tags": [],
	"description": "",
	"content": " Kubernetes Helm Helm is a package manager for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nIn this chapter, we\u0026rsquo;ll cover installing Helm. Once installed, we\u0026rsquo;ll demonstrate how Helm can be used to deploy a simple NGINX webserver, and a more sophisticated microservice.\n"
},
{
	"uri": "/statefulset/testscaling/",
	"title": "Test Scaling",
	"tags": [],
	"description": "",
	"content": " More slaves can be added to the MySQL Cluster to increase read capacity. This can be done by following command.\nkubectl scale statefulset mysql --replicas=5  You can see the message that statefulset \u0026ldquo;mysql\u0026rdquo; scaled.\nstatefulset \u0026quot;mysql\u0026quot; scaled  Watch the progress of ordered and graceful scaling.\nkubectl get pods -l app=mysql -w  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 24m mysql-3 0/2 Init:0/2 0 8s mysql-3 0/2 Init:1/2 0 9s mysql-3 0/2 PodInitializing 0 11s mysql-3 1/2 Running 0 12s mysql-3 2/2 Running 0 16s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Pending 0 0s mysql-4 0/2 Init:0/2 0 0s mysql-4 0/2 Init:1/2 0 10s mysql-4 0/2 PodInitializing 0 11s mysql-4 1/2 Running 0 12s mysql-4 2/2 Running 0 17s  It may take few minutes to launch all the pods.\n Press Ctrl+C to stop watching. Open another terminal to check loop if you closed it.\nkubectl run mysql-client-loop --image=mysql:5.7 -i -t --rm --restart=Never --\\ bash -ic \u0026quot;while sleep 1; do mysql -h mysql-read -e 'SELECT @@server_id,NOW()'; done\u0026quot;  You will see 5 servers are running.\n+-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:42 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 102 | 2018-11-14 13:56:43 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:44 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:45 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 104 | 2018-11-14 13:56:46 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 101 | 2018-11-14 13:56:47 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 100 | 2018-11-14 13:56:48 | +-------------+---------------------+ +-------------+---------------------+ | @@server_id | NOW() | +-------------+---------------------+ | 103 | 2018-11-14 13:56:49 | +-------------+---------------------+  Verify if the newly deployed slave (mysql-3) have the same data set by following command.\nkubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\ mysql -h mysql-3.mysql -e \u0026quot;SELECT * FROM test.messages\u0026quot;  It will show the same data that master has.\n+--------------------------+ | message | +--------------------------+ | hello, from mysql-client | +--------------------------+  Scale down replicas to 3 by following command.\nkubectl scale statefulset mysql --replicas=3  You can see statefulset \u0026ldquo;mysql\u0026rdquo; scaled\nstatefulset \u0026quot;mysql\u0026quot; scaled  Note that scale in doesn\u0026rsquo;t delete the data or PVCs attached to the pods. You have to delete them manually. Check scale in is completed by following command.\nkubectl get pods -l app=mysql  NAME READY STATUS RESTARTS AGE mysql-0 2/2 Running 0 1d mysql-1 2/2 Running 0 1d mysql-2 2/2 Running 0 35m  Check 2 PVCs(data-mysql-3, data-mysql-4) still exist by following command.\nkubectl get pvc -l app=mysql  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE data-mysql-0 Bound pvc-83e9dfeb-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-1 Bound pvc-977e7806-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-2 Bound pvc-b3009b02-e721-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 1d data-mysql-3 Bound pvc-de14acd8-e811-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 34m data-mysql-4 Bound pvc-e916c3ec-e812-11e8-86c5-069628ef0c9c 10Gi RWO mysql-gp2 26m  Challenge: By default, deleting a PersistentVolumeClaim will delete its associated persistent volume. What if you wanted to keep the volume? Change the reclaim policy of the PersistentVolume associated with PVC \u0026ldquo;data-mysql-3\u0026rdquo; to \u0026ldquo;Retain\u0026rdquo;. Please see Kubernetes documentation for help\n  Expand here to see the solution   Change the reclaim policy:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Retain\u0026quot;}}'  Now, if you delete the PersistentVolumeClaim data-mysql-3, you can still see the EBS volume in your AWS EC2 console, with its state as \u0026ldquo;available\u0026rdquo;.\nLet\u0026rsquo;s change the reclaim policy back to \u0026ldquo;Delete\u0026rdquo; to avoid orphaned volumes:\nkubectl patch pv \u0026lt;your-pv-name\u0026gt; -p '{\u0026quot;spec\u0026quot;:{\u0026quot;persistentVolumeReclaimPolicy\u0026quot;:\u0026quot;Delete\u0026quot;}}'    Delete data-mysql-3, data-mysql-4 by following command.\nkubectl delete pvc data-mysql-3 kubectl delete pvc data-mysql-4  persistentvolumeclaim \u0026quot;data-mysql-3\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-4\u0026quot; deleted  "
},
{
	"uri": "/advanced-networking/secondary_cidr/eniconfig_crd/",
	"title": "Create CRDs",
	"tags": [],
	"description": "",
	"content": " Create custom resources for ENIConfig CRD As next step, we will add custom resources to ENIConfig custom resource definition (CRD). CRD\u0026rsquo;s are extensions of Kubernetes API that stores collection of API objects of certain kind. In this case, we will store VPC Subnet and SecurityGroup configuration information in these CRD\u0026rsquo;s so that Worker nodes can access them to configure VPC CNI plugin.\nYou should have ENIConfig CRD already installed with latest CNI version (1.3+). You can check if its installed by running this command.\nkubectl get crd  You should see response similar to this\nNAME CREATED AT eniconfigs.crd.k8s.amazonaws.com 2019-03-07T20:06:48Z  If you don\u0026rsquo;t have ENIConfig installed, you can install it by using this command\nkubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/v1.3/aws-k8s-cni.yaml  Create custom resources for each subnet by replacing Subnet and SecurityGroup IDs. Since we created three secondary subnets, we need create three custom resources.\nHere is the template for custom resource. Notice the values for Subnet ID and SecurityGroup ID needs to be replaced with appropriate values\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: $SUBNETID1 securityGroups: - $SECURITYGROUPID1 - $SECURITYGROUPID2  Check the AZ\u0026rsquo;s and Subnet IDs for these subnets. Make note of AZ info as you will need this when you apply annotation to Worker nodes using custom network config\naws ec2 describe-subnets --filters \u0026quot;Name=cidr-block,Values=100.64.*\u0026quot; --query 'Subnets[*].[CidrBlock,SubnetId,AvailabilityZone]' --output table  -------------------------------------------------------------- | DescribeSubnets | +-----------------+----------------------------+-------------+ | 100.64.32.0/19 | subnet-07dab05836e4abe91 | us-east-2a | | 100.64.64.0/19 | subnet-0692cd08cc4df9b6a | us-east-2c | | 100.64.0.0/19 | subnet-04f960ffc8be6865c | us-east-2b | +-----------------+----------------------------+-------------+  Check your Worker Node SecurityGroup\nINSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text`) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;SecurityGroup for EC2 instance $i ...\u0026quot; aws ec2 describe-instances --instance-ids $INSTANCE_IDS | jq -r '.Reservations[].Instances[].SecurityGroups[].GroupId' done  SecurityGroup for EC2 instance i-03ea1a083c924cd78 ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-0a635aed890c7cc3e ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef SecurityGroup for EC2 instance i-048e5ec8815e5ea8a ... sg-070d03008bda531ad sg-06e5cab8e5d6f16ef  Create custom resource group1-pod-netconfig.yaml for first subnet (100.64.0.0/19). Replace the SubnetId and SecuritGroupIds with the values from above. Here is how it looks with the configuration values for my environment\nNote: We are using same SecurityGroup for pods as your Worker Nodes but you can change these and use custom SecurityGroups for your Pod Networking\napiVersion: crd.k8s.amazonaws.com/v1alpha1 kind: ENIConfig metadata: name: group1-pod-netconfig spec: subnet: subnet-04f960ffc8be6865c securityGroups: - sg-070d03008bda531ad - sg-06e5cab8e5d6f16ef  Create custom resource group2-pod-netconfig.yaml for second subnet (100.64.32.0/19). Replace the SubnetId and SecuritGroupIds as above.\nSimilarly, create custom resource group3-pod-netconfig.yaml for third subnet (100.64.64.0/19). Replace the SubnetId and SecuritGroupIds as above.\nCheck the instance details using this command as you will need AZ info when you apply annotation to Worker nodes using custom network config\naws ec2 describe-instances --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --query 'Reservations[*].Instances[*].[PrivateDnsName,Tags[?Key==`Name`].Value|[0],Placement.AvailabilityZone,PrivateIpAddress,PublicIpAddress]' --output table  ------------------------------------------------------------------------------------------------------------------------------------------ | DescribeInstances | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+ | ip-192-168-9-228.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2c | 192.168.9.228 | 18.191.57.131 | | ip-192-168-71-211.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2a | 192.168.71.211 | 18.221.77.249 | | ip-192-168-33-135.us-east-2.compute.internal | eksworkshop-eksctl-ng-475d4bc8-Node | us-east-2b | 192.168.33.135 | 13.59.167.90 | +-----------------------------------------------+---------------------------------------+-------------+-----------------+----------------+  Apply the CRD\u0026rsquo;s\nkubectl apply -f group1-pod-netconfig.yaml kubectl apply -f group2-pod-netconfig.yaml kubectl apply -f group3-pod-netconfig.yaml  As last step, we will annotate nodes with custom network configs.\nBe sure to annotate the instance with config that matches correct AZ. For ex, in my environment instance ip-192-168-33-135.us-east-2.compute.internal is in us-east-2b. So, I will apply group1-pod-netconfig.yaml to this instance. Similarly, I will apply group2-pod-netconfig.yaml to ip-192-168-71-211.us-east-2.compute.internal and group3-pod-netconfig.yaml to ip-192-168-9-228.us-east-2.compute.internal\n kubectl annotate node \u0026lt;nodename\u0026gt;.\u0026lt;region\u0026gt;.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  As an example, here is what I would run in my environment\nkubectl annotate node ip-192-168-33-135.us-east-2.compute.internal k8s.amazonaws.com/eniConfig=group1-pod-netconfig  You should now see secondary IP address from extended CIDR assigned to annotated nodes.\n"
},
{
	"uri": "/batch/deploy/",
	"title": "Deploy Argo",
	"tags": [],
	"description": "",
	"content": " Deploy Argo Argo run in its own namespace and deploys as a CustomResourceDefinition.\nDeploy the Controller and UI.\nkubectl create ns argo kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml  namespace/argo created customresourcedefinition.apiextensions.k8s.io/workflows.argoproj.io created serviceaccount/argo created serviceaccount/argo-ui created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-admin created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-edit created clusterrole.rbac.authorization.k8s.io/argo-aggregate-to-view created clusterrole.rbac.authorization.k8s.io/argo-cluster-role created clusterrole.rbac.authorization.k8s.io/argo-ui-cluster-role created clusterrolebinding.rbac.authorization.k8s.io/argo-binding created clusterrolebinding.rbac.authorization.k8s.io/argo-ui-binding created configmap/workflow-controller-configmap created service/argo-ui created deployment.apps/argo-ui created deployment.apps/workflow-controller created  To use advanced features of Argo for this demo, create a RoleBinding to grant admin privileges to the \u0026lsquo;default\u0026rsquo; service account.\nThis is for demo purposes only. In any other environment, you should use Workflow RBAC to set appropriate permissions.\n kubectl create rolebinding default-admin --clusterrole=admin --serviceaccount=default:default  "
},
{
	"uri": "/servicemesh_with_appmesh/create_virtual_routers/",
	"title": "Create Virtual Routers and Routes",
	"tags": [],
	"description": "",
	"content": " Virtual routers handle traffic for one or more service names within your mesh. After you create a virtual router, you can create and associate routes for your virtual router that direct incoming requests to different virtual nodes.\nNext, we\u0026rsquo;ll create two Virtual Routers.\nCreating the routers Each service name within the mesh must be fronted by a virtual router, and the service name you specify for the virtual router must be a real DNS service name within your VPC. In most cases you should just use the same service name that you specified for your virtual nodes.\nThe following JSON represents a virtual router called colorgateway-vr, for the service name colorgateway.default.svc.cluster.local.\nCopy and paste the following into your terminal to create colorgateway-vr:\naws appmesh create-virtual-router --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;serviceNames\u0026quot;: [ \u0026quot;colorgateway.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorgateway-vr\u0026quot; }'  Similarly, for the virtual node colorteller-vn, copy and paste the following into your terminal to create colorteller-vr:\naws appmesh create-virtual-router --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;spec\u0026quot;: { \u0026quot;serviceNames\u0026quot;: [ \u0026quot;colorteller.default.svc.cluster.local\u0026quot; ] }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  Next we\u0026rsquo;ll add routes to these virtual routers.\nCreating the Routes A route is associated with a virtual router, and it is used to match requests for a virtual router and distribute traffic accordingly to its associated virtual nodes.\nYou can use the prefix parameter in your route specification for path-based routing of requests. For example, if your virtual router service name is my-service.local, and you want the route to match requests to my-service.local/metrics, then your prefix should be /metrics.\nIf your route matches a request, you can distribute traffic to one or more target virtual nodes with relative weighting.\nThe following JSON represents a route called colorgateway-route, for the virtual router colorgateway-vr.\nThis route directs 100% of traffic to colorgateway-vn on requests matching the / prefix.\nCopy and paste the following into your terminal to create colorgateway-route:\naws appmesh create-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorgateway-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorgateway-vn\u0026quot;, \u0026quot;weight\u0026quot;: 100 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorgateway-vr\u0026quot; }'  Similarly, for the virtual router colorteller-vr, copy and paste the following into your terminal to create colorteller-route:\naws appmesh create-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-vn\u0026quot;, \u0026quot;weight\u0026quot;: 1 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  "
},
{
	"uri": "/servicemesh_with_istio/deploy/",
	"title": "Deploy Sample Apps",
	"tags": [],
	"description": "",
	"content": " Now that we have all the resources installed for Istio, we will use sample application called BookInfo to review key capabilities of the service mesh such as intelligent routing, and review telemetry data using Prometheus \u0026amp; Grafana.\nSample Apps The Bookinfo application is broken into four separate microservices:\n productpage\n The productpage microservice calls the details and reviews microservices to populate the page.  details\n The details microservice contains book information.  reviews\n The reviews microservice contains book reviews. It also calls the ratings microservice.  ratings\n The ratings microservice contains book ranking information that accompanies a book review.   There are 3 versions of the reviews microservice:\n Version v1\n doesn’t call the ratings service.  Version v2\n calls the ratings service, and displays each rating as 1 to 5 black stars.  Version v3\n calls the ratings service, and displays each rating as 1 to 5 red stars.   Deploy Sample Apps Deploy sample apps by manually injecting istio proxy and confirm pods, services are running correctly\nkubectl apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)  The output from\nkubectl get pod,svc  Should look similar to:\nNAME READY STATUS RESTARTS AGE details-v1-64558cf56b-dxbx2 2/2 Running 0 14s productpage-v1-5b796957dd-hqllk 2/2 Running 0 14s ratings-v1-777b98fcc4-5bfr8 2/2 Running 0 14s reviews-v1-866dcb7ff-k69jm 2/2 Running 0 14s reviews-v2-6d7959c9d-5ppnc 2/2 Running 0 14s reviews-v3-7ddf94f545-m7vls 2/2 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.100.102.153 \u0026lt;none\u0026gt; 9080/TCP 17s kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 138d productpage ClusterIP 10.100.222.154 \u0026lt;none\u0026gt; 9080/TCP 17s ratings ClusterIP 10.100.1.63 \u0026lt;none\u0026gt; 9080/TCP 17s reviews ClusterIP 10.100.255.157 \u0026lt;none\u0026gt; 9080/TCP 17s  Next we\u0026rsquo;ll define the virtual service and ingress gateway:\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  Next, we\u0026rsquo;ll query the DNS name of the ingress gateway and use it to connect via the browser.\nkubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' -n istio-system ; echo  This may take a minute or two, first for the Ingress to be created, and secondly for the Ingress to hook up with the services it exposes.\nTo test, do the following:\n Open a new browser tab Paste the DNS endpoint returned from the previous get service istiogateway command Add /productpage to the end of that DNS endpoint Hit enter to retrieve the page.  Remember to add /productpage to the end of the URI in the browser to see the sample webpage!\n Click reload multiple times to see how the layout and content of the reviews changes as differnt versions (v1, v2, v3) of the app are called.\n"
},
{
	"uri": "/introduction/basics/concepts_nodes/",
	"title": "쿠버네티스 노드",
	"tags": [],
	"description": "",
	"content": "쿠버네티스 클러스터를 구성하는 머신들은 노드(node) 라고 불립니다.\n쿠버네티스 클러스터 내 노드들은 물리적이거나 가상 머신일 수 있습니다.\n노드에는 두가지 종류가 있습니다:\n 콘트롤 플레인(Control Plane)을 형성하며, 클러스터의 \u0026ldquo;두뇌\u0026rdquo;로 역할하는 마스터 노드(Master-node).\n 데이터 플레인(Data Plane)을 형성하며, 파드(pod)들을 통해 실제 컨테이너 이미지들을 작동시키는 워커 노드(Worker-node).\n  우리는 프레젠테이션을 통해 어떻게 노드들이 서로 상호작용하는지 자세히 알아볼 것입니다.\n"
},
{
	"uri": "/deploy/viewservices/",
	"title": "서비스(service) 주소 찾기",
	"tags": [],
	"description": "",
	"content": "이제type : LoadBalancer로 실행중인 서비스(service)가 있으니, ELB의 주소를 알아봐야 합니다. 이를 위해 kubectl로 get services 하면 됩니다.\nkubectl get service ecsdemo-frontend  위 명령어 출력은 ELB의 FQDN을 보여줄 만큼 필드가 길지 않습니다. 아래 명령으로 출력 형식을 조정할 수 있습니다.\nkubectl get service ecsdemo-frontend -o wide  이 데이터를 프로그램적으로 사용하기 원한다면, json 형식으로 출력할 수 있습니다. 다음은 json 출력을 사용하는 예제 입니다.\nELB=$(kubectl get service ecsdemo-frontend -o json | jq -r '.status.loadBalancer.ingress[].hostname') curl -m3 -v $ELB  ELB가 준비되고 트래픽을 프론트엔드 파드로 전송하기까지는 몇십초가 걸립니다.\n loadBalancer 호스트네임을 웹 브라우저에 카피/붙여넣기해서 우리의 애플리케이션이 실행중인지 확인합니다. 다음 페이지에서 서비스를 스케일 업할 때까지 이 탭을 열어둡니다.\n"
},
{
	"uri": "/statefulset/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " First delete the StatefulSet. This will also terminates the pods. It may take some while.\nkubectl delete statefulset mysql  Verify there are no pods running by following command.\nkubectl get pods -l app=mysql  No resources found.  Delete ConfigMap, Service and PVC by following command.\nkubectl delete configmap,service,pvc -l app=mysql  configmap \u0026quot;mysql-config\u0026quot; deleted service \u0026quot;mysql\u0026quot; deleted service \u0026quot;mysql-read\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-0\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-1\u0026quot; deleted persistentvolumeclaim \u0026quot;data-mysql-2\u0026quot; deleted  Congratulation! You\u0026rsquo;ve finished the StatefulSets lab. "
},
{
	"uri": "/logging/configurecwl/",
	"title": "Configure CloudWatch Logs and Kibana",
	"tags": [],
	"description": "",
	"content": " Configure CloudWatch Logs Subscription CloudWatch Logs can be delivered to other services such as Amazon Elasticsearch for custom processing. This can be achieved by subscribing to a real-time feed of log events. A subscription filter defines the filter pattern to use for filtering which log events gets delivered to Elasticsearch, as well as information about where to send matching log events to.\nIn this section, we’ll subscribe to the CloudWatch log events from the fluent-cloudwatch stream from the eks/eksworkshop-eksctl log group. This feed will be streamed to the Elasticsearch cluster.\nOriginal instructions for this are available at:\nhttp://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html\nCreate Lambda Basic Execution Role\ncat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/iam_policy/lambda.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Principal\u0026quot;: { \u0026quot;Service\u0026quot;: \u0026quot;lambda.amazonaws.com\u0026quot; }, \u0026quot;Action\u0026quot;: \u0026quot;sts:AssumeRole\u0026quot; } ] } EoF aws iam create-role --role-name lambda_basic_execution --assume-role-policy-document file://~/environment/iam_policy/lambda.json aws iam attach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole  Go to the CloudWatch Logs console\nSelect the log group /eks/eksworkshop-eksctl/containers. Click on Actions and select Stream to Amazon ElasticSearch Service. Select the ElasticSearch Cluster kubernetes-logs and IAM role lambda_basic_execution\nClick Next\nSelect Common Log Format and click Next\nReview the configuration. Click Next and then Start Streaming\nCloudwatch page is refreshed to show that the filter was successfully created\nConfigure Kibana In Amazon Elasticsearch console, select the kubernetes-logs under My domains\nOpen the Kibana dashboard from the link. After a few minutes, records will begin to be indexed by ElasticSearch. You\u0026rsquo;ll need to configure an index patterns in Kibana.\nSet Index Pattern as cwl-* and click Next\nSelect @timestamp from the dropdown list and select Create index pattern\nClick on Discover and explore your logs\n"
},
{
	"uri": "/scaling/test_ca/",
	"title": "Scale a Cluster with CA",
	"tags": [],
	"description": "",
	"content": " Deploy a Sample App We will deploy an sample nginx application as a ReplicaSet of 1 Pod\ncat \u0026lt;\u0026lt;EoF\u0026gt; ~/environment/cluster-autoscaler/nginx.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-to-scaleout spec: replicas: 1 template: metadata: labels: service: nginx app: nginx spec: containers: - image: nginx name: nginx-to-scaleout resources: limits: cpu: 500m memory: 512Mi requests: cpu: 500m memory: 512Mi EoF kubectl apply -f ~/environment/cluster-autoscaler/nginx.yaml kubectl get deployment/nginx-to-scaleout  Scale our ReplicaSet OK, let\u0026rsquo;s scale out the replicaset to 10\nkubectl scale --replicas=10 deployment/nginx-to-scaleout  Some pods will be in the Pending state, which triggers the cluster-autoscaler to scale out the EC2 fleet.\nkubectl get pods -o wide --watch  NAME READY STATUS RESTARTS AGE nginx-to-scaleout-7cb554c7d5-2d4gp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-2nh69 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-45mqz 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-4qvzl 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5jddd 1/1 Running 0 34s nginx-to-scaleout-7cb554c7d5-5sx4h 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-5xbjp 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-6l84p 0/1 Pending 0 11s nginx-to-scaleout-7cb554c7d5-7vp7l 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-86pr6 0/1 Pending 0 12s nginx-to-scaleout-7cb554c7d5-88ttw 0/1 Pending 0 12s  View the cluster-autoscaler logs\nkubectl logs -f deployment/cluster-autoscaler -n kube-system  You will notice Cluster Autoscaler events similar to below Check the AWS Management Console to confirm that the Auto Scaling groups are scaling up to meet demand. This may take a few minutes. You can also follow along with the pod deployment from the command line. You should see the pods transition from pending to running as nodes are scaled up.\n"
},
{
	"uri": "/helm_root/helm_micro/service/",
	"title": "Test the Service",
	"tags": [],
	"description": "",
	"content": "To test the service our eksdemo Chart created, we\u0026rsquo;ll need to get the name of the ELB endpoint that was generated when we deployed the Chart:\nkubectl get svc ecsdemo-frontend -o jsonpath=\u0026quot;{.status.loadBalancer.ingress[*].hostname}\u0026quot;; echo  Copy that address, and paste it into a new tab in your browser. You should see something similar to:\n"
},
{
	"uri": "/healthchecks/",
	"title": "Health Checks",
	"tags": [],
	"description": "",
	"content": " Health Checks By default, Kubernetes will restart a container if it crashes for any reason. It uses Liveness and Readiness probes which can be configured for running a robust application by identifying the healthy containers to send traffic to and restarting the ones when required.\nIn this section, we will understand how liveness and readiness probes are defined and test the same against different states of a pod. Below is the high level description of how these probes work.\nLiveness probes are used in Kubernetes to know when a pod is alive or dead. A pod can be in a dead state for different reasons while Kubernetes kills and recreates the pod when liveness probe does not pass.\nReadiness probes are used in Kubernetes to know when a pod is ready to serve traffic. Only when the readiness probe passes, a pod will receive traffic from the service. When readiness probe fails, traffic will not be sent to a pod until it passes.\nWe will review some examples in this module to understand different options for configuring liveness and readiness probes.\n"
},
{
	"uri": "/scaling/",
	"title": "Autoscaling our Applications and Clusters",
	"tags": [],
	"description": "",
	"content": " Implement AutoScaling with HPA and CA In this Chapter, we will show patterns for scaling your worker nodes and applications deployments automatically. Automatic scaling in K8s comes in two forms:\n Horizontal Pod Autoscaler (HPA) scales the pods in a deployment or replica set. It is implemented as a K8s API resource and a controller. The controller manager queries the resource utilization against the metrics specified in each HorizontalPodAutoscaler definition. It obtains the metrics from either the resource metrics API (for per-pod resource metrics), or the custom metrics API (for all other metrics).\n Cluster Autoscaler (CA) is the default K8s component that can be used to perform pod scaling as well as scaling nodes in a cluster. It automatically increases the size of an Auto Scaling group so that pods have a place to run. And it attempts to remove idle nodes, that is, nodes with no running pods.\n  "
},
{
	"uri": "/codepipeline/",
	"title": "CI/CD with CodePipeline",
	"tags": [],
	"description": "",
	"content": " CI/CD with CodePipeline Continuous integration (CI) and continuous delivery (CD) are essential for deft organizations. Teams are more productive when they can make discrete changes frequently, release those changes programmatically and deliver updates without disruption.\nIn this module, we will build a CI/CD pipeline using AWS CodePipeline. The CI/CD pipeline will deploy a sample Kubernetes service, we will make a change to the GitHub repository and observe the automated delivery of this change to the cluster.\n"
},
{
	"uri": "/x-ray/",
	"title": "Tracing with X-Ray",
	"tags": [],
	"description": "",
	"content": " Tracing with X-Ray As distributed systems evolve, monitoring and debugging services becomes challenging. Container-orchestration platforms like Kubernetes solve a lot of problems, but they also introduce new challenges for developers and operators in understanding how services interact and where latency exists. AWS X-Ray helps developers analyze and debug distributed services.\nIn this module, we are going to deploy the X-Ray agent as a DaemonSet, deploy sample front-end and back-end services that are instrumented with the X-Ray SDK, make some sample requests and then examine the traces and service maps in the AWS Management Console.\n"
},
{
	"uri": "/batch/",
	"title": "Batch Processing with Argo",
	"tags": [],
	"description": "",
	"content": " Batch Processing In this Chapter, we will deploy common batch processing scenarios using Kubernetes and Argo.\nWhat is Argo? Argo is an open source container-native workflow engine for getting work done on Kubernetes. Argo is implemented as a Kubernetes CRD (Custom Resource Definition).\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo workflows on Kubernetes.  "
},
{
	"uri": "/advanced-networking/secondary_cidr/test_networking/",
	"title": "Test Networking",
	"tags": [],
	"description": "",
	"content": " Launch pods into Secondary CIDR network Let\u0026rsquo;s launch few pods and test networking\nkubectl run nginx --image=nginx kubectl scale --replicas=3 deployments/nginx kubectl expose deployment/nginx --type=NodePort --port 80 kubectl get pods -o wide  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE nginx-64f497f8fd-k962k 1/1 Running 0 40m 100.64.6.147 ip-192-168-52-113.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-lkslh 1/1 Running 0 40m 100.64.53.10 ip-192-168-74-125.us-east-2.compute.internal \u0026lt;none\u0026gt; nginx-64f497f8fd-sgz6f 1/1 Running 0 40m 100.64.80.186 ip-192-168-26-65.us-east-2.compute.internal \u0026lt;none\u0026gt;  You can use busybox pod and ping pods within same host or across hosts using IP address\nkubectl run -i --rm --tty debug --image=busybox -- sh  Test access to internet and to nginx service\n# connect to internet / # wget google.com -O - Connecting to google.com (172.217.5.238:80) Connecting to www.google.com (172.217.5.228:80) \u0026lt;!doctype html\u0026gt;\u0026lt;html itemscope=\u0026quot;\u0026quot; itemtype=\u0026quot;http://schema.org/WebPage\u0026quot; lang=\u0026quot;en\u0026quot;\u0026gt;\u0026lt;head\u0026gt;\u0026lt;meta content=\u0026quot;Search the world's information, including webpages, images, videos and more. Google has many special ... # connect to service (testing core-dns) / # wget nginx -O - Connecting to nginx (10.100.170.156:80) \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ...  "
},
{
	"uri": "/batch/artifact/",
	"title": "Configure Artifact Repository",
	"tags": [],
	"description": "",
	"content": " Configure Artifact Repository Argo uses an artifact repository to pass data between jobs in a workflow, known as artifacts. Amazon S3 can be used as an artifact repository.\nLet\u0026rsquo;s create a S3 bucket using the AWS CLI.\nACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 mb s3://batch-artifact-repository-${ACCOUNT_ID}/  Next, edit the workflow-controller ConfigMap to use the S3 bucket.\nkubectl edit -n argo configmap/workflow-controller-configmap  Add the following lines to the end of the ConfigMap, substituting your Account ID for {{ACCOUNT_ID}}:\ndata: config: | artifactRepository: s3: bucket: batch-artifact-repository-{{ACCOUNT_ID}} endpoint: s3.amazonaws.com  Create an IAM Policy In order for Argo to read from/write to the S3 bucket, we need to configure an inline policy and add it to the EC2 instance profile of the worker nodes.\nCollect the Instance Profile, Role name, and Account ID from the CloudFormation Stack.\nINSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)  Create and policy and attach to the worker node role.\nmkdir ~/environment/batch_policy cat \u0026lt;\u0026lt;EoF \u0026gt; ~/environment/batch_policy/k8s-s3-policy.json { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;s3:*\u0026quot; ], \u0026quot;Resource\u0026quot;: [ \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}\u0026quot;, \u0026quot;arn:aws:s3:::batch-artifact-repository-${ACCOUNT_ID}/*\u0026quot; ] } ] } EoF aws iam put-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker --policy-document file://~/environment/batch_policy/k8s-s3-policy.json  Validate that the policy is attached to the role\naws iam get-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  "
},
{
	"uri": "/servicemesh_with_istio/routing/",
	"title": "Intelligent Routing",
	"tags": [],
	"description": "",
	"content": " Intelligent Routing Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, and more in a consistent manner across the services, and the application.\nBefore you can use Istio to control the Bookinfo version routing, you\u0026rsquo;ll need to define the available versions, called subsets, in destination rules.\nService versions (a.k.a. subsets) - In a continuous deployment scenario, for a given service, there can be distinct subsets of instances running different variants of the application binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Common scenarios where this occurs include A/B testing, canary rollouts, etc. The choice of a particular version can be decided based on various criterion (headers, url, etc.) and/or by weights assigned to each version. Each service has a default version consisting of all its instances.\n kubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml kubectl get destinationrules -o yaml  To route to one version only, you apply virtual services that set the default version for the microservices. In this case, the virtual services will route all traffic to reviews:v1 of microservice.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1  Try now to reload the page multiple times, and note how only version 1 of reviews is displayed each time.\nNext, we\u0026rsquo;ll change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named Jason will be routed to the service reviews:v2.\nkubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml kubectl get virtualservices reviews -o yaml  The subset is set to v1 in default and route v2 if the logged user is match with \u0026lsquo;jason\u0026rsquo; for reviews request.\nspec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1  To test, click Sign in from the top right corner of the page, and login using jason as user name with a blank password. You will only see reviews:v2 all the time. Others will see reviews:v1.\nTo test for resiliency, inject a 7s delay between the reviews:v2 and ratings microservices for user jason. This test will uncover a bug that was intentionally introduced into the Bookinfo app.\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 in default and added 7s delay for all the request if the logged user is match with \u0026lsquo;jason\u0026rsquo; for ratings.\nspec: hosts: - ratings http: - fault: delay: fixedDelay: 7s percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  Logout, then click Sign in from the top right corner of the page, using jason as the user name with a blank password. You will see the delays and it ends up display error for reviews. Others will see reviews without error.\nThe timeout between the productpage and the reviews service is 6 seconds - coded as 3s + 1 retry for 6s total.\nTo test for another resiliency, introduce an HTTP abort to the ratings microservices for the test user jason. The page will immediately display the “Ratings service is currently unavailable”\nkubectl apply -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml kubectl get virtualservice ratings -o yaml  The subset is set to v1 and by default returns an error message of \u0026ldquo;Ratings service is currently unavailable\u0026rdquo; below the reviewer name if the logged username matches \u0026lsquo;jason\u0026rsquo;.\nspec: hosts: - ratings http: - fault: abort: httpStatus: 500 percent: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: - destination: host: ratings subset: v1  To test, click Sign in from the top right corner of the page and login using jason for the user name with a blank password. As jason you will see the error message. Others (not logged in as jason) will see no error message.\nNext, we\u0026rsquo;ll demonstrate how to gradually migrate traffic from one version of a microservice to another. In our example, we\u0026rsquo;ll send 50% of traffic to reviews:v1 and 50% to reviews:v3.\nkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml kubectl get virtualservice reviews -o yaml  The subset is set to 50% of traffic to v1 and 50% of traffic to v3 for all reviews request.\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50  To test it, refresh your browser over and over, and you\u0026rsquo;ll see only reviews:v1 and reviews:v3.\n"
},
{
	"uri": "/servicemesh_with_appmesh/create_k8s_app/",
	"title": "Create the k8s app",
	"tags": [],
	"description": "",
	"content": " Up to this point, we\u0026rsquo;ve created all required components of the app mesh (virtual nodes, virtual routers, and routes) to support our application. In this chapter, we\u0026rsquo;ll actually deploy the k8s application.\nDeploying the k8s Colorteller App To deploy app, copy and paste the following into your terminal:\nkubectl apply -f https://raw.githubusercontent.com/geremyCohen/colorapp/master/colorapp.yaml  Deploying Curler In addition to deploying the application, we\u0026rsquo;ll also deploy Curler, which is a simple image that provide curl functionality. To deploy the curler pods, copy and paste the following:\nkubectl run -it curler --image=tutum/curl /bin/bash  "
},
{
	"uri": "/servicemesh_with_appmesh/testing_the_app_mesh/",
	"title": "Test the App on App Mesh",
	"tags": [],
	"description": "",
	"content": " We now have both App Mesh and the application deployed. Next comes the fun part of seeing how we can use App Mesh to change the charactertics of the application.\nOpen Two Terminals You should already have one terminal open. Click the widget on the Cloud9 GUI and open a second terminal.\nUse curler to fetch the application response In the first terminal, run the following command to connect to the curler pod with a bash shell (you may be prompted to hit enter to get a command line prompt):\nkubectl run -it curler --image=tutum/curl /bin/bash  Next, with a shell open to the curler pod, paste the following to repeatedly request the colorgateway service:\nwhile [ 1 ]; do curl -s --connect-timeout 2 http://colorgateway.default.svc.cluster.local:9080/color;echo;sleep 1; done  Every second, you should see the response white.\nThis is because colorgateway always forwards to colorteller, which via the colorteller-route, always routes to colorteller-vn (which will always respond with white).\nLet\u0026rsquo;s modify the colorteller-route so it instead routes to the blue, red, and black colorteller virtual nodes, each at a 30% weighted ratio.\nModify colorteller-route To modify colorteller-route, copy and paste the following into the other terminal (the one that is not making the curl requests):\naws appmesh update-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-blue-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot;, \u0026quot;weight\u0026quot;: 3 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  If you look at the curler terminal, you should now see an equal distribution of traffic to the blue, red, and black virtual nodes.\nFor fun, to see a 50\u0026frasl;50 weighted response of only the red and black virtual nodes, copy and paste the following route:\naws appmesh update-route --mesh-name APP_MESH_DEMO --cli-input-json '{ \u0026quot;routeName\u0026quot;: \u0026quot;colorteller-route\u0026quot;, \u0026quot;spec\u0026quot;: { \u0026quot;httpRoute\u0026quot;: { \u0026quot;action\u0026quot;: { \u0026quot;weightedTargets\u0026quot;: [ { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-red-vn\u0026quot;, \u0026quot;weight\u0026quot;: 5 }, { \u0026quot;virtualNode\u0026quot;: \u0026quot;colorteller-black-vn\u0026quot;, \u0026quot;weight\u0026quot;: 5 } ] }, \u0026quot;match\u0026quot;: { \u0026quot;prefix\u0026quot;: \u0026quot;/\u0026quot; } } }, \u0026quot;virtualRouterName\u0026quot;: \u0026quot;colorteller-vr\u0026quot; }'  "
},
{
	"uri": "/introduction/basics/concepts_objects/",
	"title": "K8s 오브젝트 개요",
	"tags": [],
	"description": "",
	"content": "쿠버네티스의 오브젝트(objects)는 클러스터의 상태를 나타내는 단위(entities)입니다.\n오브젝트는 \u0026ldquo;의도를 담은 레코드\u0026rdquo;입니다. 생성된 클러스터는 그 의도대로 존재할 수 있도록 최선을 다합니다. 이는 클러스터의 \u0026ldquo;의도한 상태(desired state)\u0026ldquo;라고 알려져 있습니다.\n쿠버네티스는 항상 오브젝트의 \u0026ldquo;현재 상태\u0026rdquo;를 \u0026ldquo;의도한 상태\u0026rdquo;와 동일하게 만들게끔 작동합니다. 이때 의도한 상태란 다음과 같습니다.\n 어떤 파드(컨테이너)들이 어느 노드에서 동작(running) 중인지 컨테이너들의 논리 그룹과 매핑된 IP 엔드포인트 동작 중인 컨테이너 레플리카(replicas)의 개수 기타 다수 상태들\u0026hellip;  k8s 오브젝트들에 대해 조금 더 자세히 알아봅시다\u0026hellip;\n"
},
{
	"uri": "/deploy/scalebackend/",
	"title": "백엔드 서비스 스케일링",
	"tags": [],
	"description": "",
	"content": "서비스 시작시에 각각의 컨테이너를 딱 1개씩만 런칭했었습니다. 이는 운영 중인 파드를 살펴보면 확인할 수 있습니다.\nkubectl get deployments  이제 백엔드 서비스를 스케일 업 합니다.\nkubectl scale deployment ecsdemo-nodejs --replicas=3 kubectl scale deployment ecsdemo-crystal --replicas=3  디폴로이먼트를 다시 확인해봅니다.\nkubectl get deployments  또한 브라우저에서 실행 중인 우리 애플리케이션을 확인합니다. 이제는 여러 백엔드 서비스로 트래픽이 흐르는 것을 볼 수 있어야 합니다.\n"
},
{
	"uri": "/logging/cleanup/",
	"title": "Cleanup Logging",
	"tags": [],
	"description": "",
	"content": "cd ~/environment INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') kubectl delete -f ~/environment/fluentd/fluentd.yml rm -rf ~/environment/fluentd/ aws es delete-elasticsearch-domain --domain-name kubernetes-logs aws logs delete-log-group --log-group-name /eks/eksworkshop-eksctl/containers aws iam delete-role-policy --role-name $ROLE_NAME --policy-name Logs-Policy-For-Worker aws iam detach-role-policy --role-name lambda_basic_execution --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole aws iam delete-role --role-name lambda_basic_execution rm -rf ~/environment/iam_policy/  "
},
{
	"uri": "/scaling/cleanup/",
	"title": "Cleanup Scaling",
	"tags": [],
	"description": "",
	"content": "kubectl delete -f ~/environment/cluster-autoscaler/cluster_autoscaler.yml kubectl delete -f ~/environment/cluster-autoscaler/nginx.yaml rm -rf ~/environment/cluster-autoscaler aws iam delete-role-policy --role-name $ROLE_NAME --policy-name ASG-Policy-For-Worker kubectl delete hpa,svc php-apache kubectl delete deployment php-apache load-generator  "
},
{
	"uri": "/helm_root/helm_micro/rolling_back/",
	"title": "Rolling Back",
	"tags": [],
	"description": "",
	"content": " Mistakes will happen during deployment, and when they do, Helm makes it easy to undo, or \u0026ldquo;roll back\u0026rdquo; to the previously deployed version.\nUpdate the demo application chart with a breaking change Open values.yaml and modify the image name under nodejs.image to brentley/ecsdemo-nodejs-non-existing. This image does not exist, so this will break our deployment.\nDeploy the updated demo application chart:\nhelm upgrade workshop ~/environment/eksdemo  The rolling upgrade will begin by creating a new nodejs pod with the new image. The new ecsdemo-nodejs Pod should fail to pull non-existing image. Run helm status command to see the ImagePullBackOff error:\nhelm status workshop  Rollback the failed upgrade Now we are going to rollback the application to the previous working release revision.\nFirst, list Helm release revisions:\nhelm history workshop  Then, rollback to the previous application revision (can rollback to any revision too):\n# rollback to the 1st revision helm rollback workshop 1  Validate workshop release status with:\nhelm status workshop  "
},
{
	"uri": "/cleanup/workspace/",
	"title": "Cleanup the Workspace",
	"tags": [],
	"description": "",
	"content": "Since we no longer need the Cloud9 instance to have Administrator access to our account, we can delete the role we created:\n Go to the IAM Console Click Delete role in the upper right corner  Finally, let\u0026rsquo;s delete our Cloud9 EC2 Instance:\n Go to your Cloud9 Environment Select the environment named eksworkshop and pick delete  "
},
{
	"uri": "/calico/",
	"title": "Create Network Policies Using Calico",
	"tags": [],
	"description": "",
	"content": " Create Network Policies Using Calico In this Chapter, we will create some network policies using Calico and see the rules in action.\nNetwork policies allow you to define rules that determine what type of traffic is allowed to flow between different services. Using network policies you can also define rules to restrict traffic. They are a means to improve your cluster\u0026rsquo;s security.\nFor example, you can only allow traffic from frontend to backend in your application.\nNetwork policies also help in isolating traffic within namespaces. For instance, if you have separate namespaces for development and production, you can prevent traffic flow between them by restrict pod to pod communication within the same namespace.\n"
},
{
	"uri": "/spotworkers/cleanup/",
	"title": "깔끔하게 제거하기",
	"tags": [],
	"description": "",
	"content": "마이크로서비스 디플로이먼트를 모두 제거합니다.\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  스팟 핸들러 데몬셋을 제거합니다.\nkubectl delete -f ~/environment/spot/spot-interrupt-handler-example.yml  이번 모듈에서 생성된 워커를 모두 제거하기 위해서, 아래의 명령어를 실행합니다.\nEKS에서 워커 노드 모두 제거하기:\naws cloudformation delete-stack --stack-name \u0026quot;eksworkshop-spot-workers\u0026quot;  "
},
{
	"uri": "/logging/",
	"title": "Logging with Elasticsearch, Fluentd, and Kibana (EFK)",
	"tags": [],
	"description": "",
	"content": " Implement Logging with EFK In this Chapter, we will deploy a common Kubernetes logging pattern which consists of the following:\n Fluentd is an open source data collector providing a unified logging layer, supported by 500+ plugins connecting to many types of systems. Elasticsearch is a distributed, RESTful search and analytics engine. Kibana lets you visualize your Elasticsearch data.  Together, Fluentd, Elasticsearch and Kibana is also known as “EFK stack”. Fluentd will forward logs from the individual instances in the cluster to a centralized logging backend (CloudWatch Logs) where they are combined for higher-level reporting using ElasticSearch and Kibana.\n"
},
{
	"uri": "/monitoring/",
	"title": "Monitoring using Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Monitoring using Prometheus and Grafana In this Chapter, we will deploy Prometheus and Grafana to monitor Kubernetes cluster\n"
},
{
	"uri": "/servicemesh_with_istio/",
	"title": "Service Mesh with Istio",
	"tags": [],
	"description": "",
	"content": " Service Mesh With Istio A service mesh is a dedicated infrastructure layer for handling service-to-service communication. It’s responsible for the reliable delivery of requests through the complex topology of services that comprise a modern, cloud native application.\nService mesh solutions have two distinct components that behave somewhat differently: 1) a data plane, and 2) a control plane. The following diagram illustrates the basic architecture.\n The data plane is composed of a set of intelligent proxies (Envoy) deployed as sidecars. These proxies mediate and control all network communication between microservices along with Mixer, a general-purpose policy and telemetry hub.\n The control plane manages and configures the proxies to route traffic. Additionally, the control plane configures Mixers to enforce policies and collect telemetry.\n  "
},
{
	"uri": "/servicemesh_with_appmesh/",
	"title": "Service Mesh with AWS App Mesh",
	"tags": [],
	"description": "",
	"content": " Service Mesh With AWS App Mesh A service mesh is a dedicated infrastructure layer for handling service-to-service communication.\nAWS App Mesh is a service mesh based on the Envoy proxy that makes it easy to monitor and control containerized microservices. App Mesh standardizes how your microservices communicate, giving you end-to-end visibility and helping to ensure high-availability for your applications.\nApp Mesh gives you consistent visibility and network traffic controls for every microservice in an application. You can use App Mesh with Amazon ECS (using the Amazon EC2 launch type), Amazon EKS, and Kubernetes on AWS.\nApp Mesh is currently in Public Preview. In addition to this workshop module, you can visit the official AWS App Mesh page.\nThe content of this chapter was based on work found at https://github.com/awslabs/aws-app-mesh-examples. Be sure to check that repo often for the latest App Mesh demos.\n"
},
{
	"uri": "/advanced-networking/",
	"title": "Advanced VPC Networking with EKS",
	"tags": [],
	"description": "",
	"content": " Advanced VPC Networking with EKS In this Chapter, we will review some of the advanced VPC networking features with EKS.\n"
},
{
	"uri": "/statefulset/",
	"title": "Stateful containers using StatefulSets",
	"tags": [],
	"description": "",
	"content": " Stateful containers using StatefulSets StatefulSets manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods, suitable for applications that require one or more of the following.\n Stable, unique network identifiers Stable, persistent storage Ordered, graceful deployment and scaling Ordered, automated rolling updates  In this Chapter, we will review how to deploy MySQL database using StatefulSets and EBS as PersistentVolume. The example is a MySQL single master topology with multiple slaves running asynchronous replication.\n"
},
{
	"uri": "/advanced-networking/secondary_cidr/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s cleanup this tutorial\nkubectl delete deployments --all  Edit aws-node configmap and comment AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG and its value\nkubectl edit daemonset -n kube-system aws-node  ... spec: containers: - env: #- name: AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG # value: \u0026quot;true\u0026quot; - name: AWS_VPC_K8S_CNI_LOGLEVEL value: DEBUG - name: MY_NODE_NAME ...  Delete custom resource objects from ENIConfig CRD\nkubectl delete eniconfig/group1-pod-netconfig kubectl delete eniconfig/group2-pod-netconfig kubectl delete eniconfig/group3-pod-netconfig  Terminate EC2 instances so that fresh instances are launched with default CNI configuration\nUse caution before you run the next command because it terminates all worker nodes including running pods in your workshop\n INSTANCE_IDS=(`aws ec2 describe-instances --query 'Reservations[*].Instances[*].InstanceId' --filters \u0026quot;Name=tag:Name,Values=eksworkshop*\u0026quot; --output text` ) for i in \u0026quot;${INSTANCE_IDS[@]}\u0026quot; do echo \u0026quot;Terminating EC2 instance $i ...\u0026quot; aws ec2 terminate-instances --instance-ids $i done  Delete secondary CIDR from your VPC\nVPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=eksctl-eksworkshop* | jq -r '.Vpcs[].VpcId') ASSOCIATION_ID=$(aws ec2 describe-vpcs --vpc-id $VPC_ID | jq -r '.Vpcs[].CidrBlockAssociationSet[] | select(.CidrBlock == \u0026quot;100.64.0.0/16\u0026quot;) | .AssociationId') aws ec2 delete-subnet --subnet-id $CGNAT_SNET1 aws ec2 delete-subnet --subnet-id $CGNAT_SNET2 aws ec2 delete-subnet --subnet-id $CGNAT_SNET3 aws ec2 disassociate-vpc-cidr-block --association-id $ASSOCIATION_ID  "
},
{
	"uri": "/batch/workflow-simple/",
	"title": "Simple Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Simple Batch Workflow Save the below manifest as \u0026lsquo;workflow-whalesay.yaml\u0026rsquo; using your favorite editor and let\u0026rsquo;s deploy the whalesay example from before using Argo.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: whalesay- spec: entrypoint: whalesay templates: - name: whalesay container: image: docker/whalesay command: [cowsay] args: [\u0026quot;This is an Argo Workflow!\u0026quot;]  Now deploy the workflow using the argo CLI.\nYou can also run workflow specs directly using kubectl but the argo CLI provides syntax checking, nicer output, and requires less typing. For the equivalent kubectl commands, see Argo CLI.\n argo submit --watch workflow-whalesay.yaml  Name: whalesay-2kfxb Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Started: Sat Nov 17 10:32:13 -0500 (3 seconds ago) Finished: Sat Nov 17 10:32:16 -0500 (now) Duration: 3 seconds STEP PODNAME DURATION MESSAGE ✔ whalesay-2kfxb whalesay-2kfxb 2s  Make a note of the workflow\u0026rsquo;s name from your output (It should be similar to whalesay-xxxxx).\nConfirm the output by running the following command, substituting name of your workflow for \u0026ldquo;whalesay-xxxxx\u0026rdquo;:\nargo logs whalesay-xxxxx  ___________________________ \u0026lt; This is an Argo Workflow! \u0026gt; --------------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  "
},
{
	"uri": "/servicemesh_with_istio/visualize/",
	"title": "Monitor &amp; Visualize",
	"tags": [],
	"description": "",
	"content": " Collecting new telemetry data Next, download a YAML file to hold configuration for the new metric and log stream that Istio will generate and collect automatically.\ncurl -LO https://eksworkshop.com/servicemesh/deploy.files/istio-telemetry.yaml kubectl apply -f istio-telemetry.yaml  Make sure Prometheus and Grafana are running\nkubectl -n istio-system get svc prometheus kubectl -n istio-system get svc grafana  Setup port-forwarding for Grafana by executing the following command:\nkubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 8080:3000 \u0026amp;  Open the Istio Dashboard via the Grafana UI\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /dashboard/db/istio-mesh-dashboard  Open a new terminal tab and enter to send a traffic to the mesh\nexport SMHOST=$(kubectl get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname} ' -n istio-system) SMHOST=\u0026quot;$(echo -e \u0026quot;${SMHOST}\u0026quot; | tr -d '[:space:]')\u0026quot; while true; do curl -o /dev/null -s \u0026quot;${SMHOST}/productpage\u0026quot;; done  You will see that the traffic is evenly spread between reviews:v1 and reviews:v3\nWe encourage you to explore other Istio dashboards that are available by clicking the Istio Mesh Dashboard menu on top left of the page, and selecting a different dashboard.\n"
},
{
	"uri": "/servicemesh_with_appmesh/cleanup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": " There are three groups of components to remove when we\u0026rsquo;re done:\n App Mesh components k8s Colorteller app curler deployment  Remove App Mesh components To remove the App Mesh components, copy and paste this code into your terminal:\naws appmesh delete-route --mesh-name APP_MESH_DEMO --route-name colorteller-route --virtual-router-name colorteller-vr aws appmesh delete-route --mesh-name APP_MESH_DEMO --route-name colorgateway-route --virtual-router-name colorgateway-vr aws appmesh delete-virtual-router --mesh-name APP_MESH_DEMO --virtual-router-name colorteller-vr aws appmesh delete-virtual-router --mesh-name APP_MESH_DEMO --virtual-router-name colorgateway-vr aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorgateway-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-red-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-black-vn aws appmesh delete-virtual-node --mesh-name APP_MESH_DEMO --virtual-node-name colorteller-blue-vn aws appmesh delete-mesh --mesh-name APP_MESH_DEMO  Remove Colorteller App To remove the Colorteller App, copy and paste this code into your terminal:\nkubectl delete -f https://raw.githubusercontent.com/geremyCohen/colorapp/master/colorapp.yaml  Remove Curler To remove curler, copy and paste this code into your terminal:\nkubectl delete deployment.apps/curler  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_1/",
	"title": "K8s 오브젝트 세부내용 (1/2)",
	"tags": [],
	"description": "",
	"content": " 파드(Pod)  하나 이상의 컨테이너를 둘러싼 가장 작은 래퍼(wrapper) 단위.  데몬셋(DaemonSet)  워커 노드에 파드의 단일 인스턴스를 실행합니다.  디플로이먼트(Deployment)  어플리케이션 버전의 롤아웃(또는 롤백) 방법에 대한 세부내용.  "
},
{
	"uri": "/deploy/scalefrontend/",
	"title": "프론트엔드 스케일링",
	"tags": [],
	"description": "",
	"content": "또한 프론트엔드 서비스도 같은 방식으로 스케일링 합니다.\nkubectl get deployments kubectl scale deployment ecsdemo-frontend --replicas=3 kubectl get deployments  브라우저에서 실행 중인 우리 애플리케이션을 확인합니다. 이제는 여러 프론트엔드 서비스로 트래픽이 흐르는 것을 볼 수 있어야 합니다.\n"
},
{
	"uri": "/helm_root/helm_micro/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To delete the workshop release, run:\nhelm del --purge workshop  "
},
{
	"uri": "/helm_root/helm_intro/",
	"title": "Install Helm on EKS",
	"tags": [],
	"description": "",
	"content": " Install Helm on EKS Helm is a package manager and application management tool for Kubernetes that packages multiple Kubernetes resources into a single logical deployment unit called Chart.\nHelm helps you to:\n Achieve a simple (one command) and repeatable deployment Manage application dependency, using specific versions of other application and services Manage multiple deployment configurations: test, staging, production and others Execute post/pre deployment jobs during application deployment Update/rollback and test application deployments  "
},
{
	"uri": "/conclusion/survey/",
	"title": "Let us know what you think!",
	"tags": [],
	"description": "",
	"content": " Please take our survey! (function(t,e,s,n){var o,a,c;t.SMCX=t.SMCX||[],e.getElementById(n)||(o=e.getElementsByTagName(s),a=o[o.length-1],c=e.createElement(s),c.type=\"text/javascript\",c.async=!0,c.id=n,c.src=[\"https:\"===location.protocol?\"https://\":\"http://\",\"widget.surveymonkey.com/collect/website/js/tRaiETqnLgj758hTBazgd_2BU860jlhPrsKW9DSM0aec7fijRMWQEdDb7y2zM_2FUrIx.js\"].join(\"\"),a.parentNode.insertBefore(c,a))})(window,document,\"script\",\"smcx-sdk\");Create your own user feedback survey    "
},
{
	"uri": "/calico/stars_policy_demo/",
	"title": "Stars Policy Demo",
	"tags": [],
	"description": "",
	"content": " Stars Policy Demo In this sub-chapter we create frontend, backend, client and UI services on the EKS cluster and define network policies to allow or block communication between these services. This demo also has a management UI that shows the available ingress and egress paths between each service.\n"
},
{
	"uri": "/batch/workflow-advanced/",
	"title": "Advanced Batch Workflow",
	"tags": [],
	"description": "",
	"content": " Advanced Batch Workflow Let\u0026rsquo;s take a look at a more complex workflow, involving passing artifacts between jobs, multiple dependencies, etc.\nSave the below manifest as teardrop.yaml using your favorite editor.\napiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: teardrop- spec: entrypoint: teardrop templates: - name: create-chain container: image: alpine:latest command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;touch /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay inputs: parameters: - name: message artifacts: - name: chain path: /tmp/message container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: whalesay-reduce inputs: parameters: - name: message artifacts: - name: chain-0 path: /tmp/message.0 - name: chain-1 path: /tmp/message.1 container: image: docker/whalesay command: [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;] args: [\u0026quot;echo Chain: ; cat /tmp/message* | sort | uniq | tee /tmp/message; cowsay This is Job {{inputs.parameters.message}}! ; echo {{inputs.parameters.message}} \u0026gt;\u0026gt; /tmp/message\u0026quot;] outputs: artifacts: - name: chain path: /tmp/message - name: teardrop dag: tasks: - name: create-chain template: create-chain - name: Alpha dependencies: [create-chain] template: whalesay arguments: parameters: [{name: message, value: Alpha}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Bravo dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Bravo}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Charlie dependencies: [Alpha] template: whalesay arguments: parameters: [{name: message, value: Charlie}] artifacts: - name: chain from: \u0026quot;{{tasks.Alpha.outputs.artifacts.chain}}\u0026quot; - name: Delta dependencies: [Bravo] template: whalesay arguments: parameters: [{name: message, value: Delta}] artifacts: - name: chain from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: Echo dependencies: [Bravo, Charlie] template: whalesay-reduce arguments: parameters: [{name: message, value: Echo}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Bravo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Charlie.outputs.artifacts.chain}}\u0026quot; - name: Foxtrot dependencies: [Charlie] template: whalesay arguments: parameters: [{name: message, value: Foxtrot}] artifacts: - name: chain from: \u0026quot;{{tasks.create-chain.outputs.artifacts.chain}}\u0026quot; - name: Golf dependencies: [Delta, Echo] template: whalesay-reduce arguments: parameters: [{name: message, value: Golf}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Delta.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: Hotel dependencies: [Echo, Foxtrot] template: whalesay-reduce arguments: parameters: [{name: message, value: Hotel}] artifacts: - name: chain-0 from: \u0026quot;{{tasks.Echo.outputs.artifacts.chain}}\u0026quot; - name: chain-1 from: \u0026quot;{{tasks.Foxtrot.outputs.artifacts.chain}}\u0026quot;  This workflow uses a Directed Acyclic Graph (DAG) to explicitly define job dependencies. Each job in the workflow calls a whalesay template and passes a parameter with a unique name. Some jobs call a whalesay-reduce template which accepts multiple artifacts and combines them into a single artifact.\nEach job in the workflow pulls the artifact(s) and lists them in the \u0026ldquo;Chain\u0026rdquo;, then calls whalesay for the current job. Each job will then have a list of the previous job dependency chain (list of all jobs that had to complete before current job could run).\nRun the workflow.\nargo submit --watch teardrop.yaml  Name: teardrop-jfg5w Namespace: default ServiceAccount: default Status: Succeeded Created: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Started: Sat Nov 17 16:01:42 -0500 (7 minutes ago) Finished: Sat Nov 17 16:03:35 -0500 (5 minutes ago) Duration: 1 minute 53 seconds STEP PODNAME DURATION MESSAGE ✔ teardrop-jfg5w ├-✔ create-chain teardrop-jfg5w-3938249022 3s ├-✔ Alpha teardrop-jfg5w-3385521262 6s ├-✔ Bravo teardrop-jfg5w-1878939134 35s ├-✔ Charlie teardrop-jfg5w-3753534620 35s ├-✔ Foxtrot teardrop-jfg5w-2036090354 5s ├-✔ Delta teardrop-jfg5w-37094256 34s ├-✔ Echo teardrop-jfg5w-4165010455 31s ├-✔ Hotel teardrop-jfg5w-2342859904 4s └-✔ Golf teardrop-jfg5w-1687601882 30s  Continue to the Argo Dashboard to explore this model further.\n"
},
{
	"uri": "/servicemesh_with_istio/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "To cleanup, follow the below steps.\nTo remove telemetry configuration / port-forward process\nkubectl delete -f istio-telemetry.yaml  To remove the application virtual services / destination rules\nkubectl delete -f samples/bookinfo/networking/virtual-service-all-v1.yaml kubectl delete -f samples/bookinfo/networking/destination-rule-all.yaml  To remove the gateway / application\nkubectl delete -f samples/bookinfo/networking/bookinfo-gateway.yaml kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml  To remove Istio\nkubectl delete -f istio.yaml kubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml kubectl delete namespace istio-system  "
},
{
	"uri": "/introduction/basics/concepts_objects_details_2/",
	"title": "K8s 오브젝트 세부내용 (2/2)",
	"tags": [],
	"description": "",
	"content": " 레플리카 셋(ReplicaSet)  계속 동작할 파드의 개수를 정의합니다.  잡(Job)  파드가 제대로 완성(completion)되어 동작할 수 있도록 합니다.  서비스(Service)  고정 IP 주소를 파드의 논리 그룹과 매핑합니다.  레이블(Label)  연결(association)과 필터링에 사용되는 키/밸류(Key/Value) 쌍.  "
},
{
	"uri": "/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup "
},
{
	"uri": "/helm_root/helm_nginx/",
	"title": "Deploy Nginx With Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Nginx With Helm In this Chapter, we will dig deeper with Helm and demonstrate how to install the NGINX web server via the following steps:\n Update the Chart Repository   Search the Chart Repository   Add the Bitnami Repository   Install bitnami/nginx   Clean Up   "
},
{
	"uri": "/batch/dashboard/",
	"title": "Argo Dashboard",
	"tags": [],
	"description": "",
	"content": " Argo Dashboard Argo UI lists the workflows and visualizes each workflow (very handy for our last workflow).\nTo connect, use the same proxy connection setup in Deploy the Official Kubernetes Dashboard.\n  Show me the command   kubectl proxy --port=8080 --address='0.0.0.0' --disable-filter=true \u0026amp;  This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests.\nThis command will continue to run in the background of the current terminal\u0026rsquo;s session.\nWe are disabling request filtering, a security feature that guards against XSRF attacks. This isn\u0026rsquo;t recommended for a production environment, but is useful for our dev environment.\n   To access the Argo Dashboard:\n In your Cloud9 environment, click Preview / Preview Running Application Scroll to the end of the URL and append:  /api/v1/namespaces/argo/services/argo-ui/proxy/  You will see the teardrop workflow from Advanced Batch Workflow. Click on it to see a visualization of the workflow.\nThe workflow should relatively look like a teardrop, and provide a live status for each job. Click on Hotel to see a summary of the Hotel job.\nThis details basic information about the job, and includes a link to the Logs. The Hotel job logs list the job dependency chain and the current whalesay, and should look similar to:\nChain: Alpha Bravo Charlie Echo Foxtrot ____________________ \u0026lt; This is Job Hotel! \u0026gt; -------------------- \\ \\ \\ ## . ## ## ## == ## ## ## ## === /\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;\u0026quot;___/ === ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ / ===- ~~~ \\______ o __/ \\ \\ __/ \\____\\______/  Explore the other jobs in the workflow to see each job\u0026rsquo;s status and logs.\n"
},
{
	"uri": "/introduction/architecture/",
	"title": "쿠버네티스 아키텍처",
	"tags": [],
	"description": "",
	"content": "이번 장에서는 다음의 주제를 다룹니다.\n 아키텍처 개요   콘트롤 플레인   데이터 플레인   쿠버네티스 클러스터 설치   "
},
{
	"uri": "/helm_root/helm_micro/",
	"title": "Deploy Example Microservices Using Helm",
	"tags": [],
	"description": "",
	"content": " Deploy Example Microservices Using Helm In this chapter, we will demonstrate how to deploy microservices using a custom Helm Chart, instead of doing everything manually using kubectl.\nFor detailed information on working with chart templates, refer to the Helm docs\n"
},
{
	"uri": "/batch/cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Cleanup Delete all workflows argo delete --all  Remove Artifact Repository Bucket ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws s3 rb s3://batch-artifact-repository-${ACCOUNT_ID}/ --force  Remove permissions for Artifact Repository Bucket INSTANCE_PROFILE_PREFIX=$(aws cloudformation describe-stacks | jq -r '.Stacks[].StackName' | grep eksctl-eksworkshop-eksctl-nodegroup) INSTANCE_PROFILE_NAME=$(aws iam list-instance-profiles | jq -r '.InstanceProfiles[].InstanceProfileName' | grep $INSTANCE_PROFILE_PREFIX) ROLE_NAME=$(aws iam get-instance-profile --instance-profile-name $INSTANCE_PROFILE_NAME | jq -r '.InstanceProfile.Roles[] | .RoleName') ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) aws iam delete-role-policy --role-name $ROLE_NAME --policy-name S3-Policy-For-Worker  Undeploy Argo kubectl delete -n argo -f https://raw.githubusercontent.com/argoproj/argo/v2.2.1/manifests/install.yaml kubectl delete ns argo  Cleanup Kubernetes Job kubectl delete job/whalesay  "
},
{
	"uri": "/introduction/architecture/architecture_control_and_data_overview/",
	"title": "아키텍처 개요",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 kubectl--api controller--api scheduler--api api--kubelet1 api--etcd kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;  "
},
{
	"uri": "/introduction/eks/",
	"title": "Amazon EKS",
	"tags": [],
	"description": "",
	"content": "이번 섹션에서는 다음 주제를 다룹니다.\n EKS 클러스터 생성 작업 순서   EKS 클러스터를 생성할 때 무슨 일이 일어날까   EKS 콘트롤 플레인과 워커 노드간의 통신 아키텍처   클러스터 관리 도구   Amazon EKS!   "
},
{
	"uri": "/deploy/cleanup/",
	"title": "애플리케이션 정리",
	"tags": [],
	"description": "",
	"content": "애플리케이션이 생성한 리소스를 삭제하려면 해당 애플리케이션의 디폴로이먼트를 삭제해야 합니다.\n애플리케이션 디폴로이먼트 삭제\ncd ~/environment/ecsdemo-frontend kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-crystal kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml cd ~/environment/ecsdemo-nodejs kubectl delete -f kubernetes/service.yaml kubectl delete -f kubernetes/deployment.yaml  "
},
{
	"uri": "/conclusion/",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": " Conclusion "
},
{
	"uri": "/introduction/architecture/architecture_control/",
	"title": "콘트롤 플레인",
	"tags": [],
	"description": "",
	"content": "graph TB kubectl{kubectl} subgraph ControlPlane api(API Server) controller(Controller Manager) scheduler(Scheduler) etcd(etcd) end kubectl--api controller--api scheduler--api api--kubelet api--etcd classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   1개 이상의 API 서버: kubectl 의 REST 진입점\n etcd: 분산 키/벨류 저장소\n 콘트롤 매니저: 항상 현재 상태와 원하는 상태르 평가\n 스케줄러: 작업 노드에 파드를 스케줄링\n  쿠버네티스 공식 문서에서 콘트롤 플레인에 대해 더 깊이 있는 설명을 확인할 수 있습니다.\n"
},
{
	"uri": "/helm_root/helm_nginx/updatecharts/",
	"title": "Update the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Helm uses a packaging format called Charts. A Chart is a collection of files that describe k8s resources.\nCharts can be simple, describing something like a standalone web server (which is what we are going to create), but they can also be more complex, for example, a chart that represents a full web application stack included web servers, databases, proxies, etc.\nInstead of installing k8s resources manually via kubectl, we can use Helm to install pre-defined Charts faster, with less chance of typos or other operator errors.\nWhen you install Helm, you are provided with a default repository of Charts from the official Helm Chart Repository.\nThis is a very dynamic list that always changes due to updates and new additions. To keep Helm\u0026rsquo;s local list updated with all these changes, we need to occasionally run the repository update command.\nTo update Helm\u0026rsquo;s local list of Charts, run:\nhelm repo update  And you should see something similar to:\nHang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026quot;stable\u0026quot; chart repository Update Complete. ⎈ Happy Helming!⎈  Next, we\u0026rsquo;ll search for the NGINX web server Chart.\n"
},
{
	"uri": "/introduction/architecture/architecture_worker/",
	"title": "데이터 플레인",
	"tags": [],
	"description": "",
	"content": "graph TB internet((internet)) subgraph worker1 kubelet1(kubelet) kube-proxy1(kube-proxy) subgraph docker1 subgraph podA containerA[container] end subgraph podB containerB[container] end end end internet--kube-proxy1 api--kubelet1 kubelet1--containerA kubelet1--containerB kube-proxy1--containerA kube-proxy1--containerB classDef green fill:#9f6,stroke:#333,stroke-width:4px; classDef orange fill:#f96,stroke:#333,stroke-width:4px; classDef blue fill:#6495ed,stroke:#333,stroke-width:4px; class api blue; class internet green; class kubectl orange;   작업 노드로 구성\n kubelet: API 서버와 노드 간에 연결 역할\n kube-proxy: IP 변환과 라우팅 관리\n  쿠버네티스 공식 문서에서 데이터 플레인에 대해 더 깊이 있는 설명을 확인할 수 있습니다.\n"
},
{
	"uri": "/introduction/architecture/cluster_setup_options/",
	"title": "쿠버네티스 클러스터 설치",
	"tags": [],
	"description": "",
	"content": "매니지드 Amazon EKS 솔루션에 외에 자체 관리 쿠버네티스 클러스터를 시작하고 설정하는데 도움이 되는 도구가 많습니다. 일부는 다음과 같습니다.\n Minikube – 개발과 학습 Kops – 학습, 개발, 제품화 Kubeadm – 학습, 개발, 제품화 맥을 위한 도커 - 학습, 개발 도커에서 쿠버네티스 - 학습, 개발  이러한 오픈 소스 솔루션 외에도 많은 상용 옵션을 사용할 수 있습니다.\nAmazon EKS를 살펴 봅시다!\n"
},
{
	"uri": "/introduction/eks/eks_customers/",
	"title": "EKS 클러스터 생성 작업 순서",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_control_plane/",
	"title": "EKS 클러스터를 생성할 때 무슨 일이 일어날까",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_architecture/",
	"title": "EKS 콘트롤 플레인과 워커 노드간의 통신 아키텍처",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/introduction/eks/eks_high_level/",
	"title": "클러스터 관리 도구",
	"tags": [],
	"description": "",
	"content": "귀하의 EKS 클러스터가 준비되면, API 엔드포인트를 얻을 수 있고 클러스터와 상호작용하도록 커뮤니티에서 개발한 도구인 kubectl을 사용할 수 있습니다.\n"
},
{
	"uri": "/introduction/eks/stay_tuned/",
	"title": "Amazon EKS!",
	"tags": [],
	"description": "",
	"content": "다음 모듈에서 EKS로 여행을 진행하며 계속 지켜봐주십시오!\n질문은 늘 열려있습니다! 이 워크숍에서나 공식 쿠버네티스 슬랙 이나 AWSKRUG 슬랙에서 언제든지 직접 물어보십시오.\n"
},
{
	"uri": "/helm_root/helm_nginx/searchchart/",
	"title": "Search the Chart Repository",
	"tags": [],
	"description": "",
	"content": "Now that our repository Chart list has been updated, we can search for Charts.\nTo list all Charts:\nhelm search  That should output something similiar to:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.0 2.1.1 Scales worker... stable/aerospike 0.1.7 v3.14.1.2 A Helm chart... ...  You can see from the output that it dumped the list of all Charts it knows about. In some cases that may be useful, but an even more useful search would involve a keyword argument. So next, we\u0026rsquo;ll search just for NGINX:\nhelm search nginx  That results in:\nNAME CHART VERSION APP VERSION DESCRIPTION stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress ... stable/nginx-ldapauth-proxy 0.1.2 1.13.5 nginx proxy ... stable/nginx-lego 0.3.1 Chart for... stable/gcloud-endpoints 0.1.2 1 DEPRECATED Develop... ...  This new list of Charts are specific to nginx, because we passed the nginx argument to the search command.\n"
},
{
	"uri": "/helm_root/helm_nginx/addbitnamirepo/",
	"title": "Add the Bitnami Repository",
	"tags": [],
	"description": "",
	"content": "In the last slide, we saw that NGINX offers many different products via the default Helm Chart repository, but the NGINX standalone web server is not one of them.\nAfter a quick web search, we discover that there is a Chart for the NGINX standalone web server available via the Bitnami Chart repository.\nTo add the Bitnami Chart repo to our local list of searchable charts:\nhelm repo add bitnami https://charts.bitnami.com/bitnami  Once that completes, we can search all Bitnami Charts:\nhelm search bitnami  Which results in:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/bitnami-common 0.0.3 0.0.1 Chart with... bitnami/apache 2.1.2 2.4.37 Chart for Apache... bitnami/cassandra 0.1.0 3.11.3 Apache Cassandra... ...  Search once again for NGINX:\nhelm search nginx  Now we are seeing more NGINX options, across both repositories:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress... stable/nginx-ingress 0.31.0 0.20.0 An nginx Ingress controller ...  Or even search the Bitnami repo, just for NGINX:\nhelm search bitnami/nginx  Which narrows it down to NGINX on Bitnami:\nNAME CHART VERSION APP VERSION DESCRIPTION bitnami/nginx 1.1.2 1.14.1 Chart for the nginx server bitnami/nginx-ingress-controller 2.1.4 0.20.0 Chart for the nginx Ingress...  In both of those last two searches, we see\nbitnami/nginx  as a search result. That\u0026rsquo;s the one we\u0026rsquo;re looking for, so let\u0026rsquo;s use Helm to install it to the EKS cluster.\n"
},
{
	"uri": "/helm_root/helm_nginx/installnginx/",
	"title": "Install bitnami/nginx",
	"tags": [],
	"description": "",
	"content": " Installing the Bitnami standalone NGINX web server Chart involves us using the helm install command.\nWhen we install using Helm, we need to provide a deployment name, or a random one will be assigned to the deployment automatically.\nChallenge: How can you use Helm to deploy the bitnami/nginx chart?\nHINT: Use the helm utility to install the bitnami/nginx chart and specify the name mywebserver for the Kubernetes deployment. Consult the helm install documentation or run the helm install --help command to figure out the syntax\n  Expand here to see the solution   helm install --name mywebserver bitnami/nginx    Once you run this command, the output confirms the types of k8s objects that were created as a result:\nNAME: mywebserver LAST DEPLOYED: Tue Nov 13 19:55:25 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/Deployment NAME AGE mywebserver-nginx 0s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 0/1 ContainerCreating 0 0s ==\u0026gt; v1/Service NAME AGE mywebserver-nginx 0s  In the following kubectl command examples, it may take a minute or two for each of these objects\u0026rsquo; DESIRED and CURRENT values to match; if they don\u0026rsquo;t match on the first try, wait a few seconds, and run the command again to check the status.\n The first object shown in this output is a Deployment. A Deployment object manages rollouts (and rollbacks) of different versions of an application.\nYou can inspect this Deployment object in more detail by running the following command:\nkubectl describe deployment mywebserver-nginx  The next object shown created by the Chart is a Pod. A Pod is a group of one or more containers.\nTo verify the Pod object was successfully deployed, we can run the following command:\nkubectl get pods -l app=mywebserver-nginx  And you should see output similar to:\nNAME READY STATUS RESTARTS AGE mywebserver-nginx-85985c8466-tczst 1/1 Running 0 10s  The third object that this Chart creates for us is a Service The Service enables us to contact this NGINX web server from the Internet, via an Elastic Load Balancer (ELB).\nTo get the complete URL of this Service, run:\nkubectl get service mywebserver-nginx -o wide  That should output something similar to:\nNAME TYPE CLUSTER-IP EXTERNAL-IP mywebserver-nginx LoadBalancer 10.100.223.99 abc123.amazonaws.com  Copy the value for EXTERNAL-IP, open a new tab in your web browser, and paste it in. It may take a couple minutes for the ELB and its associated DNS name to become available; if you get an error, wait one minute, and hit reload.\n When the Service does come online, you should see a welcome message similar to:\nCongrats! You\u0026rsquo;ve now successfully deployed the NGINX standalone web server to your EKS cluster!\n"
},
{
	"uri": "/helm_root/helm_nginx/cleaningup/",
	"title": "Clean Up",
	"tags": [],
	"description": "",
	"content": "To remove all the objects that the Helm Chart created, we can use Helm delete.\nBefore we delete it though, we can verify what we have running via the Helm list command:\nhelm list  You should see output similar to below, which show that mywebserver is installed:\nNAME REVISION UPDATED STATUS CHART APP VERSION mywebserver 1 Tue Nov 13 19:55:25 2018 DEPLOYED nginx-1.1.2 1.14.1  It was a lot of fun; we had some great times sending HTTP back and forth, but now its time to delete this deployment. To delete:\nhelm delete --purge mywebserver  And you should be met with the output:\nrelease \u0026quot;mywebserver\u0026quot; deleted  kubectl will also demonstrate that our pods and service are no longer available:\nkubectl get pods -l app=mywebserver-nginx kubectl get service mywebserver-nginx -o wide  As would trying to access the service via the web browser via a page reload.\nWith that, cleanup is complete.\n"
},
{
	"uri": "/tabs-example/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tabs with regular text:  Tab 1 Tab 2  echo \"This is tab 1.\"  println \"This is tab 2.\"   $(function(){$(\"#tab\").tabs();}); Tabs with code blocks:  Tab 1 Tab 2  echo \u0026#34;This is tab 1.\u0026#34;   println \u0026#34;This is tab 2.\u0026#34;    $(function(){$(\"#tab-with-code\").tabs();}); Tabs showing installation process:   eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#tab-installation\").tabs();}); Second set of tabs showing installation process:   eksctl terraform cloudformation  To create a basic EKS cluster, run:\neksctl create cluster --name=eksworkshop-eksctl --nodes=3 --node-ami=auto --region=${AWS_REGION}  Launching EKS and all the dependencies will take approximately 15 minutes\n  We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n  To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n  $(function(){$(\"#more-tab-installation\").tabs();}); "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/authors/",
	"title": "Credits",
	"tags": [],
	"description": "",
	"content": " Thanks to our wonderful contributors  for making Open Source a better place! .ghContributors{ display:flex; flex-flow: wrap; align-content: flex-start } .ghContributors  div{ width: 50% ; display: inline-flex; margin-bottom: 5px; } .ghContributors  div label{ padding-left: 4px ; } .ghContributors  div span{ font-size: x-small; padding-left: 4px ; }  "
},
{
	"uri": "/tabs-example/tabs/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tabs-example/tabs/eks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "To build the EKS cluster, we need to tell the EKS service which IAM Service role to use, and which Subnets and Security Group to use. We can gather this information from our previous labs where we built the IAM role and VPC:\nexport SERVICE_ROLE=$(aws iam get-role --role-name \u0026quot;eks-service-role-workshop\u0026quot; --query Role.Arn --output text) export SECURITY_GROUP=$(aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SecurityGroups'].OutputValue\u0026quot; --output text) export SUBNET_IDS=$( aws cloudformation describe-stacks --stack-name \u0026quot;eksworkshop-cf\u0026quot; --query \u0026quot;Stacks[0].Outputs[?OutputKey=='SubnetIds'].OutputValue\u0026quot; --output text)  Let\u0026rsquo;s confirm the variables are now set in our environment:\necho SERVICE_ROLE=${SERVICE_ROLE} echo SECURITY_GROUP=${SECURITY_GROUP} echo SUBNET_IDS=${SUBNET_IDS}  Now we can create the EKS cluster:\naws eks create-cluster --name eksworkshop-cf --role-arn \u0026quot;${SERVICE_ROLE}\u0026quot; --resources-vpc-config subnetIds=\u0026quot;${SUBNET_IDS}\u0026quot;,securityGroupIds=\u0026quot;${SECURITY_GROUP}\u0026quot;  Cluster provisioning usually takes less than 10 minutes.\n You can query the status of your cluster with the following command:\naws eks describe-cluster --name \u0026quot;eksworkshop-cf\u0026quot; --query cluster.status --output text  When your cluster status is ACTIVE you can proceed.\n"
},
{
	"uri": "/tabs-example/tabs/launcheks/",
	"title": "Embedded tab content",
	"tags": [],
	"description": "",
	"content": "We start by initializing the Terraform state:\nterraform init  We can now plan our deployment:\nterraform plan -var 'cluster-name=eksworkshop-tf' -var 'desired-capacity=3' -out eksworkshop-tf  And if we want to apply that plan:\nterraform apply \u0026quot;eksworkshop-tf\u0026quot;  Applying the fresh terraform plan will take approximately 15 minutes\n "
},
{
	"uri": "/example_cf_templates/",
	"title": "Example of using CloudFormation Templates",
	"tags": [],
	"description": "",
	"content": " Click below to add a CloudFormation Stack    Use these templates:       Template 1 example  Launch    Download     Template 2 example  Launch    Download     Template 3 example  Launch    Download      "
},
{
	"uri": "/prerequisites/self_paced/eu-west-1/",
	"title": "Ireland",
	"tags": [],
	"description": "",
	"content": "Cloud9 환경 생성하기: https://eu-west-1.console.aws.amazon.com/cloud9/home?region=eu-west-1\n"
},
{
	"uri": "/more_resources/",
	"title": "More Resources",
	"tags": [],
	"description": "",
	"content": " Discover more AWS resources for building and running your application on AWS:\nMore Workshops  Amazon ECS Workshop - Learn how to use Stelligent Mu to deploy a microservice architecture that runs in AWS Fargate Amazon Lightsail Workshop - If you are getting started with the cloud and looking for a way to run an extremely low cost environment Lightsail is perfect. Learn how to deploy to Amazon Lightsail with this workshop.  Tools for AWS Fargate and Amazon ECS  Containers on AWS - Learn common best-practices for running containers on AWS fargate - Command line tool for interacting with AWS Fargate. With just a single command you can build, push, and launch your container in Fargate, orchestrated by ECS. Terraform - Use Terraform to deploy your docker containers in Fargate Wonqa is a tool for spinning up disposable QA environments in AWS Fargate, with SSL enabled by Let\u0026rsquo;s Encrypt. More details about Wonqa on the Wonder Engineering blog coldbrew - Fantastic tool that provisions ECS infrastructure, builds and deploys your container, and connects your services to an application load balancer automatically. Has a great developer experience for day to day use mu - Automates everything relating to ECS devops and CI/CD. This framework lets you write a simple metadata file and it constructs all the infrastructure you need so that you can deploy to ECS by simply pushing to your Git repo.  Courses  Microservices with Docker, Flask, and React - Learn how to build, test, and deploy microservices powered by Docker, Flask, React Amazon ECS!  "
},
{
	"uri": "/prerequisites/self_paced/us-east-2/",
	"title": "Ohio",
	"tags": [],
	"description": "",
	"content": "Cloud9 환경 생성하기: https://us-east-2.console.aws.amazon.com/cloud9/home?region=us-east-2\n"
},
{
	"uri": "/prerequisites/self_paced/us-west-2/",
	"title": "Oregon",
	"tags": [],
	"description": "",
	"content": "Cloud9 환경 생성하기: https://us-west-2.console.aws.amazon.com/cloud9/home?region=us-west-2\n"
},
{
	"uri": "/prerequisites/self_paced/ap-southeast-1/",
	"title": "Singapore",
	"tags": [],
	"description": "",
	"content": "Cloud9 환경 생성하기: https://ap-southeast-1.console.aws.amazon.com/cloud9/home?region=ap-southeast-1\n"
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]